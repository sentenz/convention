{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Convention","text":"<p>General articles, conventions and guides for software projects.</p> <p>Software Development</p> <p>A roadmap for software development, including the software development lifecycle, software development methodologies, and software development tools.</p>"},{"location":"articles/","title":"About","text":"<p>Best practices, articles of interest and general topics.</p> <ul> <li>Abstraction Layer</li> <li>App Resources</li> <li>Architecture Decision Records</li> <li>Architecture Modeling Concepts</li> <li>Asymptotic Notation</li> <li>Branching Strategies</li> <li>Build Systems</li> <li>Chaos Engineering</li> <li>Comment</li> <li>Configuration Management</li> <li>Continuous Pipelines</li> <li>CRLF vs LF</li> <li>Cross Compiler</li> <li>Cryptography</li> <li>Data Management</li> <li>Data Serialization Formats</li> <li>Database</li> <li>Deployment Strategies</li> <li>Docs as Code</li> <li>DORA</li> <li>Everything as Code</li> <li>Feature Flags</li> <li>File Systems</li> <li>Githooks</li> <li>Incident Management</li> <li>Licenses</li> <li>Logging and Monitoring</li> <li>Merging Strategies</li> <li>Monorepo vs Polyrepo</li> <li>Package Managers</li> <li>Programming Paradigms</li> <li>Project Layout</li> <li>Risk Management</li> <li>Serial Communication</li> <li>Software Analysis</li> <li>Software Design Principles</li> <li>Software Development Environment</li> <li>Software Metric</li> <li>Software Testing</li> <li>Static Site Generators</li> <li>Tabs vs Spaces</li> <li>Technical Debt</li> <li>Twelve-Factor App</li> <li>Unit-Testing Frameworks</li> <li>XOps</li> </ul>"},{"location":"articles/abstraction-layer/","title":"Abstraction Layer","text":"<p>Abstraction layers refer to a concept that simplifies the complexity of software development by breaking down the different components of software into manageable, self-contained parts. Layers make software development efficient and effective by providing a standard interface for interacting with complex systems. It is a way to manage complexity and create a separation between the underlying implementation details and the functionality that is exposed to the user.</p> <p>Abstraction layers can be found in different types of software systems, such as operating systems, databases, programming languages, and web applications. Abstraction layers create maintainable and flexible software systems that are easier to understand, extend, and modify.</p> <ul> <li>1. Category</li> <li>1.1. Application Layer<ul> <li>1.1.1. Presentation Layer</li> <li>1.1.2. Business Logic Layer</li> <li>1.1.3. Data Access Layer</li> </ul> </li> <li>1.2. Middleware Layer<ul> <li>1.2.1. Communication Middleware</li> <li>1.2.2. Integration Middleware</li> <li>1.2.3. Security Middleware</li> </ul> </li> <li>1.3. System Layer<ul> <li>1.3.1. Hardware Abstraction Layer</li> <li>1.3.2. Operating System Layer</li> <li>1.3.3. Virtualization Layer</li> </ul> </li> <li>1.4. General Layer<ul> <li>1.4.1. Network Abstraction Layer</li> <li>1.4.2. Database Abstraction Layer</li> </ul> </li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/abstraction-layer/#1-category","title":"1. Category","text":"<p>Abstraction layers provide a modular and reusable architecture for software development, allowing for better maintainability, scalability, and interoperability.</p> <p>Each layer of abstraction is designed to interact with the layer above or below it and provide a interface. This approach simplifies the software development process, improves code reusability, and enhances system performance by minimizing direct interaction.</p>"},{"location":"articles/abstraction-layer/#11-application-layer","title":"1.1. Application Layer","text":"<p>Application Layer is the topmost layer that is directly accessible by end-users, which consists of user interfaces, applications, and programs.</p>"},{"location":"articles/abstraction-layer/#111-presentation-layer","title":"1.1.1. Presentation Layer","text":"<p>Presentation Layer is responsible for presenting data to users, and allows users to interact with the software application.</p> <p>It abstracts the presentation-specific features, such as:</p> <ul> <li>User interface</li> <li>Data rendering and visualization</li> <li>Layout and formatting</li> <li>Interaction management</li> </ul>"},{"location":"articles/abstraction-layer/#112-business-logic-layer","title":"1.1.2. Business Logic Layer","text":"<p>Business Logic Layer is responsible for implementing the business logic of the software application, and is often separated from the presentation and data access layers.</p> <p>It abstracts the application-specific features, such as:</p> <ul> <li>Rules and workflows</li> <li>Use cases and scenarios</li> <li>Business entities and objects</li> <li>Business process management</li> </ul>"},{"location":"articles/abstraction-layer/#113-data-access-layer","title":"1.1.3. Data Access Layer","text":"<p>Data Access Layer is responsible for accessing and manipulating data from the underlying data storage, and is often separated from the business logic and presentation layers.</p> <p>It abstracts the data storage-specific features, such as:</p> <ul> <li>Data storage and retrieval</li> <li>Query optimization</li> <li>Data mapping and transformation</li> <li>Transaction management</li> <li>Data validation</li> </ul>"},{"location":"articles/abstraction-layer/#12-middleware-layer","title":"1.2. Middleware Layer","text":"<p>Middleware Layer provides a bridge between the application layer and the operating system, which manages data transfer, network communication, and other essential functions.</p>"},{"location":"articles/abstraction-layer/#121-communication-middleware","title":"1.2.1. Communication Middleware","text":"<p>Communication Middleware provides an abstraction of the communication mechanisms between software components or services, and is responsible for enabling inter-process communication, message exchange, and remote procedure calls.</p> <p>It abstracts the communication-specific features, such as:</p> <ul> <li>Protocol adaptation</li> <li>Message exchange and routing</li> <li>Remote procedure calls</li> <li>Data transformation</li> </ul>"},{"location":"articles/abstraction-layer/#122-integration-middleware","title":"1.2.2. Integration Middleware","text":"<p>Integration Middleware provides an abstraction of the integration mechanisms between software systems or services, and is responsible for enabling data mapping, service discovery, and protocol conversion.</p> <p>It abstracts the integration-specific features, such as:</p> <ul> <li>Data mapping and transformation</li> <li>Transaction management</li> <li>Service discovery</li> <li>Protocol conversion</li> <li>Message exchange patterns</li> <li>Event-driven architecture</li> </ul>"},{"location":"articles/abstraction-layer/#123-security-middleware","title":"1.2.3. Security Middleware","text":"<p>Security Middleware provides an abstraction of the security mechanisms and policies of the software, and is responsible for securing data and resources across the software application or system.</p> <p>It abstracts the security-specific features, such as:</p> <ul> <li>Authentication and authorization</li> <li>Encryption and decryption</li> <li>Access control</li> <li>Audit and logging</li> </ul>"},{"location":"articles/abstraction-layer/#13-system-layer","title":"1.3. System Layer","text":"<p>The System Layer is the lowest layer that interacts directly with the computer hardware, including the operating system, device drivers, and other system-level software.</p>"},{"location":"articles/abstraction-layer/#131-hardware-abstraction-layer","title":"1.3.1. Hardware Abstraction Layer","text":"<p>Hardware Abstraction Layer (HAL) provides an abstraction of the hardware components and device drivers of the system, and is responsible for enabling software to interact with different types of hardware without being tied to specific hardware implementations or architectures.</p> <p>It abstracts the hardware-specific features, such as:</p> <ul> <li>I/O operations</li> <li>Interrupt handling</li> <li>Memory management</li> <li>Device drivers</li> </ul>"},{"location":"articles/abstraction-layer/#132-operating-system-layer","title":"1.3.2. Operating System Layer","text":"<p>Operating System (OS) Layer provides an abstraction of the hardware resources and system services of the underlying operating system, and is responsible for managing system resources, scheduling processes, and providing system-level services.</p> <p>It abstracts the OS-specific features, such as:</p> <ul> <li>Process management</li> <li>Memory management</li> <li>File system management</li> <li>System services</li> </ul>"},{"location":"articles/abstraction-layer/#133-virtualization-layer","title":"1.3.3. Virtualization Layer","text":"<p>Virtualization Layer provides an abstraction of the hardware resources and system services of the underlying virtualization technology, and is responsible for creating and managing virtual machines, virtual networks, and virtual storage.</p> <p>It abstracts the virtualization-specific features, such as:</p> <ul> <li>Hypervisor management</li> <li>Virtual machine creation</li> <li>Virtual machine migration</li> <li>Virtual storage management</li> <li>Virtual network management</li> </ul>"},{"location":"articles/abstraction-layer/#14-general-layer","title":"1.4. General Layer","text":""},{"location":"articles/abstraction-layer/#141-network-abstraction-layer","title":"1.4.1. Network Abstraction Layer","text":"<p>Network Abstraction Layer (NAL) provides an abstraction of the network and communication protocols, allowing software to communicate with various network devices and services.</p> <p>It abstracts the network-specific features, such as:</p> <ul> <li>Packet routing</li> <li>Addressing</li> <li>Socket management</li> </ul>"},{"location":"articles/abstraction-layer/#142-database-abstraction-layer","title":"1.4.2. Database Abstraction Layer","text":"<p>Database Abstraction Layer (DBAL) provides an abstraction of the database management system, allowing software to interact with various types of databases without knowing the underlying database structures and SQL queries.</p> <p>It abstracts the database-specific features, such as:</p> <ul> <li>Schema management</li> <li>Query optimization</li> <li>Transaction management</li> </ul>"},{"location":"articles/abstraction-layer/#2-principle","title":"2. Principle","text":"<ul> <li> <p>Separation of Concerns</p> <p>The Separation of Concerns principle states that abstraction layers should separate different concerns, such as presentation, business logic, and data access. This separation ensures that changes in one layer do not affect other layers. The goal of this principle is to reduce coupling between layers and increase the modularity and maintainability of the system.</p> </li> <li> <p>Encapsulation</p> <p>The Encapsulation principle states that abstraction layers should encapsulate their internal logic and data structures, and expose only the necessary interfaces to other layers or components. This encapsulation ensures that the internal workings of the layer are hidden from the outside world, and that the layer can be modified or replaced without affecting the rest of the system.</p> </li> <li> <p>Modularity</p> <p>The Modularity principle states that abstraction layers should be modular and reusable. Modularity ensures that the layer can be easily adapted and extended for different use cases or applications. Modularity also promotes code reuse and reduces development time and costs.</p> </li> <li> <p>Flexibility</p> <p>The Flexibility principle states that abstraction layers should be designed with flexibility and adaptability in mind. Flexibility ensures that the layer can handle different types of data, platforms, and technologies. This principle also promotes scalability and future-proofing of the system.</p> </li> <li> <p>Standardization</p> <p>The Standardization principle states that abstraction layers should follow established standards and best practices for their domain. Standardization ensures interoperability, consistency, and reliability of the system. This principle also promotes reuse and reduces development costs.</p> </li> <li> <p>Performance</p> <p>The Performance principle states that abstraction layers should be designed to minimize performance overhead and avoid unnecessary processing or data transformation. Performance ensures that the layer does not introduce bottlenecks or slowdowns in the system. This principle also promotes scalability and responsiveness of the system.</p> </li> <li> <p>Testability</p> <p>The Testability principle states that abstraction layers should be testable and verifiable. Testability ensures that the layer is correct, reliable, and consistent. This principle also promotes quality and reduces the risk of defects or failures in the system.</p> </li> </ul>"},{"location":"articles/abstraction-layer/#3-best-practice","title":"3. Best Practice","text":"<ul> <li> <p>Follow a standard design pattern</p> <p>A standard design pattern, such as Model-View-Controller (MVC), can help developers structure their abstraction layers and ensure consistency across the system. Design patterns provide a proven solution to common design problems, and can help developers save time and reduce errors.</p> </li> <li> <p>Use dependency injection</p> <p>Dependency injection allows components to be loosely coupled and promotes modularity and flexibility. By injecting dependencies, rather than creating them directly, developers can ensure that components can be easily replaced or extended without affecting the rest of the system.</p> </li> <li> <p>Provide clear and concise interfaces</p> <p>Abstraction layers should provide clear and concise interfaces that are easy to understand and use. Interfaces should be well-documented and follow established naming conventions, so that they can be easily integrated with other components or systems.</p> </li> <li> <p>Test early and often</p> <p>Testing are an integral part of the development process, and should be performed early and often. Developers should use automated testing tools to ensure that their abstraction layers are correct and reliable, and to catch defects and errors before they become critical issues.</p> </li> </ul>"},{"location":"articles/abstraction-layer/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Interface</p> <p>An interface is the set of methods or properties that a component or system exposes to other components or systems. An interface is an abstraction of the underlying implementation, and provides a standard way of communicating with the component or system.</p> </li> <li> <p>API</p> <p>An API (Application Programming Interface) is a set of interfaces and protocols that enable software components or systems to communicate with each other. An API can be used to access the functionality of another system or component, and can be implemented at various levels of abstraction.</p> </li> <li> <p>Library</p> <p>A library is a collection of pre-built software components that can be used to extend the functionality of a system. Libraries can be used to provide abstraction layers for common tasks or functionality, and can be used across different projects or applications.</p> </li> <li> <p>Framework</p> <p>A framework is a set of rules and conventions that guide the development of a system. A framework can provide abstraction layers for common tasks or functionality, and can be used to structure and organize the components of a system.</p> </li> <li> <p>Middleware</p> <p>Middleware is software that acts as a bridge between different software components or systems. Middleware can provide abstraction layers for different types of data or protocols, and can be used to enable interoperability and integration between different systems.</p> </li> <li> <p>Adapter</p> <p>An adapter is a software component that translates between two different interfaces or data formats. An adapter can be used to provide abstraction layers for different types of data or protocols, and can be used to enable interoperability and integration between different systems.</p> </li> <li> <p>Layer</p> <p>A layer is a modular component of a software system that provides a specific set of functionality or abstraction. Layers can be stacked on top of each other to provide a complete system, and can be used to separate different concerns or levels of abstraction.</p> </li> <li> <p>Abstraction</p> <p>Abstraction refers to the process of hiding complexity and providing a simplified view of a system or component. Abstraction can be used to provide a level of separation between different components or systems, and can help to manage complexity and reduce the risk of errors.</p> </li> <li> <p>Decoupling</p> <p>Decoupling refers to the process of separating different components or systems to reduce interdependencies and promote modularity. Decoupling can be achieved through the use of interfaces, adapters, and other abstraction layers, and can help to improve flexibility and maintainability.</p> </li> <li> <p>Cohesion</p> <p>Cohesion refers to the degree to which the elements of a module or component are related to each other. High cohesion can improve the maintainability and readability of the component, while low cohesion can make the component difficult to understand or modify.</p> </li> <li> <p>Inversion of Control</p> <p>Inversion of Control (IoC) refers to the practice of delegating control over the execution of a system or component to a framework or container. IoC can be used to provide abstraction layers for common tasks or functionality, and can help to reduce the complexity of the system.</p> </li> <li> <p>Service</p> <p>A service is a software component that provides a specific set of functionality or functionality. Services can be accessed through a well-defined interface or API, and can be used to provide abstraction layers for common tasks or functionality.</p> </li> <li> <p>Component</p> <p>A component is a modular unit of a software system that can be independently developed, deployed, and managed. Components can be combined to create a larger system, and can be used to provide abstraction layers for different concerns or levels of functionality.</p> </li> <li> <p>Coupling</p> <p>Coupling refers to the level of interdependence between different components or systems in a software system. High coupling can make the system difficult to modify or maintain, while low coupling can improve flexibility and modularity.</p> </li> </ul>"},{"location":"articles/app-resources/","title":"App Resources","text":"<p>App resources refer to the various assets and components utilized by an application, such as images, icons, sounds, videos, and other media files, as well as configuration files, database connections, external API endpoints, and managing static strings and magic numbers. Efficiently managing these resources is crucial for optimizing the performance and user experience of an application.</p> <ul> <li>1. Category</li> <li>1.1. Resource Management System</li> <li>1.2. Custom Package/File</li> <li>1.3. Constants</li> <li>1.4. Configuration Files</li> <li>1.5. Resource Files</li> <li>1.6. Localization</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/app-resources/#1-category","title":"1. Category","text":""},{"location":"articles/app-resources/#11-resource-management-system","title":"1.1. Resource Management System","text":"<p>The Resource Management System (RMS) is a software component or system that facilitates the management of app resources. It typically provides functionalities to handle resource loading, caching, synchronization, and disposal. The RMS helps streamline the process of accessing and utilizing resources within an application, ensuring they are available when needed and released when no longer required.</p> <p>Resource Management System simplifies the handling and utilization of app resources, leading to improved performance, efficient memory management, and better user experiences. It helps developers focus on the core logic of their applications while abstracting away the complexities of resource management.</p> <p>Benefits of RMS:</p> <ol> <li>Resource Loading</li> </ol> <p>The RMS handles the loading of resources from their storage locations, such as local files or remote servers. It abstracts the complexity of accessing different resource types and provides a unified interface for retrieval.</p> <ol> <li>Caching</li> </ol> <p>The RMS often incorporates caching mechanisms to store frequently used resources in memory, reducing the need for repetitive loading from disk or network. Caching improves performance and reduces latency, especially for large or frequently accessed resources.</p> <ol> <li>Resource Synchronization</li> </ol> <p>In cases where resources are modified or updated, the RMS helps manage synchronization across multiple instances of an application. It ensures that all instances have access to the latest version of a resource, preventing inconsistencies or conflicts.</p> <ol> <li>Memory Management</li> </ol> <p>The RMS helps manage memory usage by controlling the lifecycle of resources. It ensures that resources are efficiently allocated and deallocated, preventing memory leaks or excessive memory consumption.</p> <ol> <li>Resource Disposal</li> </ol> <p>When resources are no longer needed, the RMS facilitates their proper disposal. It releases allocated memory, closes database connections, terminates network connections, or performs other cleanup tasks, freeing up system resources.</p> <ol> <li>Localization and Internationalization</li> </ol> <p>In the context of multi-language support, the RMS can assist in managing localized resources. It enables the loading of language-specific versions of resources based on the user's preferred language or locale.</p> <ol> <li>Resource Dependency Management</li> </ol> <p>Some applications have dependencies between resources, where one resource relies on another. The RMS can handle the resolution and management of these dependencies, ensuring that all required resources are available and loaded in the correct order.</p> <ol> <li>Error Handling and Recovery</li> </ol> <p>The RMS can handle error scenarios related to resource loading, such as missing or corrupted files. It provides error handling mechanisms and enables recovery strategies, such as fallback to default resources or graceful degradation.</p> <p>Example of RMS:</p> <ol> <li> <p>Go</p> <p>To simulate a basic form of resource management in Go, create a custom package that provides functions or methods to access and manage resources.</p> <p>Utilizing a custom package simulates a basic resource management system in Go, allowing to add, retrieve, and remove resources as needed.</p> <ul> <li><code>resources</code></li> </ul> <p>Create a new directory called <code>resources</code> in the project.</p> <ul> <li><code>manager.go</code></li> </ul> <p>Inside the <code>resources</code> directory, create a file named <code>manager.go</code>:</p> <pre><code>package resources\n\nimport (\n    \"sync\"\n)\n\ntype ResourceManager struct {\n    resources map[string]interface{}\n    mutex     sync.Mutex\n}\n\nfunc NewResourceManager() *ResourceManager {\n    return &amp;ResourceManager{\n        resources: make(map[string]interface{}),\n    }\n}\n\nfunc (rm *ResourceManager) AddResource(key string, resource interface{}) {\n    rm.mutex.Lock()\n    defer rm.mutex.Unlock()\n\n    rm.resources[key] = resource\n}\n\nfunc (rm *ResourceManager) GetResource(key string) (interface{}, bool) {\n    rm.mutex.Lock()\n    defer rm.mutex.Unlock()\n\n    resource, ok := rm.resources[key]\n    return resource, ok\n}\n\nfunc (rm *ResourceManager) RemoveResource(key string) {\n    rm.mutex.Lock()\n    defer rm.mutex.Unlock()\n\n    delete(rm.resources, key)\n}\n</code></pre> <ul> <li>Client</li> </ul> <p>In the <code>main.go</code> file (or any other Go file where a RMS is needed), import and use the custom package:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"yourproject/resources\"\n)\n\nfunc main() {\n    rm := resources.NewResourceManager()\n\n    // Add resources\n    rm.AddResource(\"db\", \"Database connection\")\n    rm.AddResource(\"cache\", \"Cache instance\")\n\n    // Get resources\n    db, ok := rm.GetResource(\"db\")\n    if ok {\n        fmt.Println(\"Database:\", db)\n    }\n\n    cache, ok := rm.GetResource(\"cache\")\n    if ok {\n        fmt.Println(\"Cache:\", cache)\n    }\n\n    // Remove a resource\n    rm.RemoveResource(\"db\")\n\n    // Get the removed resource\n    _, ok = rm.GetResource(\"db\")\n    if !ok {\n        fmt.Println(\"Database resource has been removed.\")\n    }\n}\n</code></pre> <p>In this example, the <code>ResourceManager</code> struct manages a map of resources using a mutex to ensure concurrent access safety. The <code>AddResource</code> function adds a new resource to the map, <code>GetResource</code> retrieves a resource by its key, and <code>RemoveResource</code> removes a resource from the map.</p> </li> </ol>"},{"location":"articles/app-resources/#12-custom-packagefile","title":"1.2. Custom Package/File","text":"<p>Custom packages or files are dedicated to storing and managing static strings and magic numbers. By organizing the code in this way, that groups related constants together and separate them from the rest of the codebase. This approach helps with code organization and reduces the chance of duplicating values across the project.</p> <p>Example of Custom Package/File:</p> <ol> <li> <p>Go</p> <p>Organize the code in <code>resource</code> package, in this way related constants and static strings can be grouped together. This separation allows for better code organization, reduces duplication, and makes it easier to manage and modify the values when necessary.</p> <p>NOTE While the example showcases the use of separate files within a custom package, it's essential to strike a balance between code organization and avoiding excessive fragmentation. Always consider the size and complexity of the project when deciding how to structure and separate code.</p> <ul> <li><code>constants.go</code></li> </ul> <p>Inside the <code>resource</code> directory, create a <code>constants.go</code> file.</p> <pre><code>package resource\n\nconst (\n    DefaultPort      = 8080\n    MaxConnections   = 100\n    WelcomeMessage   = \"Welcome to our application!\"\n    AdminEmail       = \"admin@example.com\"\n)\n</code></pre> <ul> <li><code>messages.go</code></li> </ul> <p>Inside the <code>resource</code> directory, create a <code>messages.go</code> file.</p> <pre><code>package resource\n\nconst (\n    ErrorMessage       = \"An error occurred.\"\n    SuccessMessage     = \"Operation completed successfully.\"\n    GreetingMessage    = \"Hello, %s!\"\n)\n</code></pre> <ul> <li>Client</li> </ul> <p>In the <code>main.go</code> file (or any other Go file where resource values need to be used), import and utilize the custom package:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n\n    \"project/resource\"\n)\n\nfunc main() {\n    fmt.Println(resource.DefaultPort)\n    fmt.Println(resource.ErrorMessage)\n    fmt.Printf(resource.GreetingMessage, \"John\")\n}\n</code></pre> </li> <li> <p>C</p> <p>In the example, we define the constants in the <code>resource.c</code> file with the appropriate types, and we declare them as <code>extern</code> in the <code>resource.h</code> header file. By doing so, the constants can be accessed and utilized in other C files that include the <code>resource.h</code> header.</p> <p>This approach provides strict typing for the constants and allows better type checking during compilation. It also separates the constant declarations from their usages, improving code organization and maintainability.</p> <p>NOTE The <code>extern</code> keyword is used to indicate that the variable or constant declaration is an \"external\" reference. It informs the compiler that the actual definition of the variable or constant is located in another source file. In this case, it is used to indicate that the constant definitions exist in the <code>resource.c</code> file.</p> <p>By declaring the constants as <code>extern</code> in the header file, other C files that include the <code>resource.h</code> header can access and use these constants. The actual definitions are resolved during the linking phase of the compilation process.</p> <p>This approach allows for separation of declaration and definition. The header file acts as an interface that exposes the constants to other parts of the program, while the source file provides the actual definitions. It helps in keeping the implementation details hidden and provides a clean way to share constants across multiple source files.</p> <p>Without the <code>extern</code> keyword in the header file, each source file that includes the <code>resource.h</code> header would have its own separate copy of the constants, leading to duplicated definitions and potential linker errors. The <code>extern</code> keyword ensures that there is only one definition of the constants, promoting better code organization and preventing duplication.</p> <ul> <li><code>resource.h</code></li> </ul> <p>Create a header file named <code>resource.h</code>:</p> <pre><code>#ifndef RESOURCE_H\n#define RESOURCE_H\n\nextern const int DefaultPort;\nextern const int MaxConnections;\n\nextern const char* ErrorMessage;\nextern const char* SuccessMessage;\n\n#endif /* RESOURCE_H */\n</code></pre> <ul> <li><code>resource.c</code></li> </ul> <p>Create a source file named <code>resource.c</code>:</p> <pre><code>#include \"resource.h\"\n\nconst int DefaultPort = 8080;\nconst int MaxConnections = 100;\n\nconst char* ErrorMessage = \"An error occurred.\";\nconst char* SuccessMessage = \"Operation completed successfully.\";\n</code></pre> <ul> <li>Client</li> </ul> <p>In the <code>main.c</code> file (or any other C file), include the <code>resource.h</code> header and utilize the constants:</p> <pre><code>#include &lt;stdio.h&gt;\n\n#include \"resource.h\"\n\nint main() {\n    printf(\"%d\\n\", DefaultPort);\n    printf(\"%s\\n\", ErrorMessage);\n\n    return 0;\n}\n</code></pre> </li> </ol>"},{"location":"articles/app-resources/#13-constants","title":"1.3. Constants","text":"<p>Constants are fixed values that do not change during the execution of a program. They are used to represent values that are known and fixed at compile-time. In most programming languages, constants are typically declared and defined using specific syntax and naming conventions.</p> <p>Features of Constants:</p> <ol> <li>Declaration</li> </ol> <p>Constants are typically declared using a specific keyword or syntax provided by the programming language. For example, in languages like Java and C#, the <code>final</code> and <code>const</code> keywords respectively are used to declare constants.</p> <ol> <li>Naming Convention</li> </ol> <p>Constants are often named using uppercase letters with words separated by underscores or by using a combination of uppercase and lowercase letters. This naming convention helps distinguish constants from variables and improves readability.</p> <ol> <li>Value Assignment</li> </ol> <p>Constants are assigned a value at the time of declaration, and this value cannot be changed during program execution. The assigned value is usually a literal, such as a numeric value, string, boolean, or a reference to another constant.</p> <ol> <li>Scope</li> </ol> <p>Constants have a specific scope within which they are accessible and can be used. The scope may be limited to a particular file, a class, a module, or a global scope depending on the programming language and the location of the constant declaration.</p> <ol> <li>Compile-Time Evaluation</li> </ol> <p>Constant values are known and evaluated at compile-time rather than runtime. This means that the constant's value is determined and fixed during the compilation phase, allowing for optimizations and potential performance benefits.</p> <ol> <li>Benefits</li> </ol> <p>Constants provide several benefits in programming, including improved code readability, easier maintenance, prevention of accidental modification of values, and the ability to use constants in expressions and calculations without worrying about their values changing.</p> <p>Example of Constants:</p> <p>Constants define magic numbers and static strings. Constants provide named values that can be used throughout the codebase and make the code more readable and maintainable. Most programming languages support the concept of constants.</p> <ol> <li> <p>Go</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nconst (\n    MaxRetries       = 3\n    DefaultTimeout   = 10\n)\n\nfunc main() {\n    fmt.Println(MaxRetries)\n    fmt.Println(DefaultTimeout)\n}\n</code></pre> </li> <li> <p>Java</p> <pre><code>public class ConstantsExample {\n    public static final int MAX_VALUE = 100;\n    public static final String DEFAULT_MESSAGE = \"Hello, world!\";\n\n    public static void main(String[] args) {\n        System.out.println(\"Maximum value: \" + MAX_VALUE);\n        System.out.println(\"Default message: \" + DEFAULT_MESSAGE);\n    }\n}\n</code></pre> </li> <li> <p>Python</p> <pre><code>MAX_VALUE = 100\nDEFAULT_MESSAGE = \"Hello, world!\"\n\ndef main():\n    print(\"Maximum value:\", MAX_VALUE)\n    print(\"Default message:\", DEFAULT_MESSAGE)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </li> <li> <p>C++</p> <pre><code>#include &lt;iostream&gt;\n\nconst int MAX_VALUE = 100;\nconst std::string DEFAULT_MESSAGE = \"Hello, world!\";\n\nint main() {\n    std::cout &lt;&lt; \"Maximum value: \" &lt;&lt; MAX_VALUE &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Default message: \" &lt;&lt; DEFAULT_MESSAGE &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre> </li> </ol>"},{"location":"articles/app-resources/#14-configuration-files","title":"1.4. Configuration Files","text":"<p>Configuration files are files used to store settings, parameters, and options that configure the behavior of an application or system. These files typically contain key-value pairs or structured data that define various aspects of the application's functionality.</p> <p>Configuration files play a crucial role in software development and administration as they provide a flexible way to customize and control the behavior of an application without modifying its source code.</p> <p>Configuration files provide a flexible and powerful mechanism for managing application settings and behavior. They enable easy customization, improve maintainability, and facilitate the separation of configuration from code, allowing for greater flexibility and adaptability in different environments and deployment scenarios.</p> <p>Features of Configuration Files:</p> <ol> <li> <p>Purpose</p> <p>Configuration files are used to store configuration settings and parameters that determine how an application or system should behave. These settings can include options such as database connection details, network settings, logging levels, feature flags, security settings, and more.</p> </li> <li> <p>Formats</p> <p>Configuration files can be written in various formats depending on the requirements and conventions of the application or system. Common formats include plain text formats (such as INI, XML, JSON, YAML), property files (such as Java properties files), or specific configuration formats designed for a particular framework or technology.</p> </li> <li> <p>Location</p> <p>Configuration files are typically stored as separate files outside the application's source code. The exact location and naming conventions for configuration files can vary depending on the operating system, framework, or development practices. Common locations include a dedicated \"config\" directory, a specific location within the user's home directory, or in a central configuration repository.</p> </li> <li> <p>Readability and Modifiability</p> <p>Configuration files are designed to be human-readable and easily modifiable. They should provide a clear and understandable representation of the configuration settings, allowing administrators or users to modify them without requiring knowledge of the underlying codebase.</p> </li> <li> <p>Parsing and Loading</p> <p>To make use of the configuration settings, the application needs to parse and load the configuration files at runtime. The parsing process involves reading the file and extracting the configuration values into memory for the application to access and utilize.</p> </li> <li> <p>Overrides and Hierarchies</p> <p>Configuration files can support hierarchical structures or inheritance, allowing for the definition of default settings that can be overridden or extended by subsequent configuration files. This allows for a modular approach to configuration and simplifies managing configuration variations for different environments or deployment scenarios.</p> </li> <li> <p>Security Considerations</p> <p>Care should be taken when storing sensitive information, such as passwords or access keys, in configuration files. It is important to protect configuration files from unauthorized access and encryption or obfuscation techniques may be used to secure sensitive data.</p> </li> <li> <p>Reloadability</p> <p>Some applications provide the ability to dynamically reload configuration files without restarting the application. This feature allows for runtime configuration changes and avoids the need for application restarts when modifying settings.</p> </li> </ol> <p>Best Practices of Configuration Files:</p> <p>Applying best practices enhance the reliability, maintainability, and flexibility of the configuration design pattern, making it easier to manage and utilize configuration values within the application.</p> <ol> <li> <p>Validation and Default Values</p> <p>Add validation logic to ensure that the loaded configuration values meet the expected criteria. Validate data types, required fields, or custom validation rules. Additionally, consider setting default values for configuration properties that are optional but don't have explicit values in the configuration file.</p> </li> <li> <p>Separation of Concerns</p> <p>Ensure that the configuration package focuses solely on loading and providing configuration values. Avoid adding business logic or unrelated functionality to the configuration package. Keep the responsibility of configuration loading separate from other parts of the application.</p> </li> <li> <p>Immutable Configurations</p> <p>Consider making the <code>Config</code> struct immutable after it is loaded to prevent accidental modification of the configuration values during runtime. This can achieve by making the fields of the <code>Config</code> struct read-only or providing only getter methods to access the configuration properties.</p> </li> <li> <p>Error Handling</p> <p>Implement proper error handling when loading and parsing the configuration file. Consider returning meaningful error messages or wrapping errors with additional context to aid in troubleshooting and debugging.</p> </li> <li> <p>Configuration Reload</p> <p>If an application requires the ability to reload the configuration during runtime, consider adding a method or functionality to refresh the configuration values. This can be useful in situations where its need to update the configuration without restarting the entire application.</p> </li> <li> <p>Unit Testing</p> <p>Write unit tests for the configuration loading and retrieval logic. Test different scenarios, such as missing configuration values, incorrect data types, or valid configurations. Unit tests can help ensure that the configuration package functions correctly and provides the expected values.</p> </li> <li> <p>Documentation</p> <p>Document the purpose and usage of the configuration package, including any assumptions or requirements. Clearly define the configuration properties, their expected values, and any constraints or dependencies.</p> </li> </ol> <p>Example of Configuration Files:</p> <ol> <li> <p>Go</p> <p>The example code follows best practices such as separation of concerns, error handling, validation, and reloading of the configuration. It also provides a clean and straightforward API for accessing the configuration throughout the application.</p> <ul> <li><code>config.json</code></li> </ul> <p>Create a configuration file named <code>.env</code>:</p> <pre><code>DB_DRIVER=postgres\nDB_URI=postgresql://root:secret@localhost:5432/crypto?sslmode=disable\n</code></pre> <ul> <li>Config Library</li> </ul> <p>In the Go code, install the <code>github.com/spf13/viper</code> package by running:</p> <pre><code>go get github.com/spf13/viper\n</code></pre> <ul> <li><code>properties.go</code></li> </ul> <p>In this example, <code>Properties</code> is a struct that represents the properties of the configuration, and it provides getter methods to access the respective values. Separating the <code>Properties</code> struct and its methods into a separate file allows to have better organization and encapsulation of the configuration properties and related functionality.</p> <pre><code>package config\n\nimport \"github.com/go-playground/validator/v10\"\n\n// Properties defines the properties of the configuration.\ntype (\n Properties struct {\n  DatabaseURI    string `mapstructure:\"DB_URI\" validate:\"required\"`\n  DatabaseDriver string `mapstructure:\"DB_DRIVER\" validate:\"required\"`\n }\n)\n\n// Validate validates the EnvProperties.\nfunc (p *Properties) Validate() error {\n validate := validator.New()\n\n return validate.Struct(p)\n}\n\n// GetDatabaseURI returns the database URI.\nfunc (p *Properties) GetDatabaseURI() string {\n return p.DatabaseURI\n}\n\n// GetDBDriver returns the application port.\nfunc (p *Properties) GetDatabaseDriver() string {\n return p.DatabaseDriver\n}\n</code></pre> <ul> <li><code>config.go</code></li> </ul> <p>This code implements a configuration package that handles loading and managing application configurations using the Viper library. Here's a breakdown of the code:</p> <ul> <li> <p>The package uses a singleton pattern to ensure that only one instance of the <code>Config</code> struct is created and accessed throughout the application.</p> </li> <li> <p>The <code>New</code> function returns a new instance of the <code>Config</code> struct.</p> </li> <li> <p>The <code>GetInstance</code> method returns the singleton instance of the <code>Config</code> struct. It uses the <code>once.Do</code> function to ensure that the instance is loaded only once.</p> </li> <li> <p>The <code>load</code> function loads the configuration from the specified file. It uses Viper to read and unmarshal the configuration file.</p> </li> <li> <p>The <code>validate</code> function performs validation using the Go Playground Validator.</p> </li> <li> <p>The <code>reload</code> function reloads the configuration from the config file. It unmarshals the configuration into the instance.</p> </li> <li> <p>The <code>watch</code> function sets up a configuration file watcher to trigger reloads on changes.</p> </li> <li> <p>The <code>getFilepath</code> function returns the absolute file path for the config file by getting the current working directory and joining it with the provided filename.</p> </li> </ul> <pre><code>package config\n\nimport (\n  \"fmt\"\n  \"os/exec\"\n  \"path/filepath\"\n  \"strings\"\n  \"sync\"\n\n  \"github.com/fsnotify/fsnotify\"\n  \"github.com/spf13/viper\"\n)\n\n// Config represents the configuration structure.\ntype Config struct {\n  Properties *Properties\n}\n\nvar (\n  once     sync.Once\n  instance *Config\n)\n\n// New creates a new instance of the Config struct.\nfunc New() *Config {\n  return &amp;Config{\n    Properties: &amp;Properties{},\n  }\n}\n\n// GetInstance returns the singleton instance of the Config struct, loaded from the specified\n// configuration file. It ensures that the configuration is loaded only once, and handles the\n// verification and watching of the config file.\nfunc (c *Config) GetInstance(filename string) *Config {\n  once.Do(func() {\n    var err error\n    instance, err = load(filename)\n    if err != nil {\n      instance = nil\n\n      return\n    }\n\n    if err := validate(instance); err != nil {\n      instance = nil\n\n      return\n    }\n\n    watch()\n  })\n\n  return instance\n}\n\n// load loads the configuration from the specified file.\nfunc load(filename string) (*Config, error) {\n  filepath, err := getFilepath(filename)\n  if err != nil {\n    return nil, err\n  }\n\n  viper.SetConfigFile(filepath)\n\n  if err := viper.ReadInConfig(); err != nil {\n    return nil, fmt.Errorf(\"read configuration file: %v\", err)\n  }\n\n  cfg := &amp;Config{\n    Properties: &amp;Properties{},\n  }\n\n  if err := viper.Unmarshal(cfg.Properties); err != nil {\n    return nil, fmt.Errorf(\"unmarshal configuration: %v\", err)\n  }\n\n  return cfg, nil\n}\n\n// validate validates the loaded configuration.\nfunc validate(cfg *Config) error {\n  if err := cfg.Properties.Validate(); err != nil {\n    return fmt.Errorf(\"validate configuration: %v\", err)\n  }\n\n  return nil\n}\n\n// watch starts watching for changes in the config file and triggers a reload when a change occurs.\nfunc watch() {\n  viper.OnConfigChange(func(e fsnotify.Event) {\n    if err := reload(); err == nil {\n      fmt.Errorf(\"watched file changed: %v\", e.Name)\n    }\n  })\n  viper.WatchConfig()\n}\n\n// reload reloads the configuration from the config file.\nfunc reload() error {\n  if err := viper.Unmarshal(instance.Properties); err != nil {\n    return fmt.Errorf(\"unmarshal configuration: %v\", err)\n  }\n\n  return nil\n}\n\n// getFilepath returns the root path for the config file.\nfunc getFilepath(filename string) (string, error) {\n  output, err := exec.Command(\"git\", \"rev-parse\", \"--show-toplevel\").Output()\n  if err != nil {\n    return \"\", fmt.Errorf(\"project root directory: %v\", err)\n  }\n\n  rootpath := strings.TrimSpace(string(output))\n\n  return filepath.Join(rootpath, filename), nil\n}\n</code></pre> </li> <li> <p>C</p> <ul> <li><code>config.txt</code></li> </ul> <p>Create a configuration file named <code>config.txt</code>:</p> <pre><code>maxRetries=3\ndefaultTimeout=10\n</code></pre> <ul> <li><code>config.h</code></li> </ul> <p>Create a header file named <code>config.h</code>:</p> <pre><code>#ifndef CONFIG_H\n#define CONFIG_H\n\ntypedef struct {\n    int maxRetries;\n    int defaultTimeout;\n} Config;\n\nvoid loadConfig(const char* filename, Config* config);\n\n#endif\n</code></pre> <ul> <li><code>config.c</code></li> </ul> <p>Create a source file named <code>config.c</code>:</p> <pre><code>#include &lt;stdio.h&gt;\n\n#include \"config.h\"\n\nvoid loadConfig(const char* filename, Config* config) {\n    FILE* file = fopen(filename, \"r\");\n    if (file == NULL) {\n      fprintf(stderr, \"Failed to open configuration file: %s\\n\", filename);\n      return;\n    }\n\n  char key[256];\n  int value;\n\n  while (fscanf(file, \"%255[^=]=%d\\n\", key, &amp;value) == 2) {\n    if (strcmp(key, \"maxRetries\") == 0) {\n        config-&gt;maxRetries = value;\n      } else if (strcmp(key, \"defaultTimeout\") == 0) {\n        config-&gt;defaultTimeout = value;\n      }\n    }\n\n    fclose(file);\n}\n</code></pre> <ul> <li> <p>Client</p> <p>In the <code>main.c</code> file, include the header file and use the configuration values:</p> <pre><code>#include &lt;stdio.h&gt;\n\n#include \"config.h\"\n\nint main() {\n    Config config;\n    loadConfig(\"config.txt\", &amp;config);\n\n    // Access configuration values\n    printf(\"Max Retries: %d\\n\", config.maxRetries);\n    printf(\"Default Timeout: %d\\n\", config.defaultTimeout);\n\n    // Use the configuration values in the code\n    // ...\n\n    return 0;\n}\n</code></pre> <p>In this example, we define a struct <code>Config</code> to hold the configuration values. The <code>loadConfig</code> function reads the configuration file line by line, parses the key-value pairs, and assigns the values to the appropriate struct members.</p> <p>This example, loads the configuration file, parse the values, and access the magic numbers and static strings using the struct members.</p> <p>NOTE Error handling has been omitted for brevity in this example. In the actual code, make sure to handle potential errors while opening and reading the configuration file.</p> </li> </ul> </li> </ol>"},{"location":"articles/app-resources/#15-resource-files","title":"1.5. Resource Files","text":"<p>Resource files are files used to store non-code assets or data that are used by an application. These files often include images, icons, audio files, video files, configuration files, localization files, templates, and other types of static content that are needed by the application to provide its intended functionality or enhance the user experience.</p> <p>Resource files are typically separate from the source code and are bundled with the application during the build process. They can be accessed and utilized by the application at runtime.</p> <p>Resource files are an essential component of many applications, enabling the storage and access of non-code assets that are necessary for the application's proper functioning or to enhance the user experience. Proper organization, management, and utilization of resource files contribute to the overall effectiveness and efficiency of the application.</p> <p>Features of Resource Files:</p> <ol> <li>Types of Resource Files</li> </ol> <p>Resource files can encompass a wide range of file types and formats, depending on the specific requirements of the application. Some common examples include image files (JPEG, PNG, SVG), audio files (MP3, WAV), video files (MP4, AVI), configuration files (INI, XML, JSON), and localization files (properties files, language-specific resource files).</p> <ol> <li>Storage Location</li> </ol> <p>Resource files are often stored in a specific directory or folder within the application's project structure. The exact location and naming conventions for resource files can vary depending on the programming language, framework, or development practices. Common directories for resource files include an \"assets\" folder, \"resources\" folder, or a dedicated folder structure based on the file types.</p> <ol> <li>Accessing Resource Files</li> </ol> <p>Applications access resource files programmatically by using file I/O operations or through higher-level abstractions provided by the programming language or framework. The exact methods for accessing resource files depend on the platform and programming language being used.</p> <ol> <li>Packaging and Distribution</li> </ol> <p>During the build process, resource files are typically packaged and distributed along with the application's executable or bundled into a deployable package, such as a JAR file, WAR file, or an application bundle. This ensures that the necessary resources are available to the application when it is deployed or distributed to end-users.</p> <ol> <li>Localization and Internationalization</li> </ol> <p>Resource files play a critical role in supporting localization and internationalization efforts. They allow for the storage of translated text, localized media, and other resources specific to different languages, regions, or cultural requirements. Localization frameworks and libraries often provide mechanisms for managing and accessing these localized resources.</p> <ol> <li>File Compression and Optimization</li> </ol> <p>Depending on the size and type of resource files, compression techniques and optimization processes may be applied to reduce file size and improve performance. This can include image compression, audio and video encoding, minification of text-based resources, and other optimization techniques.</p> <ol> <li>Updating and Managing Resource Files</li> </ol> <p>Resource files may require updates or changes during the application's lifecycle. Developers and administrators should establish processes and tools to manage the versioning, updating, and synchronization of resource files to ensure that the application remains up to date and consistent with the intended functionality.</p> <p>Example of Resource Files:</p> <p>Create resource files in a language-agnostic manner. These files can be in any format suitable for the project, such as XML, JSON, or YAML. Define key-value pairs or structures in these files to represent the static strings or values. Load and use these resources in the code.</p> <ol> <li> <p>Go</p> <ul> <li><code>resources.json</code></li> </ul> <p>Let's assume we have a JSON resource file named <code>resources.json</code>:</p> <pre><code>{\n    \"ErrorMessage\": \"An error occurred.\",\n    \"SuccessMessage\": \"Operation completed successfully.\"\n}\n</code></pre> <ul> <li>Client</li> </ul> <pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n)\n\ntype Resources struct {\n    ErrorMessage   string `json:\"ErrorMessage\"`\n    SuccessMessage string `json:\"SuccessMessage\"`\n}\n\nfunc main() {\n    data, err := ioutil.ReadFile(\"resources.json\")\n    if err != nil {\n      fmt.Println(\"Error reading resource file:\", err)\n      return\n    }\n\n    var resources Resources\n    err = json.Unmarshal(data, &amp;resources)\n    if err != nil {\n      fmt.Println(\"Error parsing resource file:\", err)\n      return\n    }\n\n    fmt.Println(resources.ErrorMessage)\n    fmt.Println(resources.SuccessMessage)\n}\n</code></pre> </li> </ol>"},{"location":"articles/app-resources/#16-localization","title":"1.6. Localization","text":"<p>Localization, often abbreviated as l10n (where \"10\" represents the number of omitted letters between \"l\" and \"n\"), is the process of adapting an application, software, or content to the linguistic, cultural, and regional requirements of a specific target audience or locale. Localization involves making an application or content accessible and culturally appropriate for users in different countries, regions, or language communities.</p> <p>Localization is a complex process that requires collaboration between translators, localization engineers, designers, and cultural experts. It goes beyond mere language translation and aims to create a seamless user experience that feels natural and relevant to users in different regions.</p> <p>Localization frameworks, tools, and technologies are available to streamline the localization process and help manage translations, resource files, and language-specific assets effectively. These tools often provide features for string extraction, translation management, and resource file generation to simplify the localization workflow.</p> <p>Features of Localization:</p> <ol> <li>Language Translation</li> </ol> <p>Translating the user interface (UI) elements, such as menus, buttons, labels, messages, and other textual content, into the target language(s). This ensures that users can interact with the application in their preferred language.</p> <ol> <li>Date and Time Formats</li> </ol> <p>Adapting the date and time formats to match the conventions used in the target locale. This includes formatting dates, times, and time zones according to the regional preferences.</p> <ol> <li>Number and Currency Formats</li> </ol> <p>Modifying number and currency formats to align with the local conventions. This involves adapting decimal separators, thousand separators, currency symbols, and other numeric representations.</p> <ol> <li>Units of Measurement</li> </ol> <p>Converting units of measurement, such as length, weight, temperature, and volume, to the units commonly used in the target locale. For example, displaying distances in kilometers instead of miles in a country that uses the metric system.</p> <ol> <li>Cultural Sensitivity</li> </ol> <p>Adhering to cultural norms, customs, and sensitivities in the target locale. This includes considerations for icons, colors, images, symbols, and any visual or design elements that might have cultural significance.</p> <ol> <li>Legal and Regulatory Compliance</li> </ol> <p>Ensuring that the application adheres to the legal, regulatory, and industry-specific requirements of the target locale. This may involve modifications to terms of service, privacy policies, or compliance with specific data protection laws.</p> <ol> <li>Localized Content</li> </ol> <p>Adapting content specific to the target locale, such as region-specific offerings, marketing messages, or localized documentation.</p> <p>Example of Localization:</p> <p>If static strings need to be localized for different languages, adopt a localization framework or library that allows to manage language-specific resources. This enables to handle translations of static strings without hard-coding them directly in the code.</p> <p>Language-specific resource files containing translations and localized content are commonly stored in a dedicated directory such as <code>resources/locale</code> or <code>resources/i18n</code>.</p> <ol> <li> <p>Go</p> <ul> <li>Localization Resource Files</li> </ul> <p>Let's assume we have localization resource files for English and French named <code>en.json</code> and <code>fr.json</code> respectively:</p> <p><code>en.json</code>:</p> <pre><code>{\n    \"Greeting\": \"Hello!\"\n}\n</code></pre> <p><code>fr.json</code>:</p> <pre><code>{\n    \"Greeting\": \"Bonjour!\"\n}\n</code></pre> <ul> <li>Client</li> </ul> <pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n)\n\ntype Localization struct {\n    Greeting string `json:\"Greeting\"`\n}\n\nfunc main() {\n    lang := \"en\" // Assuming language selection\n\n    data, err := ioutil.ReadFile(lang + \".json\")\n    if err != nil {\n      fmt.Println(\"Error reading localization file:\", err)\n      return\n    }\n\n    var localization Localization\n    err = json.Unmarshal(data, &amp;localization)\n    if err != nil {\n      fmt.Println(\"Error parsing localization file:\", err)\n      return\n    }\n\n    fmt.Println(localization.Greeting)\n}\n</code></pre> </li> </ol>"},{"location":"articles/app-resources/#2-principles","title":"2. Principles","text":"<p>Principles guide developers in ensuring efficient and effective utilization of resources. Principles create applications that effectively manage resources, optimize performance, and provide a seamless user experience while maintaining scalability, reliability, and security.</p> <ul> <li> <p>Optimize Resource Usage</p> <p>Strive to use resources efficiently and minimize waste. This involves techniques such as resource pooling, lazy loading, and caching to reduce redundant loading and unnecessary consumption of system resources.</p> </li> <li> <p>Lifecycle Management</p> <p>Properly manage the lifecycle of resources, including their creation, usage, and disposal. Resources should be acquired when needed, released when no longer required, and properly cleaned up to avoid memory leaks or resource exhaustion.</p> </li> <li> <p>Prioritize Critical Resources</p> <p>Identify and prioritize critical resources that have a significant impact on the application's performance or user experience. Allocate resources strategically, giving priority to those that are most essential for the smooth functioning of the application.</p> </li> <li> <p>Scalability and Performance</p> <p>Design resource management strategies that can scale with the application's growth. Consider factors such as resource caching, asynchronous loading, and distributed resource management to handle increasing load and ensure optimal performance.</p> </li> <li> <p>Error Handling and Recovery</p> <p>Implement robust error handling mechanisms to handle resource-related errors and failures gracefully. This includes proper error reporting, fallback strategies, and recovery mechanisms to maintain the application's stability and functionality.</p> </li> <li> <p>Compatibility and Versioning</p> <p>Consider compatibility and versioning issues when managing resources. Ensure that resources are compatible with different platforms, devices, and software versions. Manage resource versions effectively to handle updates, backward compatibility, and potential conflicts.</p> </li> <li> <p>Security and Access Control</p> <p>Implement appropriate security measures when accessing and managing resources. Authenticate and authorize access to sensitive resources, protect against unauthorized access or data breaches, and ensure data integrity and confidentiality.</p> </li> <li> <p>Monitoring and Analytics</p> <p>Implement monitoring and analytics capabilities to gain insights into resource usage, performance bottlenecks, and potential optimization opportunities. This allows for data-driven decision-making and continuous improvement in resource management.</p> </li> <li> <p>Maintainability and Modularity</p> <p>Design resource management systems that are modular and maintainable. Ensure clear separation of concerns, modular resource loading and unloading, and well-defined interfaces to promote code reusability and ease of maintenance.</p> </li> <li> <p>Testing and Performance Optimization</p> <p>Thoroughly test resource management mechanisms and performance-tune resource-intensive operations. Use profiling tools, load testing, and performance monitoring to identify and address any bottlenecks or inefficiencies in resource utilization.</p> </li> </ul>"},{"location":"articles/app-resources/#3-best-practice","title":"3. Best Practice","text":"<p>Best practices ensures efficient and reliable utilization of resources. Adhering best practices ensure effective resource management, improve application performance, reduce resource wastage, and provide a smooth and optimized user experience.</p> <ul> <li> <p>Use Resource Bundling</p> <p>Bundle resources together to reduce the number of individual resource requests. For example, combine CSS and JavaScript files into minified and compressed bundles to minimize network overhead and improve loading times.</p> </li> <li> <p>Implement Caching</p> <p>Utilize caching mechanisms to store and retrieve frequently used resources. This can include browser caching, server-side caching, or content delivery networks (CDNs). Caching reduces the need for repetitive resource loading and improves overall performance.</p> </li> <li> <p>Optimize Resource Sizes</p> <p>Compress and optimize resource sizes to minimize bandwidth usage and improve loading times. Techniques such as image compression, minification of CSS and JavaScript, and gzip compression for server responses can significantly reduce resource sizes.</p> </li> <li> <p>Lazy Loading</p> <p>Employ lazy loading techniques, especially for resources that are not immediately required. Lazy loading defers the loading of non-critical resources until they are needed, reducing initial page load times and conserving network bandwidth.</p> </li> <li> <p>Dispose of Unused Resources</p> <p>Properly dispose of resources when they are no longer needed to free up memory and system resources. This applies to objects, database connections, network connections, and any other resources that are acquired during runtime.</p> </li> <li> <p>Use Resource Pools</p> <p>Implement resource pooling techniques to manage reusable resources effectively. Resource pools can be used for database connections, thread pools, or any other resources that are costly to create and destroy. Reusing resources from a pool can improve performance and reduce overhead.</p> </li> <li> <p>Monitor Resource Usage</p> <p>Implement monitoring mechanisms to track resource usage and identify bottlenecks or inefficiencies. Monitoring helps identify resource-heavy operations, memory leaks, or excessive resource consumption, allowing for timely optimizations and improvements.</p> </li> <li> <p>Handle Errors and Failures</p> <p>Implement robust error handling and recovery strategies for resource-related operations. Gracefully handle errors, provide appropriate error messages, and implement fallback mechanisms to maintain application stability and user experience.</p> </li> <li> <p>Plan for Scalability</p> <p>Design resource management systems that can scale with increased demand. Consider factors such as distributed resource management, load balancing, and horizontal scaling to handle larger user bases or growing data volumes.</p> </li> <li> <p>Regularly Review and Optimize</p> <p>Continuously review and optimize resource management strategies based on usage patterns, feedback, and performance metrics. Regularly analyze resource utilization, identify areas for improvement, and apply optimizations to enhance efficiency and user experience.</p> </li> </ul>"},{"location":"articles/app-resources/#4-terminology","title":"4. Terminology","text":"<p>Understanding and using terminology help facilitate effective communication and implementation of resource management strategies in application development.</p> <ul> <li> <p>Resource</p> <p>A generic term referring to any asset or component used by an application, such as images, sounds, configuration files, database connections, etc.</p> </li> <li> <p>Resource Management</p> <p>The process of handling and optimizing the allocation, utilization, and disposal of resources within an application.</p> </li> <li> <p>Resource Management System (RMS)</p> <p>A software component or system that facilitates the management of app resources. It provides functionalities for loading, caching, synchronization, and disposal of resources.</p> </li> <li> <p>Resource Loading</p> <p>The process of retrieving resources from their storage locations, such as disk or network, and making them available for use in an application.</p> </li> <li> <p>Resource Caching</p> <p>The technique of storing frequently used resources in memory or other fast-access storage to improve performance and reduce the need for repetitive loading.</p> </li> <li> <p>Resource Synchronization</p> <p>The process of managing the availability and consistency of resources across multiple instances or components of an application.</p> </li> <li> <p>Memory Management</p> <p>The management of system memory, including the allocation and deallocation of memory for resources, to prevent memory leaks and optimize memory usage.</p> </li> <li> <p>Dependency Manager</p> <p>The handling of dependencies between resources or software components, ensuring that all required resources are available and loaded in the correct order.</p> </li> <li> <p>Lazy Loading</p> <p>A technique that defers the loading of resources until they are actually needed, reducing initial loading times and conserving network bandwidth.</p> </li> <li> <p>Resource Pooling</p> <p>The practice of creating and managing a pool of reusable resources, such as database connections or threads, to improve performance and efficiency.</p> </li> <li> <p>Localization</p> <p>The process of adapting an application to different languages, cultures, or regions. It involves managing language-specific resources and translations.</p> </li> <li> <p>Error Handling</p> <p>The handling of errors and exceptions that occur during resource loading, utilization, or disposal. Proper error handling ensures graceful recovery and prevents application crashes or instability.</p> </li> <li> <p>Scalability</p> <p>The ability of an application or resource management system to handle increased load or demand. It involves designing systems that can scale horizontally or vertically to accommodate growing resource requirements.</p> </li> <li> <p>Performance Optimization</p> <p>The process of improving the performance of resource-intensive operations through techniques such as caching, compression, parallelization, and algorithmic optimizations.</p> </li> <li> <p>Monitoring and Analytics</p> <p>The practice of collecting and analyzing data on resource usage, performance metrics, and other relevant information to gain insights and make data-driven decisions for resource management optimizations.</p> </li> </ul>"},{"location":"articles/app-resources/#5-references","title":"5. References","text":"<ul> <li>Android app resources article.</li> <li>Android resource types article.</li> <li>Microsoft app resources article.</li> </ul>"},{"location":"articles/architectural-pattern/","title":"Architectural Pattern","text":"<p>An architectural pattern is a general, reusable solution to a commonly occurring problem in software architecture within a given context.</p> <ul> <li>1. MVVM</li> <li>2. MVP</li> <li>3. MVC</li> <li>4. References</li> </ul>"},{"location":"articles/architectural-pattern/#1-mvvm","title":"1. MVVM","text":"<p>Model View ViewModel (MVVM) is a software architectural pattern that facilitates the separation of the development of the graphical user interface (the view) \u2013 be it via a markup language or GUI code \u2013 from the development of the business logic or back-end logic (the model) so that the view is not dependent on any specific model platform. The viewmodel of MVVM is a value converter, meaning the viewmodel is responsible for exposing (converting) the data objects from the model in such a way that objects are easily managed and presented. In this respect, the viewmodel is more model than view, and handles most if not all of the view's display logic. The viewmodel may implement a mediator pattern, organizing access to the back-end logic around the set of use cases supported by the view.</p> <p>MVVM is a variation of Martin Fowler's Presentation Model design pattern. It was invented by Microsoft architects Ken Cooper and Ted Peters specifically to simplify event-driven programming of user interfaces. The pattern was incorporated into Windows Presentation Foundation (WPF) (Microsoft's .NET graphics system) and Silverlight (WPF's Internet application derivative). John Gossman, one of Microsoft's WPF and Silverlight architects, announced MVVM on his blog in 2005.</p> <p>Model\u2013view\u2013viewmodel is also referred to as model\u2013view\u2013binder, especially in implementations not involving the .NET platform. ZK (a web application framework written in Java) and KnockoutJS (a JavaScript library) use model\u2013view\u2013binder.</p> <p>Components of MVVM pattern:</p> <ul> <li> <p>Model</p> <p>Model refers either to a domain model, which represents real state content (an object-oriented approach), or to the data access layer, which represents content (a data-centric approach).</p> </li> <li> <p>View</p> <p>As in the model\u2013view\u2013controller (MVC) and model\u2013view\u2013presenter (MVP) patterns, the view is the structure, layout, and appearance of what a user sees on the screen. It displays a representation of the model and receives the user's interaction with the view (mouse clicks, keyboard input, screen tap gestures, etc.), and it forwards the handling of these to the view model via the data binding (properties, event callbacks, etc.) that is defined to link the view and view model.</p> </li> <li> <p>View model</p> <p>The view model is an abstraction of the view exposing public properties and commands. Instead of the controller of the MVC pattern, or the presenter of the MVP pattern, MVVM has a binder, which automates communication between the view and its bound properties in the view model. The view model has been described as a state of the data in the model. The main difference between the view model and the Presenter in the MVP pattern is that the presenter has a reference to a view, whereas the view model does not. Instead, a view directly binds to properties on the view model to send and receive updates. To function efficiently, this requires a binding technology or generating boilerplate code to do the binding.</p> </li> <li> <p>Binder</p> <p>Declarative data and command-binding are implicit in the MVVM pattern. In the Microsoft solution stack, the binder is a markup language called XAML. The binder frees the developer from being obliged to write boiler-plate logic to synchronize the view model and view. When implemented outside of the Microsoft stack, the presence of a declarative data binding technology is what makes this pattern possible, and without a binder, one would typically use MVP or MVC instead and have to write more boilerplate (or generate it with some other tool).</p> </li> </ul>"},{"location":"articles/architectural-pattern/#2-mvp","title":"2. MVP","text":"<p>Model View Presenter (MVP) is a software architectural pattern that emerged from the Model View Controller (MVC). It describes a new approach to completely separate the model and the view and to connect them via a presenter. Besides a clearly improved testability also the stricter separation of the individual components in contrast to MVC stands in the foreground.</p> <p>For the first time this design pattern was used and called in the 1990er years of IBM and Taligent. Martin Fowler formulated however in the year 2004 model-view-presenter after its understanding. His definition is decisive today.</p> <p>Components of MVP pattern:</p> <ul> <li> <p>Model</p> <p>The model represents the logic of the view. This can also be the business logic. However, all functionality must be accessible via the model in order to operate the view. The presenter alone controls the model. The model itself knows neither the view nor the presenter.</p> </li> <li> <p>View</p> <p>The view does not contain any controlling logic and is solely responsible for the presentation and the inputs and outputs. It does not get access to the functionality of the presenter nor to the model. All control of the view is done by the presenter.</p> </li> <li> <p>Presenter</p> <p>The presenter is the link between model and view. It controls the logical flows between the other two layers and ensures that the view can fulfill its functionality. For the presenter, interfaces are used for model and view respectively. The interfaces define the exact structure of the two layers and the presenter merely links the interfaces with each other. This ensures the complete interchangeability and reusability of Model and View.</p> </li> </ul>"},{"location":"articles/architectural-pattern/#3-mvc","title":"3. MVC","text":"<p>Model View Controller (MVC) is a software architectural pattern commonly used for developing user interfaces that divide the related program logic into three interconnected elements. This is done to separate internal representations of information from the ways information is presented to and accepted from the user.</p> <p>Traditionally used for desktop graphical user interfaces (GUIs), this pattern became popular for designing web applications. Popular programming languages have MVC frameworks that facilitate implementation of the pattern.</p> <p>Components of MVC pattern:</p> <ul> <li> <p>Model</p> <p>The central component of the pattern. It is the application's dynamic data structure, independent of the user interface. The model is responsible for managing the data, logic and rules of the application. It receives user input from the controller.</p> </li> <li> <p>View</p> <p>Any representation of information such as a chart, diagram or table. Multiple views of the same information are possible, such as a bar chart for management and a tabular view for accountants. The view renders presentation of the model in a particular format.</p> </li> <li> <p>Controller</p> <p>The controller responds to the user input and performs interactions on the data model objects. The controller receives the input, optionally validates it and then passes the input to the model or view.</p> </li> </ul>"},{"location":"articles/architectural-pattern/#4-references","title":"4. References","text":"<ul> <li>Wikipedia MVVM article.</li> <li>Wikipedia MVP article.</li> <li>Wikipedia MVC article.</li> </ul>"},{"location":"articles/architecture-decision-records/","title":"Architecture Decision Records","text":"<p>Architecture Decision Records (ADRs) are a mechanism for documenting significant architectural decisions made during a software project. They are essentially a form of technical documentation that captures the reasoning behind the decision-making process and provides a historical record of how the project evolved over time.</p> <p>The purpose of ADRs is to ensure that important architectural decisions are properly recorded and communicated to other members of the development team, as well as stakeholders and future maintainers of the software. By creating a repository of ADRs, teams can establish a shared understanding of the architecture and the rationale behind it, which can help to maintain consistency, coherence and facilitate collaboration.</p> <p>ADRs are a valuable tool for managing the complexity of software development, providing a structure to document and communicate architectural decisions. ADRs establish a clear understanding of the architecture and ensure that important decisions are properly recorded and communicated, ultimately leading to more effective and efficient development processes.</p> <ul> <li>1. Category</li> <li>1.1. Structure</li> <li>1.2. Naming Convention</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/architecture-decision-records/#1-category","title":"1. Category","text":""},{"location":"articles/architecture-decision-records/#11-structure","title":"1.1. Structure","text":"<p>The structure of an ADR follows a standardized format to ensure that the key information is captured and communicated effectively.</p> <p>While there may be variations in the format based on specific needs, the following are the typical sections included in an ADR:</p> <ul> <li> <p>Title</p> <p>A concise and descriptive title that summarizes the decision.</p> </li> <li> <p>Status</p> <p>The current state of the decision, such as \"proposed,\" \"accepted,\" or \"deprecated.\" The status can help stakeholders understand the current state of the decision and what actions, if any, are needed to move it forward.</p> </li> <li> <p>Context</p> <p>A description of the relevant background and circumstances that led to the decision. The context should explain the problem or opportunity that the decision addresses and any relevant constraints, such as technical limitations, organizational policies, or stakeholder preferences.</p> </li> <li> <p>Decision</p> <p>A clear statement of the decision that was made. The decision should describe the chosen solution or course of action and how it addresses the problem or opportunity described in the context.</p> </li> <li> <p>Rationale</p> <p>A description of the reasoning behind the decision, including any trade-offs or alternatives that were considered. The rationale should explain why the chosen solution or course of action was selected over other options and how it aligns with the system's goals and requirements.</p> </li> <li> <p>Consequences</p> <p>An analysis of the likely effects of the decision on the system and its stakeholders. The consequences should describe any benefits or drawbacks of the decision, how it affects the system's functionality, performance, or other qualities, and how it affects the system's users, operators, or other stakeholders.</p> </li> <li> <p>Alternatives</p> <p>A list of alternative solutions that were considered but ultimately rejected. The alternatives should describe any other options that were evaluated and why they were not selected.</p> </li> </ul> <p>The Markdown template includes the key sections typically included in an ADR, and can be customized to meet the specific needs of a project or organization.</p> <pre><code># Architecture Decision Record (ADR) [Number]: [Title]\n\n## Status\n[Proposed | Accepted | Rejected | Deprecated | Superseded | ...]\n\n## Context\n[Provide context for the decision, including any relevant background information, system architecture, business requirements, etc.]\n\n## Decision\n[State the decision that was made, using clear and concise language.]\n\n## Rationale\n[Explain the reasons behind the decision, including the problem or opportunity that the decision addresses, the constraints that were considered, and the options that were evaluated.]\n\n## Alternatives\n[List the alternative options that were considered before the decision was made, and explain why they were rejected or chosen.]\n\n## Consequences\n[Describe the potential impact of the decision on the system and its stakeholders, including any technical, operational, or business-related consequences.]\n\n## Implementation\n[Explain how the decision will be implemented, including any technical details, changes to the system's architecture or code, organizational or process-related changes, etc.]\n\n## Related ADRs\n[List any related ADRs that are relevant to the decision being documented.]\n\n## References\n[List any sources that were consulted or referenced in making the decision.]\n</code></pre>"},{"location":"articles/architecture-decision-records/#12-naming-convention","title":"1.2. Naming Convention","text":"<p>A naming convention for ADRs helps to organize and find specific records in a repository Important aspect of naming convention for ADRs is consistency.</p> <ul> <li>Identifier Sequence</li> </ul> <p>The unique identifier is a sequential number, which ensures that each ADR has a unique filename. The title should be a brief and descriptive summary of the decision, with words separated by hyphens.</p> <pre><code>[unique-identifier]-[title].md\n</code></pre> <p>An ADR file for a decision to use a microservices architecture might be named:</p> <pre><code>0001-use-microservices-architecture.md\n</code></pre> <p>The numbering system allows for easy tracking and sorting of ADRs in chronological order.</p> <ul> <li>Date Format</li> </ul> <p>In this convention, the date is written in the ISO 8601 format of year-month-day, which ensures that the files are sorted in chronological order. The short description should be a brief and meaningful summary of the decision, with words separated by hyphens.</p> <pre><code>[YYYYMMDD]-[title].md\n</code></pre> <p>For example, an ADR file for a decision to use a RESTful API design might be named:</p> <pre><code>20230423-use-restful-api-design.md\n</code></pre>"},{"location":"articles/architecture-decision-records/#2-principle","title":"2. Principle","text":"<ul> <li> <p>Clarity</p> <p>ADRs should be written in clear and concise language that is easy to understand. They should avoid technical jargon and be accessible to a wide range of stakeholders, including non-technical ones.</p> </li> <li> <p>Consistency</p> <p>ADRs should follow a consistent format and structure to make it easy for stakeholders to find and understand the information they need. This can include a consistent set of elements, such as title, status, context, decision, rationale, consequences, and alternatives.</p> </li> <li> <p>Relevance</p> <p>ADRs should focus on decisions that have a significant impact on the system's architecture. They should not document trivial decisions or those that do not have a lasting impact.</p> </li> <li> <p>Accessibility</p> <p>ADRs should be easily accessible to stakeholders. They should be stored in a centralized repository, such as a version control system, and be easy to search and retrieve.</p> </li> <li> <p>Timeliness</p> <p>ADRs should be created and updated in a timely manner. They should be created as soon as possible after a significant architectural decision is made, and should be updated as necessary to reflect changes in the system or new information.</p> </li> <li> <p>Contextualization</p> <p>ADRs should provide context for the decision being made. They should describe the problem or opportunity that the decision addresses, the constraints that were considered, and the options that were evaluated.</p> </li> <li> <p>Collaboration</p> <p>ADRs should be created collaboratively by the stakeholders who were involved in making the decision. This can include architects, developers, managers, and other stakeholders who have a stake in the system's architecture.</p> </li> </ul>"},{"location":"articles/architecture-decision-records/#3-best-practice","title":"3. Best Practice","text":"<ul> <li> <p>Use a standardized format</p> <p>ADRs should follow a consistent and standardized format to make it easy for stakeholders to find and understand the information they need. This can include a consistent set of elements, such as title, status, context, decision, rationale, consequences, and alternatives.</p> </li> <li> <p>Create ADRs as soon as possible</p> <p>ADRs should be created as soon as possible after a significant architectural decision is made. This helps to ensure that the decision and its context are accurately documented and that stakeholders have a clear understanding of why the decision was made.</p> </li> <li> <p>Use a version control system</p> <p>ADRs should be stored in a version control system, such as Git, to ensure that they can be easily tracked, versioned, and updated over time. This also helps to ensure that the ADRs are easily accessible and searchable.</p> </li> <li> <p>Keep ADRs concise and focused</p> <p>ADRs should be concise and focused on the key decision being made. They should avoid technical jargon and be accessible to a wide range of stakeholders, including non-technical ones.</p> </li> <li> <p>Use a consistent and descriptive naming convention</p> <p>ADRs should be named in a consistent and descriptive way to make it easy to identify and search for them. A good naming convention can include the decision number, a brief summary of the decision, and the date it was made.</p> </li> <li> <p>Link ADRs to other artifacts</p> <p>ADRs should be linked to other artifacts, such as requirements, design documents, and code changes. This helps to ensure that stakeholders have a clear understanding of the impact of the decision on the system and its components.</p> </li> <li> <p>Review and update ADRs regularly</p> <p>ADRs should be reviewed and updated regularly to ensure that they accurately reflect the current state of the system and its architecture. This can include updating the decision, rationale, and consequences sections as necessary.</p> </li> </ul>"},{"location":"articles/architecture-decision-records/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Decision</p> <p>ADRs document important architectural decisions made during the design and development of a software system. The decision section of an ADR describes the specific decision that was made.</p> </li> <li> <p>Rationale</p> <p>The rationale section of an ADR describes the reasons behind the decision. It should include a clear explanation of the problem or opportunity that the decision addresses, the constraints that were considered, and the options that were evaluated.</p> </li> <li> <p>Alternatives</p> <p>The alternatives section of an ADR describes the alternative options that were considered before the decision was made. This section should provide a clear explanation of why each alternative was considered and why it was ultimately rejected or chosen.</p> </li> <li> <p>Consequences</p> <p>The consequences section of an ADR describes the impact of the decision on the system and its stakeholders. This can include technical, operational, and business-related consequences.</p> </li> <li> <p>Status</p> <p>The status section of an ADR describes the current state of the decision. It can include information about whether the decision has been implemented, is in progress, or has been abandoned.</p> </li> <li> <p>Context</p> <p>The context section of an ADR provides background information about the system, the problem or opportunity that the decision addresses, and the stakeholders involved in the decision-making process.</p> </li> <li> <p>Stakeholders</p> <p>ADRs are created for a wide range of stakeholders, including architects, developers, managers, and other stakeholders who have a stake in the system's architecture. The stakeholders section of an ADR should describe the individuals or groups involved in the decision-making process.</p> </li> <li> <p>Implementation</p> <p>The implementation section of an ADR describes how the decision will be implemented. This can include technical details, such as changes to the system's architecture or code, as well as organizational or process-related changes.</p> </li> </ul>"},{"location":"articles/architecture-decision-records/#5-references","title":"5. References","text":"<ul> <li>Github ADR project.</li> <li>Github ADR repository.</li> <li>Google ADR article.</li> <li>Sentenz ISO 8601 article.</li> </ul>"},{"location":"articles/architecture-modeling-concepts/","title":"Architecture Modeling Concepts","text":"<p>Software architecture involves the high level structure of software system abstraction, by using decomposition and composition, with architectural style and quality attributes. The architecture design must conform to the major functionality and performance requirements of the system, as well as satisfy the non-functional requirements such as reliability, scalability, portability, and availability. A software architecture must describe its group of components, their connections, interactions among them and deployment configuration of all components.</p> <ul> <li>Enterprise Architecture</li> <li>Solution Architecture</li> <li>Technical Architecture</li> </ul>"},{"location":"articles/architecture-modeling-concepts/#enterprise-architecture","title":"Enterprise Architecture","text":"<p>Enterprise architecture (EA) is a discipline for proactively and holistically leading enterprise responses to disruptive forces by identifying and analyzing the execution of change toward desired business vision and outcomes. EA delivers value by presenting business and IT leaders with signature-ready recommendations for adjusting policies and projects to achieve targeted business outcomes that capitalize on relevant business disruptions.</p>"},{"location":"articles/architecture-modeling-concepts/#solution-architecture","title":"Solution Architecture","text":"<p>A solution architecture (SA) is an architectural description of a specific solution. SAs combine guidance from different enterprise architecture viewpoints (business, information and technical), as well as from the enterprise architecture (ES).</p> <p>Toolings:</p> <ul> <li> <p>Fundamental Modeling Concepts (FMC) for system-related structures.</p> <p>Supported software tools.</p> <ul> <li>Eclipse FMC</li> </ul> </li> <li> <p>TAM - The SAP way combining FMC and UML</p> </li> </ul>"},{"location":"articles/architecture-modeling-concepts/#technical-architecture","title":"Technical Architecture","text":"<p>Technical Architecture (TA) realize specific technical implementation processes. Since this requires a high level of in-depth expertise, Technical Architects usually specialize in a single technology. Therefore, they are named after their area of knowledge, e.g. Java or Python architect. Like Data, Application or Information Architects, Technical Architects fall under the umbrella of Domain Architects. Overall, these roles put technological solutions into practice and ensure that application designs support the technological strategy defined by the Enterprise Architect. However, Technical Architects are not only responsible for implementing new technologies. They also make recommendations and inform stakeholders about potential threats.</p> <p>Toolings:</p> <ul> <li>Unified Modeling Language (UML) for software-related structures. <p>Supported software tools.</p> <ul> <li>VS Code UMLet</li> <li>VS Code PlantUML</li> </ul> </li> </ul>"},{"location":"articles/asymptotic-notation/","title":"Asymptotic Notation","text":"<p>Asymptotic notation is a mathematical concept used to describe the behavior of a function as the input size approaches infinity. It is commonly used in computer science and programming to analyze and compare the efficiency of different algorithms.</p> <ul> <li>1. Types of Notation</li> <li>1.1. Big O Notation (O)</li> <li>1.2. Big Omega Notation (\u03a9)</li> <li>1.3. Big Theta Notation (\u03b8)</li> <li>1.4. Little o Notation (o)</li> <li>1.5. Little omega Notation (\u03c9)</li> <li>2. Types of Complexity</li> <li>2.1. Time Complexity</li> <li>2.2. Space Complexity</li> <li>2.3. Communication Complexity</li> <li>2.4. Circuit Complexity</li> <li>2.5. Kolmogorov Complexity</li> <li>3. Types of Classes</li> <li>3.1. Time Complexity<ul> <li>3.1.1. Constant Time</li> <li>3.1.2. Logarithmic Time</li> <li>3.1.3. Linear Time</li> <li>3.1.4. Linearithmic Time</li> <li>3.1.5. Quadratic Time</li> <li>3.1.6. Cubic Time</li> <li>3.1.7. Exponential Time</li> <li>3.1.8. Factorial Time</li> <li>3.1.9. Polynomial Time</li> <li>3.1.10. Sublinear Time</li> <li>3.1.11. Pseudo-polynomial Time</li> <li>3.1.12. Exponential Time</li> </ul> </li> <li>3.2. Space Complexity<ul> <li>3.2.1. Constant Space</li> <li>3.2.2. Linear Space</li> <li>3.2.3. Quadratic Space</li> <li>3.2.4. Exponential Space</li> </ul> </li> <li>5. Complexity Classification</li> <li>Performance</li> <li>4. Terminology</li> <li>6. References</li> </ul>"},{"location":"articles/asymptotic-notation/#1-types-of-notation","title":"1. Types of Notation","text":"<p>Asymptotic notation are used in computer science to analyze the time and space complexity of algorithms.</p>"},{"location":"articles/asymptotic-notation/#11-big-o-notation-o","title":"1.1. Big O Notation (O)","text":"<p>Big O notation (O) represents the upper bound of an algorithm's running time. It gives an idea of how quickly the algorithm will grow in relation to the size of the input.</p> <p>For example, an algorithm with O(n) time complexity will have a linear growth in relation to the input size.</p>"},{"location":"articles/asymptotic-notation/#12-big-omega-notation","title":"1.2. Big Omega Notation (\u03a9)","text":"<p>Big Omega notation (\u03a9) represents the lower bound of an algorithm's running time. It gives an idea of how quickly the algorithm will perform at its best in relation to the size of the input.</p> <p>For example, an algorithm with \u03a9(n) time complexity will have a linear growth at its best.</p>"},{"location":"articles/asymptotic-notation/#13-big-theta-notation","title":"1.3. Big Theta Notation (\u03b8)","text":"<p>Big Theta notation (\u03b8) represents the tight bound of an algorithm's running time. It gives an idea of how quickly the algorithm will grow in relation to the size of the input, with both the upper and lower bounds taken into account.</p> <p>For example, an algorithm with \u03b8(n) time complexity will have a linear growth at its best and worst.</p>"},{"location":"articles/asymptotic-notation/#14-little-o-notation-o","title":"1.4. Little o Notation (o)","text":"<p>Little o notation (o) represents the upper bound of an algorithm's growth rate, but it is a stricter condition than big O notation. It is used to describe algorithms that grow slower than another algorithm.</p> <p>For example, an algorithm with o(n^2) growth rate will grow slower than an algorithm with O(n^2) growth rate.</p>"},{"location":"articles/asymptotic-notation/#15-little-omega-notation","title":"1.5. Little omega Notation (\u03c9)","text":"<p>Little omega notation (\u03c9) represents the lower bound of an algorithm's growth rate, but it is a stricter condition than omega notation. It is used to describe algorithms that grow faster than another algorithm.</p> <p>For example, an algorithm with \u03c9(n^2) growth rate will grow faster than an algorithm with \u03a9(n^2) growth rate.</p>"},{"location":"articles/asymptotic-notation/#2-types-of-complexity","title":"2. Types of Complexity","text":"<p>In computer science, there are several types of complexity that are used to analyze the performance and efficiency of algorithms. The different types of complexity are important for understanding the performance and behavior of different types of algorithms and computational systems, and they are often used in conjunction with one another to provide a more complete picture of the efficiency and complexity of a particular computational task.</p>"},{"location":"articles/asymptotic-notation/#21-time-complexity","title":"2.1. Time Complexity","text":"<p>Time complexity refers to the amount of time an algorithm takes to complete as a function of the size of its input. Time complexity is usually expressed using big-O notation, which provides an upper bound on the growth rate of the algorithm's runtime as the input size grows.</p>"},{"location":"articles/asymptotic-notation/#22-space-complexity","title":"2.2. Space Complexity","text":"<p>Space complexity refers to the amount of memory an algorithm requires to run as a function of the size of its input. Space complexity is also expressed using big-O notation, which provides an upper bound on the growth rate of the algorithm's memory usage as the input size grows.</p>"},{"location":"articles/asymptotic-notation/#23-communication-complexity","title":"2.3. Communication Complexity","text":"<p>Communication complexity: This refers to the amount of information that must be transmitted between different computational agents in a distributed computing system. Communication complexity is often used to analyze the performance of parallel algorithms that must coordinate their efforts across different processors or nodes.</p>"},{"location":"articles/asymptotic-notation/#24-circuit-complexity","title":"2.4. Circuit Complexity","text":"<p>Circuit complexity refers to the amount of resources, such as gates or wires, required to implement a particular computational function using a digital circuit. Circuit complexity is often used in the design and analysis of hardware systems, such as computer processors or memory chips.</p>"},{"location":"articles/asymptotic-notation/#25-kolmogorov-complexity","title":"2.5. Kolmogorov Complexity","text":"<p>Kolmogorov complexity refers to the length of the shortest possible program that can generate a particular string of data. Kolmogorov complexity is a measure of the amount of information contained in a string, and it is often used in the study of information theory and computational complexity.</p>"},{"location":"articles/asymptotic-notation/#3-types-of-classes","title":"3. Types of Classes","text":"<p>NOTE The specific values of k, M, and n can have a significant impact on the actual running time of an algorithm, even within the same complexity class. Additionally, other factors such as memory usage, cache efficiency, and parallelization can all influence the performance of an algorithm in practice. Asymptotic notation is a useful tool for analyzing and comparing algorithms, but it should always be used in conjunction with empirical testing and analysis to fully understand an algorithm's performance.</p>"},{"location":"articles/asymptotic-notation/#31-time-complexity","title":"3.1. Time Complexity","text":""},{"location":"articles/asymptotic-notation/#311-constant-time","title":"3.1.1. Constant Time","text":"<p>O(1), \u03a9(1), \u03b8(1)</p> <p>Algorithms with constant time complexity have a fixed running time that does not depend on the size of the input. They are the most efficient algorithms in terms of time complexity and are typically used for simple operations, such as assigning a value to a variable, accessing an array element, or performing basic arithmetic operations.</p> <p>Examples: Assigning a value to a variable, accessing an array element, performing basic arithmetic operations.</p>"},{"location":"articles/asymptotic-notation/#312-logarithmic-time","title":"3.1.2. Logarithmic Time","text":"<p>O(log n), \u03a9(log n), \u03b8(log n)</p> <p>Algorithms with logarithmic time complexity divide the input into smaller and smaller pieces until the target is found. They are typically used for search or divide-and-conquer operations, where the input is sorted or can be easily split into smaller pieces.</p> <p>Examples: Binary search, quicksort, mergesort.</p>"},{"location":"articles/asymptotic-notation/#313-linear-time","title":"3.1.3. Linear Time","text":"<p>O(n), \u03a9(n), \u03b8(n)</p> <p>Algorithms with linear time complexity have a running time that is directly proportional to the size of the input. They are typically used for operations that involve iterating through the entire input, such as searching, filtering, or counting.</p> <p>Examples: Linear search, selection sort, counting sort.</p>"},{"location":"articles/asymptotic-notation/#314-linearithmic-time","title":"3.1.4. Linearithmic Time","text":"<p>O(n log n), \u03a9(n log n), \u03b8(n log n)</p> <p>Algorithms with linearithmic time (or log-linear time) complexity have a running time that is a product of a linear and logarithmic function of the input size. They are typically used for sorting operations that involve dividing the input into smaller and smaller pieces and then merging them back together.</p> <p>Examples: Heapsort, quicksort, mergesort.</p>"},{"location":"articles/asymptotic-notation/#315-quadratic-time","title":"3.1.5. Quadratic Time","text":"<p>O(n^2), \u03a9(n^2), \u03b8(n^2)</p> <p>Algorithms with quadratic time complexity have a running time that is directly proportional to the square of the input size. They are typically used for operations that involve comparing or processing every pair of elements in the input, such as nested loops.</p> <p>Examples: Bubble sort, selection sort, insertion sort.</p>"},{"location":"articles/asymptotic-notation/#316-cubic-time","title":"3.1.6. Cubic Time","text":"<p>O(n^3), \u03a9(n^3), \u03b8(n^3)</p> <p>Algorithms with cubic time complexity have a running time that is directly proportional to the cube of the input size. They are typically used for operations that involve comparing or processing every triplet of elements in the input, such as nested loops.</p> <p>Examples: Matrix multiplication using the naive algorithm.</p>"},{"location":"articles/asymptotic-notation/#317-exponential-time","title":"3.1.7. Exponential Time","text":"<p>O(2^n), \u03a9(2^n), \u03b8(2^n)</p> <p>Algorithms with exponential time complexity have a running time that grows exponentially with the input size. They are typically used for operations that involve generating or enumerating all possible combinations or permutations of the input.</p> <p>Examples: Brute-force search, exhaustive search, traveling salesman problem using dynamic programming.</p>"},{"location":"articles/asymptotic-notation/#318-factorial-time","title":"3.1.8. Factorial Time","text":"<p>O(n!), \u03a9(n!), \u03b8(n!)</p> <p>Factorial time complexity is a class that is used to describe algorithms that have a running time that is proportional to the factorial of the input size. Algorithms with factorial time complexity are typically used for problems that involve generating or counting all possible permutations or combinations of the input. An algorithm with a factorial time complexity is considered extremely inefficient and generally impractical for large inputs.</p> <p>Examples: Brute-force search, traveling salesman problem, permutation generation.</p>"},{"location":"articles/asymptotic-notation/#319-polynomial-time","title":"3.1.9. Polynomial Time","text":"<p>O(n^k), \u03a9(n^k), \u03b8(n^k), where k is a constant. This includes linear time (k=1), quadratic time (k=2), and cubic time (k=3), among others.</p> <p>Algorithms with polynomial time complexity have a running time that grows as a polynomial function of the input size. They are generally considered to be efficient and practical for solving a wide range of problems, as their running time increases at a manageable rate with increasing input size. Many commonly used algorithms fall under this category, including sorting, searching, and graph algorithms.</p> <p>Examples: Bubble sort, Dijkstra's algorithm, polynomial regression.</p>"},{"location":"articles/asymptotic-notation/#3110-sublinear-time","title":"3.1.10. Sublinear Time","text":"<p>O(log log n), \u03a9(log log n), \u03b8(log log n)</p> <p>Algorithms with factorial time complexity have a running time that grows factorially with the input size. They are typically used for operations that involve generating or enumerating all possible permutations of the input.</p> <p>Examples: Brute-force search, exhaustive search, traveling salesman problem using dynamic programming with memoization.</p>"},{"location":"articles/asymptotic-notation/#3111-pseudo-polynomial-time","title":"3.1.11. Pseudo-polynomial Time","text":"<p>O(nM), where M is a number that is not necessarily polynomial in the size of the input n. This is used to describe algorithms that are efficient when the input data is restricted in some way, such as when M is much smaller than n.</p> <p>Algorithms with pseudo-polynomial time complexity have a runtime that is a function of both the input size and the numerical value of the input. These algorithms are typically used for problems that involve integer values or counting, and their runtime can depend on the maximum value of the input, rather than just its size.</p> <p>Examples: Knapsack problem, subset sum problem, coin change problem.</p>"},{"location":"articles/asymptotic-notation/#3112-exponential-time","title":"3.1.12. Exponential Time","text":"<p>O(k^n), where k is a constant greater than 1.</p> <p>Algorithms with exponential time complexity have a runtime that grows exponentially with the input size. These algorithms are typically considered <code>inefficient</code> for large inputs and are generally only used for small instances of a problem.</p> <p>Examples: Brute-force search, traveling salesman problem, subset generation.</p>"},{"location":"articles/asymptotic-notation/#32-space-complexity","title":"3.2. Space Complexity","text":"<p>Space complexity refers to the amount of memory required by an algorithm to solve a problem, and can depend on factors such as the size of the input, the data structures used, and the number of variables or objects created during the algorithm's execution.</p>"},{"location":"articles/asymptotic-notation/#321-constant-space","title":"3.2.1. Constant Space","text":"<p>O(1), \u03a9(1), \u03b8(1)</p> <p>Algorithms with constant space complexity use a fixed amount of memory that does not depend on the size of the input. They are typically used for simple operations that do not require additional storage beyond the input and a small number of variables.</p> <p>Examples: Assigning a value to a variable, accessing an array element, performing basic arithmetic operations.</p>"},{"location":"articles/asymptotic-notation/#322-linear-space","title":"3.2.2. Linear Space","text":"<p>O(n), \u03a9(n), \u03b8(n)</p> <p>Algorithms with linear space complexity use a memory allocation that is directly proportional to the size of the input. They are typically used for operations that require additional data structures to be created in order to solve the problem.</p> <p>Examples: Building a hash table, constructing a binary tree, merging sorted arrays.</p>"},{"location":"articles/asymptotic-notation/#323-quadratic-space","title":"3.2.3. Quadratic Space","text":"<p>O(n^2), \u03a9(n^2), \u03b8(n^2)</p> <p>Algorithms with quadratic space complexity use a memory allocation that is directly proportional to the square of the input size. They are typically used for operations that require additional data structures that depend on pairs of elements in the input.</p> <p>Examples: Matrix multiplication, bubble sort, selection sort (when implemented with a two-dimensional array).</p>"},{"location":"articles/asymptotic-notation/#324-exponential-space","title":"3.2.4. Exponential Space","text":"<p>O(2^n), \u03a9(2^n), \u03b8(2^n)</p> <p>Algorithms with exponential space complexity use a memory allocation that grows exponentially with the input size. They are typically used for operations that require storing or generating all possible combinations or permutations of the input. This space complexity class is commonly associated with problems that are known to be <code>intractable</code> in the sense that no algorithm can solve them in polynomial time, such as the subset sum problem or the knapsack problem.</p> <p>Examples: Generating all subsets of a set, computing the Fibonacci sequence using recursive memoization.</p>"},{"location":"articles/asymptotic-notation/#5-complexity-classification","title":"5. Complexity Classification","text":"<p>Complexity classes are commonly used to describe the performance of algorithms or problems. These classes include:</p> <ul> <li> <p>P</p> <p>The P (Polynomial Time) class of problems that can be solved in polynomial time using a deterministic algorithm.</p> </li> <li> <p>NP</p> <p>The NP (Nondeterministic Polynomial Time) class of problems for which a solution can be verified in polynomial time, but not necessarily found in polynomial time.</p> </li> <li> <p>NP-hard</p> <p>The NP-hard (Nondeterministic Polynomial Time Hard) class of problems that are at least as hard as the hardest problems in NP. These problems do not necessarily have polynomial-time solutions.</p> </li> <li> <p>NP-complete</p> <p>The NP-complete (Nondeterministic Polynomial Time Complete) class of problems that are both in NP and NP-hard. These problems are some of the most difficult computational problems known.</p> </li> <li> <p>EXPTIME</p> <p>The EXPTIME (Exponential Time) class of problems that can be solved in exponential time.</p> </li> <li> <p>PSPACE</p> <p>The PSPACE (Polynomial Space) class of problems that can be solved using polynomial space.</p> </li> <li> <p>EXPSPACE</p> <p>The EXPSPACE (Exponential Space) class of problems that can be solved using exponential space.</p> </li> </ul> <p>NOTE PSPACE and EXPSPACE are space complexity classes, while the others are time complexity classes. Additionally, NP, NP-hard, and NP-complete are related to the concept of nondeterminism, while P, EXPTIME, PSPACE, and EXPSPACE are related to deterministic algorithms.</p>"},{"location":"articles/asymptotic-notation/#performance","title":"Performance","text":"<p>The performance (runtime) of an algorithm depends on the size of the input n or the number of operations is required for each input item.</p> <p>The algorithms can be classified as best-to-worst performance (Time Complexity).</p> <ul> <li> <p>Best</p> <ul> <li>Constant Time: O(1)</li> </ul> </li> <li> <p>Good</p> <ul> <li>Logarithmic Time: O(log n)</li> </ul> </li> <li> <p>Fair:</p> <ul> <li>Linear Time: O(n)</li> </ul> </li> <li> <p>Bad</p> <ul> <li>Linearithmic Time: O(n log n)</li> </ul> </li> <li> <p>Worst</p> <ul> <li>Quadratic Time: O(n^2)</li> <li>Exponential Time: O(2^n)</li> <li>Factorial Time: O(n!)</li> </ul> </li> </ul>"},{"location":"articles/asymptotic-notation/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Big O notation (O)</p> <p>A function <code>f(n)</code> is said to be <code>O(g(n))</code> if there exist constants <code>c</code> and <code>n0</code> such that <code>|f(n)| &lt;= c|g(n)|</code> for all <code>n &gt;= n0</code>.</p> </li> <li> <p>Omega notation (\u03a9)</p> <p>A function <code>f(n)</code> is said to be <code>\u03a9(g(n))</code> if there exist constants <code>c</code> and <code>n0</code> such that <code>|f(n)| &gt;= c|g(n)|</code> for all <code>n &gt;= n0</code>.</p> </li> <li> <p>Theta notation (\u0398)</p> <p>A function <code>f(n)</code> is said to be <code>\u0398(g(n))</code> if it is both <code>O(g(n))</code> and <code>\u03a9(g(n))</code>.</p> </li> <li> <p>Little o notation (o)</p> <p>A function <code>f(n)</code> is said to be <code>o(g(n))</code> if for any constant <code>\u03b5 &gt; 0</code>, there exists a constant <code>n0</code> such that <code>|f(n)| &lt; \u03b5|g(n)|</code> for all <code>n &gt;= n0</code>.</p> </li> <li> <p>Little omega notation (\u03c9)</p> <p>A function <code>f(n)</code> is said to be <code>\u03c9(g(n))</code> if for any constant <code>\u03b5 &gt; 0</code>, there exists a constant <code>n0</code> such that <code>|f(n)| &gt; \u03b5|g(n)|</code> for all <code>n &gt;= n0</code>.</p> </li> </ul>"},{"location":"articles/asymptotic-notation/#6-references","title":"6. References","text":"<ul> <li>GeeksForGeeks asymptotic notations article.</li> <li>GeeksForGeeks analysis of algorithms article.</li> <li>GitHub o-notation repository.</li> </ul>"},{"location":"articles/branching-strategies/","title":"Branching Strategies","text":"<p>A branching strategy refers to the strategy that a software development team uses when writing, merging, and shipping code in the context of a version control system (VCS) such as Git.</p> <ul> <li>1. Git Flow</li> <li>2. GitHub Flow</li> <li>3. Release Flow</li> <li>4. Trunk-Based Development</li> <li>5. Scaled Trunk-Based Development</li> <li>6. References</li> </ul> <p>A branching strategy defines how a team uses branching to achieve this level of concurrent development.</p>"},{"location":"articles/branching-strategies/#1-git-flow","title":"1. Git Flow","text":"<p>Git flow is a branching model for Git, created by Vincent Driessen in 2010.</p> <p>The strategy of Git flow is to isolate the work into different types of branches. The two primary branches in Git flow are <code>main</code> and <code>develop</code>. There are three types of supporting branches with different intended purposes: <code>feature</code>, <code>release</code>, and <code>hotfix</code>.</p> <p>The base branches:</p> <ul> <li> <p>main</p> <p>Consider main to be the branch where the source code of <code>HEAD</code> always reflects a production-ready state.</p> </li> <li> <p>develop</p> <p>Consider develop to be the <code>staging</code> branch where the source code of <code>HEAD</code> always reflects a state with the latest delivered development changes for the next release.</p> </li> </ul> <p>Supporting branches:</p> <ul> <li> <p>feature</p> <p>Feature branches are used to develop new features. Feature branches are created from and merged back into develop branches. Feature branches typically exist in developer repos only, not in origin.</p> </li> <li> <p>release</p> <p>Release branches are created every iteration (i.e. sprint) from develop branch, which contains a set of features and their associated bug fixes. Furthermore, they allow for minor bug fixes and preparing meta-data for a release (version number i.e semver, build dates, etc.). The release branch is then subjected to deployed to the staging (integration and regression testing). Any bugs identified during this phase is fixed and committed directly to the release branch. Once the release branch has been confirmed to be free of bugs, it is merged into the main branch and released into production. These fixes are also merged back into develop and other release branches if any exist.</p> </li> <li> <p>fix</p> <p>Fixes branches are created from main branch, when issues need an fix upon an undesired state of a live production version. Fix branches needs to be merged back into main and into develop, in order to safeguard that the bugfix is included in the next release.</p> </li> </ul>"},{"location":"articles/branching-strategies/#2-github-flow","title":"2. GitHub Flow","text":"<p>GitHub flow is a lightweight, branch-based workflow.</p> <p>In GitHub flow, the <code>main</code> branch contains your production-ready code. The other branches, <code>feature</code> branches, should contain work on new features and bug fixes and will be merged back into the main branch when the work is finished and properly reviewed.</p> <p>The base branch:</p> <ul> <li>main</li> </ul> <p>Supporting branch:</p> <ul> <li>feature</li> </ul>"},{"location":"articles/branching-strategies/#3-release-flow","title":"3. Release Flow","text":"<p>Release flow is an industry-standard approach.</p>"},{"location":"articles/branching-strategies/#4-trunk-based-development","title":"4. Trunk-Based Development","text":"<p>Trunk-Based Development is a source-control branching model, where developers collaborate on code in a single branch called <code>trunk</code> (main in Git nomenclature), resist any pressure to create other long-lived development branches by employing documented techniques. They therefore avoid merge conflicts, do not break the build.</p> <p>The base branch:</p> <ul> <li>trunk <p>Consider trunk to be the branch where the source code of <code>HEAD</code> always contains the latest version.</p> </li> </ul>"},{"location":"articles/branching-strategies/#5-scaled-trunk-based-development","title":"5. Scaled Trunk-Based Development","text":"<p>Scaled Trunk-Based Development is done with short-lived <code>feature</code> branches. One developer over a couple of days (max) and flowing through Pull-Request style code-review and automation (CI/CD) before integrating (merging) into the <code>trunk</code> (main in Git nomenclature) branch.</p> <p>The base branch:</p> <ul> <li>trunk <p>Consider trunk to be the branch where the source code of <code>HEAD</code> always contains the latest version.</p> </li> </ul> <p>Supporting branch:</p> <ul> <li> <p>feature</p> <p>Short-living feature branches are used to develop new features. Feature branches are created from and merged back into trunk. Feature branches typically exist in developer repos only, not in origin. The style of short-living feature branches is suitable for active committer counts between 2 and 1000.</p> </li> <li> <p>release</p> <p>Release branch is a cut from trunk with an optional number of cherry picks that are developed on trunk and then pulled into the branch.</p> </li> </ul>"},{"location":"articles/branching-strategies/#6-references","title":"6. References","text":"<ul> <li>Github TBD repository.</li> <li>Github GitHub Flow article.</li> <li>Azure DevOps Release Flow article.</li> <li>YouTube branching patterns and anti-patterns video.</li> </ul>"},{"location":"articles/build-systems/","title":"Build Systems","text":"<p>A build system is a software tool or framework that automates the process of compiling source code into executable software or other target artifacts. It helps manage the dependencies, configurations, and tasks required to build a software project.</p> <ul> <li>1. Category</li> <li>1.1. Make</li> <li>1.2. CMake</li> <li>1.3. Gradle</li> <li>1.4. Maven</li> <li>1.5. Ant</li> <li>1.6. Bazel</li> <li>1.7. Ninja</li> </ul>"},{"location":"articles/build-systems/#1-category","title":"1. Category","text":""},{"location":"articles/build-systems/#11-make","title":"1.1. Make","text":"<p>Make is a widely used build system primarily found in Unix-based systems. It automates the process of compiling source code into executable programs or libraries. Make relies on makefiles, which are text files containing rules and instructions for building the project.</p> <p>Make provides various features, such as automatic dependency tracking and support for variables and macros. It allows to create complex build systems with multiple targets and conditional compilation. Makefiles can be customized to suit the specific requirements of your project.</p> <p>NOTE Makefiles use indentation with tabs (not spaces), so ensure that your text editor is configured to use tabs for indentation to avoid any issues.</p> <p>Example of Make:</p> <ul> <li>Structure</li> </ul> <pre><code>MyProject/\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.c\n\u2502   \u2514\u2500\u2500 utility.c\n\u2514\u2500\u2500 (other directories or files, if any)\n</code></pre> <ul> <li>Makefile</li> </ul> <pre><code>CC = gcc\nCFLAGS = -Wall -Werror\n\nall: myprogram\n\nmyprogram: main.o utility.o\n    $(CC) $(CFLAGS) -o myprogram main.o utility.o\n\nmain.o: main.c\n    $(CC) $(CFLAGS) -c main.c\n\nutility.o: utility.c\n    $(CC) $(CFLAGS) -c utility.c\n\nclean:\n    rm -f *.o\n</code></pre> <p>In the example:</p> <ul> <li> <p><code>myprogram</code> is the target executable, <code>main.o</code> and <code>utility.o</code> are the object file dependencies, and <code>clean</code> is a target for removing built artifacts.</p> </li> <li> <p>Open a terminal or command prompt, navigate to the project's root directory, and run the following command:</p> <pre><code>make\n</code></pre> <p>Make will read the makefile, analyze the dependencies, and execute the necessary commands to build the project.</p> </li> <li> <p>To remove the generated files run the following command:</p> <pre><code>make clean\n</code></pre> <p>This target will remove the built artifacts, such as the object files.</p> </li> </ul>"},{"location":"articles/build-systems/#12-cmake","title":"1.2. CMake","text":"<p>CMake is a cross-platform build system generator. It generates build files (e.g., Makefiles or project files) for different build systems, such as Make, Ninja, or Visual Studio. CMake provides a high-level scripting language that allows developers to define the build process and handle platform-specific differences.</p> <p>Example of CMake:</p> <ul> <li>Structure</li> </ul> <pre><code>MyProject/\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.cpp\n\u2502   \u2514\u2500\u2500 utility.cpp\n\u2514\u2500\u2500 include/\n    \u2514\u2500\u2500 (header files, if any)\n</code></pre> <ul> <li>CMakeLists.txt</li> </ul> <pre><code>cmake_minimum_required(VERSION 3.21)\nproject(MyProject)\n\n# Set C++ standard to C++11\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\n# Add source files\nset(SOURCES\n    src/main.cpp\n    src/utility.cpp\n)\n\n# Add an executable target\nadd_executable(MyExecutable ${SOURCES})\n\n# Set include directories\ntarget_include_directories(MyExecutable\n    PRIVATE include\n)\n</code></pre> <p>In the example:</p> <ul> <li> <p>Set the minimum required CMake version to 3.21 using <code>cmake_minimum_required(VERSION 3.21)</code>. Then, we specify the project name using <code>project(MyProject)</code>.</p> </li> <li> <p>Set the C++ standard to C++11 with <code>set(CMAKE_CXX_STANDARD 11)</code>.</p> </li> <li> <p>Define a list of source files (<code>src/main.cpp</code> and <code>src/utility.cpp</code>) in the <code>SOURCES</code> variable.</p> </li> <li> <p>Add an executable target named <code>MyExecutable</code> that will be built from the specified source files using <code>add_executable(MyExecutable ${SOURCES})</code>.</p> </li> <li> <p>Set the include directories using <code>target_include_directories(MyExecutable PRIVATE include)</code> to include the <code>include</code> directory for the target <code>MyExecutable</code>.</p> </li> </ul>"},{"location":"articles/build-systems/#13-gradle","title":"1.3. Gradle","text":"<p>Gradle is a build system commonly used in Java and Android development. It offers a flexible and powerful build automation framework. Gradle uses a Groovy-based domain-specific language (DSL) to define the build scripts. It supports dependency management, task parallelization, and incremental builds.</p> <p>Example of Gradle:</p> <ul> <li>Structure</li> </ul> <pre><code>MyProject/\n\u251c\u2500\u2500 build.gradle\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 main/\n    \u2502   \u2514\u2500\u2500 java/\n    \u2502       \u2514\u2500\u2500 com/\n    \u2502           \u2514\u2500\u2500 example/\n    \u2502               \u2514\u2500\u2500 MyApplication.java\n    \u2514\u2500\u2500 test/\n        \u2514\u2500\u2500 java/\n            \u2514\u2500\u2500 com/\n                \u2514\u2500\u2500 example/\n                    \u2514\u2500\u2500 MyApplicationTest.java\n</code></pre> <p>In the structure:</p> <ul> <li><code>build.gradle</code>: The Gradle build script file that defines the project configuration, dependencies, and tasks.</li> <li> <p><code>src/</code>: The directory that contains the source code for the project.</p> <ul> <li><code>main/</code>: The directory for main source code.</li> <li><code>java/</code>: The directory for Java source files.<ul> <li><code>com/</code>: The package structure for the project.</li> <li><code>example/</code>: The package for the project's classes.<ul> <li><code>MyApplication.java</code>: The main Java class file for the project.</li> </ul> </li> </ul> </li> <li><code>test/</code>: The directory for test source code.</li> <li><code>java/</code>: The directory for Java test source files.<ul> <li><code>com/</code>: The package structure for the test code.</li> <li><code>example/</code>: The package for the project's test classes.<ul> <li><code>MyApplicationTest.java</code>: The Java test class file for the project.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>build.gradle</p> </li> </ul> <pre><code>plugins {\n    id 'java'\n}\n\ngroup 'com.example'\nversion '1.0.0'\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation 'com.google.guava:guava:30.1-jre'\n    testImplementation 'junit:junit:4.13.2'\n}\n\ntasks.test {\n    useJUnit()\n}\n\ntask runApplication(type: JavaExec) {\n    mainClassName = 'com.example.MyApplication'\n    classpath = sourceSets.main.runtimeClasspath\n}\n</code></pre> <p>In the example, written in Groovy:</p> <ul> <li> <p><code>plugins</code> block: This block specifies the plugins to be applied to the project. In this case, we apply the <code>java</code> plugin, which adds support for Java compilation, testing, and packaging.</p> </li> <li> <p><code>group</code> and <code>version</code>: These properties define the project's group and version, which are used for identifying the project artifacts.</p> </li> <li> <p><code>repositories</code> block: This block configures the repositories where Gradle looks for dependencies. In this example, we use <code>mavenCentral()</code> as the repository for fetching dependencies.</p> </li> <li> <p><code>dependencies</code> block: This block defines the project's dependencies. In this example, we include <code>com.google.guava:guava:30.1-jre</code> as an implementation dependency, and <code>junit:junit:4.13.2</code> as a test implementation dependency.</p> </li> <li> <p><code>tasks.test</code> block: This block configures the test task. In this example, we use JUnit for testing by calling <code>useJUnit()</code>.</p> </li> <li> <p><code>task runApplication</code> block: This block defines a custom task named <code>runApplication</code>. It uses the <code>JavaExec</code> type to execute a Java class. We specify the main class name as <code>com.example.MyApplication</code> and set the classpath to the runtime classpath of the <code>main</code> source set.</p> </li> </ul>"},{"location":"articles/build-systems/#14-maven","title":"1.4. Maven","text":"<p>Maven is a build automation and dependency management tool primarily used for Java projects. It emphasizes convention over configuration and follows the Project Object Model (POM) approach. Maven manages project dependencies, builds the project using predefined lifecycle phases, and provides plugins for various tasks.</p> <p>Example of Maven:</p> <ul> <li>Structure</li> </ul> <pre><code>MyProject/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main/\n\u2502   \u2502   \u2514\u2500\u2500 java/\n\u2502   \u2502       \u2514\u2500\u2500 com/\n\u2502   \u2502           \u2514\u2500\u2500 example/\n\u2502   \u2502               \u2514\u2500\u2500 MyApp.java\n\u2502   \u2514\u2500\u2500 test/\n\u2502       \u2514\u2500\u2500 java/\n\u2502           \u2514\u2500\u2500 com/\n\u2502               \u2514\u2500\u2500 example/\n\u2502                   \u2514\u2500\u2500 MyAppTest.java\n\u2514\u2500\u2500 pom.xml\n</code></pre> <p>In the structure:</p> <ul> <li><code>src/</code>: The directory that contains the source code for the project.<ul> <li><code>main/</code>: The directory for main source code.</li> <li><code>java/</code>: The directory for Java source files.<ul> <li><code>com/</code>: The package structure for the project.</li> <li><code>example/</code>: The package for the project's classes.<ul> <li><code>MyApp.java</code>: The main Java class file for the project.</li> </ul> </li> </ul> </li> <li><code>test/</code>: The directory for test source code.</li> <li><code>java/</code>: The directory for Java test source files.<ul> <li><code>com/</code>: The package structure for the test code.</li> <li><code>example/</code>: The package for the project's test classes.<ul> <li><code>MyAppTest.java</code>: The Java test class file for the project.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p><code>pom.xml</code>: The Project Object Model (POM) file that contains project configuration and dependencies.</p> </li> <li> <p>pom.xml</p> </li> </ul> <pre><code>&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;groupId&gt;com.example&lt;/groupId&gt;\n    &lt;artifactId&gt;MyProject&lt;/artifactId&gt;\n    &lt;version&gt;1.0.0&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;\n    &lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;junit&lt;/groupId&gt;\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\n            &lt;version&gt;4.13.2&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n\n&lt;/project&gt;\n</code></pre> <p>In the example:</p> <ul> <li> <p><code>&lt;groupId&gt;</code>, <code>&lt;artifactId&gt;</code>, and <code>&lt;version&gt;</code>: These elements define the project's coordinates, specifying the group, artifact, and version identifiers.</p> </li> <li> <p><code>&lt;properties&gt;</code>: This section allows to define properties that can be referenced throughout the POM. In this example, we specify the source and target Java versions for compilation.</p> </li> <li> <p><code>&lt;dependencies&gt;</code>: This section defines the project's dependencies. In this example, we include the JUnit dependency with the specified version and scope.</p> </li> </ul>"},{"location":"articles/build-systems/#15-ant","title":"1.5. Ant","text":"<p>Ant (Another Neat Tool) is a Java-based build system that uses XML-based build scripts. It provides a flexible and customizable approach to building, testing, and deploying applications.</p> <p>Example of Ant:</p> <ul> <li>Structure</li> </ul> <pre><code>MyProject/\n\u251c\u2500\u2500 build.xml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 com/\n        \u2514\u2500\u2500 example/\n            \u2514\u2500\u2500 MyApp.java\n</code></pre> <p>In the structure:</p> <ul> <li><code>build.xml</code>: The Ant build script file that contains the project configuration and build targets.</li> <li> <p><code>src/</code>: The directory that contains the source code for the project.</p> <ul> <li><code>com/</code>: The package structure for the project.</li> <li><code>example/</code>: The package for the project's classes.<ul> <li><code>MyApp.java</code>: The Java source file for the project.</li> </ul> </li> </ul> </li> <li> <p>build.xml</p> </li> </ul> <pre><code>&lt;project name=\"MyProject\" default=\"build\"&gt;\n\n    &lt;!-- Define properties --&gt;\n    &lt;property name=\"src.dir\" value=\"src\"/&gt;\n    &lt;property name=\"build.dir\" value=\"build\"/&gt;\n    &lt;property name=\"dist.dir\" value=\"dist\"/&gt;\n\n    &lt;!-- Clean the build directory --&gt;\n    &lt;target name=\"clean\"&gt;\n        &lt;delete dir=\"${build.dir}\"/&gt;\n        &lt;delete dir=\"${dist.dir}\"/&gt;\n    &lt;/target&gt;\n\n    &lt;!-- Compile Java source files --&gt;\n    &lt;target name=\"compile\" depends=\"clean\"&gt;\n        &lt;mkdir dir=\"${build.dir}\"/&gt;\n        &lt;javac srcdir=\"${src.dir}\" destdir=\"${build.dir}\"/&gt;\n    &lt;/target&gt;\n\n    &lt;!-- Create a JAR file --&gt;\n    &lt;target name=\"jar\" depends=\"compile\"&gt;\n        &lt;mkdir dir=\"${dist.dir}\"/&gt;\n        &lt;jar destfile=\"${dist.dir}/MyProject.jar\" basedir=\"${build.dir}\"&gt;\n            &lt;manifest&gt;\n                &lt;attribute name=\"Main-Class\" value=\"com.example.MyApp\"/&gt;\n            &lt;/manifest&gt;\n        &lt;/jar&gt;\n    &lt;/target&gt;\n\n    &lt;!-- Default target --&gt;\n    &lt;target name=\"build\" depends=\"jar\"/&gt;\n\n&lt;/project&gt;\n</code></pre> <p>In the example:</p> <ul> <li> <p><code>&lt;project&gt;</code>: The root element of the Ant build script. It defines the project name and specifies the default target.</p> </li> <li> <p><code>&lt;property&gt;</code>: This element defines properties that can be used throughout the build script. In this example, we define properties for the source directory (<code>src.dir</code>), build directory (<code>build.dir</code>), and distribution directory (<code>dist.dir</code>).</p> </li> <li> <p><code>&lt;target&gt;</code>: Targets represent specific build actions or tasks. They can have dependencies on other targets. In this example, we have three targets: <code>clean</code>, <code>compile</code>, and <code>jar</code>.</p> </li> <li> <p><code>clean</code>: The <code>clean</code> target deletes the build and distribution directories to ensure a clean build environment.</p> </li> <li> <p><code>compile</code>: The <code>compile</code> target creates the build directory, and then compiles the Java source files in the source directory (<code>src.dir</code>) and places the compiled classes in the build directory (<code>build.dir</code>).</p> </li> <li> <p><code>jar</code>: The <code>jar</code> target creates a JAR file using the compiled classes from the build directory. It creates the distribution directory if it doesn't exist, and the resulting JAR file is named <code>MyProject.jar</code> and placed in the distribution directory (<code>dist.dir</code>). The <code>&lt;manifest&gt;</code> element inside the <code>&lt;jar&gt;</code> task specifies the Main-Class attribute for the JAR file.</p> </li> <li> <p><code>build</code>: The <code>build</code> target is the default target, which means it will be executed when no specific target is specified. In this example, the <code>build</code> target depends on the <code>jar</code> target, so running <code>ant</code> without specifying a target will clean the build directories, compile the Java source files, and create the JAR file.</p> </li> </ul>"},{"location":"articles/build-systems/#16-bazel","title":"1.6. Bazel","text":"<p>Bazel is an open-source build system developed by Google. It focuses on scalability and supports large-scale projects with multiple programming languages. Bazel uses a declarative language to define build targets and dependencies. It provides caching, parallel execution, and incremental builds.</p> <p>Bazel allows to customize the build configuration based on the project's specific requirements. It allows to define additional build targets, specify compiler flags, include additional libraries. Bazel's flexibility and extensibility provide the ability to create complex build configurations for large-scale projects.</p> <p>Example of Bazel:</p> <ul> <li>Structure</li> </ul> <pre><code>MyProject/\n\u251c\u2500\u2500 WORKSPACE\n\u251c\u2500\u2500 BUILD\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 main.cc\n    \u2514\u2500\u2500 BUILD\n</code></pre> <p>In the structure:</p> <ul> <li><code>WORKSPACE</code>: The Bazel workspace file that defines the project and its external dependencies.</li> <li><code>BUILD</code>: The project-level Bazel build file that specifies build targets and dependencies.</li> <li> <p><code>src/</code>: The directory that contains the source code for the project.</p> <ul> <li><code>main.cc</code>: The C++ source file for the project.</li> <li><code>BUILD</code>: The package-level Bazel build file that specifies build targets and dependencies for the <code>src/</code> directory.</li> </ul> </li> <li> <p>WORKSPACE</p> </li> </ul> <pre><code>workspace(name = \"MyProject\")\n\n# External dependency declarations\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n\nhttp_archive(\n    name = \"googletest\",\n    url = \"https://github.com/google/googletest/archive/refs/tags/release-1.11.0.zip\",\n    sha256 = \"9be78e475e8e262ecf1a1a205ffe05db9e6a65b7003d6b2cccbc960c5e7ed23e\",\n    strip_prefix = \"googletest-release-1.11.0\",\n)\n</code></pre> <p>In the example, we define a Bazel workspace named <code>\"MyProject\"</code> and declare an external dependency on Google Test (googletest). The <code>http_archive</code> rule is used to download and configure the external dependency.</p> <ul> <li>BUILD</li> </ul> <pre><code>cc_binary(\n    name = \"myapp\",\n    srcs = [\"main.cc\"],\n    deps = [\"@googletest//:gtest_main\"],\n)\n</code></pre> <p>In the project-level BUILD file, we define a C++ binary target named \"myapp\" using the <code>cc_binary</code> rule. The <code>srcs</code> attribute specifies the source files, and the <code>deps</code> attribute declares the dependency on the Google Test framework.</p> <ul> <li>src/BUILD</li> </ul> <pre><code>cc_library(\n    name = \"mylib\",\n    srcs = [\"main.cc\"],\n    hdrs = [\"main.h\"],\n    visibility = [\"//src:__pkg__\"],\n)\n</code></pre> <p>In the package-level BUILD file, we define a C++ library target named \"mylib\" using the <code>cc_library</code> rule. The <code>srcs</code> attribute specifies the source files, the <code>hdrs</code> attribute specifies the header files, and the <code>visibility</code> attribute sets the visibility of the target within the package.</p>"},{"location":"articles/build-systems/#17-ninja","title":"1.7. Ninja","text":"<p>Ninja is a fast and efficient build system designed for speed and simplicity. It focuses on minimizing build times and providing a streamlined build experience. Ninja is commonly used as a backend for other build systems, such as CMake and Meson, to perform the actual building process.</p> <p>NOTE Ninja build files are typically generated by higher-level build systems such as CMake or Meson.</p> <p>Example of Ninja:</p> <ul> <li>Structure</li> </ul> <pre><code>MyProject/\n\u251c\u2500\u2500 build.ninja\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 main.cpp\n</code></pre> <p>In the structure:</p> <ul> <li><code>build.ninja</code>: The Ninja build file that contains the build rules, targets, and commands for compiling and linking the project.</li> <li> <p><code>src/</code>: The directory that contains the source code for the project.</p> <ul> <li><code>main.cpp</code>: The C++ source file for the project.</li> </ul> </li> <li> <p>build.ninja</p> </li> </ul> <pre><code># Define variables\nCXX = g++\nCXXFLAGS = -std=c++11 -Wall\n\n# Define build rules\nrule compile\n    command = $CXX $CXXFLAGS -c $in -o $out\n    description = Compiling $in\n\nrule link\n    command = $CXX $in -o $out\n    description = Linking $out\n\n# Define build targets\nbuild obj/main.o: compile src/main.cpp\nbuild bin/myapp: link obj/main.o\n\n# Phony target for clean\nbuild clean:\n    command = rm -rf obj bin\n    description = Cleaning build artifacts\n</code></pre> <p>In this example:</p> <ul> <li> <p>Variables: The <code>CXX</code> variable specifies the C++ compiler command, and <code>CXXFLAGS</code> sets the compiler flags.</p> </li> <li> <p>Build Rules: The <code>compile</code> rule defines how to compile source files. The <code>link</code> rule defines how to link object files into an executable. Each rule specifies the command to execute and a description for display during the build process.</p> </li> <li> <p>Build Targets: The <code>build</code> command associates build rules with specific targets. Here, we compile <code>src/main.cpp</code> into <code>obj/main.o</code> using the <code>compile</code> rule, and then link <code>obj/main.o</code> into the final executable <code>bin/myapp</code> using the <code>link</code> rule.</p> </li> <li> <p>Phony Target: The <code>clean</code> target is a phony target that is not associated with a file. It defines a command to clean the build artifacts by removing the <code>obj</code> and <code>bin</code> directories.</p> </li> <li> <p>To use the Ninja build file, invoke the <code>ninja</code> command in the directory containing the <code>build.ninja</code> file. For example, running <code>ninja</code> would execute the build steps defined in the file, compiling the source file and linking the executable.</p> </li> </ul>"},{"location":"articles/chaos-engineering/","title":"Chaos Engineering","text":"<p>Chaos engineering is a disciplined approach to building resilient and reliable software systems by intentionally introducing controlled chaos to uncover potential weaknesses and failures.</p> <ul> <li>1. Category</li> <li>1.1. Chaos Levels<ul> <li>1.1.1. Application Level Chaos</li> <li>1.1.2. Infrastructure Level Chaos</li> <li>1.1.3. Network Level Chaos</li> <li>1.1.4. Data Level Chaos</li> <li>1.1.5. People/Process Level Chaos</li> </ul> </li> <li>1.2. Tools<ul> <li>1.2.1. Chaos Monkey</li> <li>1.2.2. Azure Chaos Studio</li> </ul> </li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/chaos-engineering/#1-category","title":"1. Category","text":""},{"location":"articles/chaos-engineering/#11-chaos-levels","title":"1.1. Chaos Levels","text":"<p>Chaos engineering approach provides a unique perspective on the system's behavior and can help identify different types of weaknesses or vulnerabilities. By combining different types of chaos engineering techniques, teams can gain a more comprehensive understanding of their system's resilience and build more reliable and resilient software systems.</p>"},{"location":"articles/chaos-engineering/#111-application-level-chaos","title":"1.1.1. Application Level Chaos","text":"<p>Application Level Chaos involves injecting failure or disturbance at the application level of a software system to observe its behavior.</p> <p>This might involve intentionally introducing latency, errors, or timeouts in API requests or database queries to test how the application handles these situations.</p>"},{"location":"articles/chaos-engineering/#112-infrastructure-level-chaos","title":"1.1.2. Infrastructure Level Chaos","text":"<p>Infrastructure Level Chaos involves injecting failure or disturbance at the infrastructure level of a software system to observe its behavior.</p> <p>This might include randomly shutting down servers, disconnecting network connections, or simulating hardware failures.</p>"},{"location":"articles/chaos-engineering/#113-network-level-chaos","title":"1.1.3. Network Level Chaos","text":"<p>Network Level Chaos involves injecting failure or disturbance at the network level of a software system to observe its behavior.</p> <p>This might include throttling network traffic, introducing packet loss or delay, or simulating network outages and traffic spikes</p>"},{"location":"articles/chaos-engineering/#114-data-level-chaos","title":"1.1.4. Data Level Chaos","text":"<p>Data Level Chaos involves injecting failure or disturbance at the data level of a software system to observe its behavior.</p> <p>This might include corrupting or deleting data, introducing data inconsistencies, or simulating data breaches.</p>"},{"location":"articles/chaos-engineering/#115-peopleprocess-level-chaos","title":"1.1.5. People/Process Level Chaos","text":"<p>People/Process Level Chaos involves injecting failure or disturbance at the people or process level of a software system to observe its behavior.</p> <p>This might include intentionally misconfiguring systems, simulating human error, or testing disaster recovery procedures.</p>"},{"location":"articles/chaos-engineering/#12-tools","title":"1.2. Tools","text":""},{"location":"articles/chaos-engineering/#121-chaos-monkey","title":"1.2.1. Chaos Monkey","text":"<p>Chaos Monkey randomly terminates virtual machine instances and containers that run inside of your production environment. Exposing engineers to failures more frequently incentivizes them to build resilient services.</p>"},{"location":"articles/chaos-engineering/#122-azure-chaos-studio","title":"1.2.2. Azure Chaos Studio","text":"<p>Azure Chaos Studio is an experimentation platform for improving app resiliency.</p> <p>Improve application resilience with chaos testing by deliberately introducing faults that simulate real-world outages. Azure Chaos Studio Preview is a fully managed chaos engineering experimentation platform for accelerating discovery of hard-to-find problems, from late-stage development through production. Disrupt apps intentionally to identify gaps and plan mitigations before customers are impacted by a problem.</p>"},{"location":"articles/chaos-engineering/#2-principle","title":"2. Principle","text":"<p>Chaos engineering is based on a set of principles that guide the design and execution of experiments to uncover potential system weaknesses.</p> <p>These principles provide a framework for conducting chaos engineering experiments in a systematic and structured.</p> <ul> <li> <p>Define a Hypothesis</p> <p>Chaos engineering begins with defining a hypothesis, which is a statement about how the system will behave under certain conditions. The hypothesis is based on an understanding of the system's architecture, design, and expected behavior. The hypothesis should be clear, specific, and testable.</p> </li> <li> <p>Minimize Blast Radius</p> <p>Chaos engineering experiments should be designed to minimize the impact on the overall system. This involves carefully selecting the scope of the experiment and ensuring that it can be safely and quickly rolled back if necessary. It is important to define the blast radius carefully to avoid causing widespread damage in the system.</p> </li> <li> <p>Vary Real-World Events</p> <p>Chaos engineering involves varying real-world events to test the system's response. This might include introducing latency, errors, or other forms of disruption to see how the system behaves. The goal is to identify potential weaknesses or points of failure in the system.</p> </li> <li> <p>Automate Experiments</p> <p>Chaos engineering experiments should be automated to ensure consistency and repeatability and to reduce the risk of human error. This involves creating scripts or other tools to introduce failures or disruptions and collect data about the system's behavior.</p> </li> <li> <p>Measure everything</p> <p>Chaos engineering experiments should collect and analyze data on system behavior, performance, and response to the failure or disturbance. This data should be used to validate or refute the hypothesis and identify potential weaknesses in the system.</p> </li> <li> <p>Learn from Failures</p> <p>Chaos engineering is an iterative process that involves learning from failures and using that knowledge to improve the system. When a failure occurs, it should be analyzed to understand the root cause and determine how it can be prevented in the future.</p> </li> <li> <p>Share Results</p> <p>The results of chaos engineering experiments should be shared with the broader team to facilitate learning and improve the overall resilience of the system. This involves documenting the experiment design, results, and any insights or lessons learned.</p> </li> </ul>"},{"location":"articles/chaos-engineering/#3-best-practice","title":"3. Best Practice","text":"<p>Best practices provide guidance for conducting effective chaos engineering experiments to gain meaningful insights into system resilience.</p> <ul> <li> <p>Start Small</p> <p>Begin with small, controlled experiments to build confidence in the process and validate the tools and techniques being used.</p> </li> <li> <p>Start with a Baseline</p> <p>Before conducting any chaos engineering experiments, it is important to establish a baseline for the system's normal behavior. This involves monitoring key metrics such as response time, throughput, and error rates under normal operating conditions. By establishing a baseline, you can more easily identify deviations from normal behavior during chaos engineering experiments.</p> </li> <li> <p>Use Production-like Environments</p> <p>Chaos engineering experiments should be performed in production-like environments that replicate the complexities and dynamics of the real production environment.</p> </li> <li> <p>Involve Cross-Functional Teams</p> <p>Chaos engineering experiments should involve cross-functional teams that represent different areas of the organization, including development, operations, security, and business stakeholders. By involving a diverse group of stakeholders, you can ensure that chaos engineering experiments are aligned with the needs of the organization and that any issues that arise are addressed holistically.</p> </li> <li> <p>Focus on Business Outcomes</p> <p>When conducting chaos engineering experiments, it is important to focus on business outcomes rather than technical metrics. This means identifying key performance indicators (KPIs) that are important to the business, such as customer satisfaction, revenue, or user engagement. By focusing on business outcomes, you can ensure that your chaos engineering experiments are aligned with the overall goals of the organization.</p> </li> <li> <p>Use Realistic Scenarios</p> <p>When designing chaos engineering experiments, it is important to use realistic scenarios that are relevant to the system and its users. This might involve simulating a surge in traffic, a database outage, or other types of failure scenarios. By using realistic scenarios, you can more accurately assess the system's resilience in real-world situations.</p> </li> <li> <p>Prioritize Experiment Scenarios</p> <p>Experiment scenarios should be prioritized based on the likelihood of occurrence and the potential impact on the system. This helps ensure that the most critical scenarios are tested first.</p> </li> <li> <p>Document and Share Results</p> <p>Documenting and sharing the results of chaos engineering experiments helps to build a culture of learning and improvement. This information can also be used to develop new tests and improve existing ones.</p> </li> <li> <p>Automate Where Possible</p> <p>Automation helps to ensure consistency and repeatability in experiments. This also allows for more frequent testing and reduces the likelihood of errors caused by manual intervention.</p> </li> <li> <p>Measure Success</p> <p>Measuring the success of chaos engineering experiments is important for assessing the effectiveness of the approach and identifying areas for improvement. Metrics such as system uptime, error rates, and response times can be used to evaluate the impact of experiments on the system.</p> </li> <li> <p>Continuously Monitor and Iterate</p> <p>Chaos engineering is an iterative process that requires continuous monitoring and iteration. This means regularly monitoring the system's behavior and making adjustments to the chaos engineering experiments as necessary. By continuously monitoring and iterating, you can ensure that the system remains resilient over time.</p> </li> </ul>"},{"location":"articles/chaos-engineering/#4-terminology","title":"4. Terminology","text":"<p>Terminologies provide a common language for discussing and conducting chaos engineering experiments. By understanding these terminologies, teams can more effectively communicate and collaborate on building resilient and reliable software systems.</p> <ul> <li> <p>Chaos Engineering</p> <p>A practice that involves intentionally introducing failures and disruptions into a system to test its resiliency and identify weaknesses.</p> </li> <li> <p>Hypothesis</p> <p>A statement about how a system will behave under certain conditions. A hypothesis is the starting point for a chaos engineering experiment.</p> </li> <li> <p>Blast Radius</p> <p>The scope of a chaos engineering experiment. This refers to the parts of the system that will be affected by the experiment and the potential impact of any failures.</p> </li> <li> <p>Game Days</p> <p>A type of chaos engineering experiment that involves simulating real-world events, such as a surge in traffic or a major outage, to test the system's response.</p> </li> <li> <p>Chaos Monkey</p> <p>An open-source tool developed by Netflix that randomly terminates instances in their production environment to test the resiliency of their system.</p> </li> <li> <p>Fault Injection</p> <p>A technique used in chaos engineering that involves intentionally introducing failures or errors into a system to test its response.</p> </li> <li> <p>Steady State</p> <p>The normal operating conditions of a system. Chaos engineering experiments are designed to test how the system responds to disruptions and failures while still maintaining a steady state.</p> </li> <li> <p>Rollback Plan</p> <p>A plan for quickly reverting the system to its previous state in case a chaos engineering experiment causes a significant impact on the system.</p> </li> <li> <p>Resilience</p> <p>The ability of a system to withstand and recover from disruptions and failures.</p> </li> </ul>"},{"location":"articles/chaos-engineering/#5-references","title":"5. References","text":"<ul> <li>Microsoft chaos engineering article.</li> <li>GitHub chaos engineering principles repository.</li> </ul>"},{"location":"articles/comment/","title":"Comment","text":"<p>Comments in computer programming, is a section of code that is not executed by the program, but is instead used to provide additional information or context about the code. Comments are typically used to explain what the code does, how it works, or why certain decisions were made in the development process.</p> <ul> <li>Comment</li> <li>1. Category<ul> <li>1.1. Styles</li> <li>1.2. Tags</li> </ul> </li> <li>2. Principle</li> <li>3. Best Practice<ul> <li>3.1. Styles</li> <li>3.2. Tags</li> </ul> </li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/comment/#1-category","title":"1. Category","text":""},{"location":"articles/comment/#11-styles","title":"1.1. Styles","text":"<p>Comments are effective for providing code context and documentation and for improving code quality and maintainability. Different comment styles are used to provide different types of information about code, and it is important to use comments effectively to ensure that code is well-documented and easy to understand. By using different comment styles in appropriate ways, developers can help ensure that code is high-quality and can be easily maintained and updated over time.</p> <p>Types of Comment Styles:</p> <ol> <li> <p>Inline Comments</p> <p>Inline comments are inserted directly into the code and are used to provide explanations or clarifications about specific lines or sections of code. They are often used to describe complex algorithms or to provide context for particular sections of code. Inline comments are useful for providing additional context and can help make code more readable and understandable for other developers.</p> <p>Example:</p> <pre><code>#include &lt;iostream&gt;\n\nint myFunction(int arg1, int arg2) {\n  return arg1 + arg2;\n}\n\nint main(int argc, char *argv[]) {\n  std::cout &lt;&lt; \"Hello, world!\" &lt;&lt; std::endl;  // This is an inline comment\n\n  std::cout &lt;&lt; \"This is a single-line comment\" &lt;&lt; std::endl;\n\n  int var = myFunction();\n\n  return 0;\n}\n</code></pre> </li> <li> <p>Single-Line Comments</p> <p>Single-line comments are a concise way to provide brief explanations or clarifications about particular lines of code. They are commonly used to explain the purpose of a particular variable or function call, or to provide additional context about how a specific piece of code works. Single-line comments are often preceded by a special character or symbol, such as <code>//</code> or <code>#</code>, and are useful for providing quick, concise documentation.</p> <p>Example:</p> <pre><code>#include &lt;iostream&gt;\n\nint myFunction(int arg1, int arg2) {\n  return arg1 + arg2;\n}\n\nint main(int argc, char *argv[]) {\n  std::cout &lt;&lt; \"Hello, world!\" &lt;&lt; std::endl;\n\n  // Single-line comment\n  std::cout &lt;&lt; \"This is a single-line comment\" &lt;&lt; std::endl;\n\n  int var = myFunction();\n\n  return 0;\n}\n</code></pre> </li> <li> <p>Multi-Line Comments</p> <p>Multi-line comments are a more extensive form of documentation that can be used to describe large sections of code or to provide detailed explanations about particular algorithms or data structures. They are often enclosed in a special block of characters or symbols, such as <code>/* ... */</code>, <code>''' ... '''</code>, or preceded by a triple forward slash <code>///</code> and can be used to provide comprehensive documentation for complex sections of code.</p> <p>Example:</p> <pre><code>#include &lt;iostream&gt;\n\nint myFunction(int arg1, int arg2) {\n  return arg1 + arg2;\n}\n\nint main(int argc, char *argv[]) {\n  std::cout &lt;&lt; \"Hello, world!\" &lt;&lt; std::endl;\n\n  std::cout &lt;&lt; \"This is a single-line comment\" &lt;&lt; std::endl;\n\n  //  Multi-line comment\n  //  This is a multi-line comment.\n  //  It can span multiple lines.\n\n  int var = myFunction();\n\n  return 0;\n}\n</code></pre> </li> <li> <p>Comment Tags</p> <p>Comment tags are used to indicate that a particular section of code requires attention or modification. They often include tags like <code>TODO</code>, <code>FIXME</code>, or <code>HACK</code>, and are used to help developers quickly identify sections of code that need to be addressed. Comment tags are useful for ensuring that code is continuously updated and maintained, and can help prevent issues from arising in the future.</p> <p>Example:</p> <pre><code>#include &lt;iostream&gt;\n\nint myFunction(int arg1, int arg2) {\n  // TODO(Sentenz) This is a comment tag to add error handling\n  return arg1 + arg2;\n}\n\nint main(int argc, char *argv[]) {\n  std::cout &lt;&lt; \"Hello, world!\" &lt;&lt; std::endl;\n\n  std::cout &lt;&lt; \"This is a single-line comment\" &lt;&lt; std::endl;\n\n  int var = myFunction();\n\n  return 0;\n}\n</code></pre> </li> <li> <p>Documentation Comments</p> <p>Documentation comments are a special type of comment that is used to generate documentation for the code. They are often used to describe functions, classes, or other components of the code, and can be automatically extracted to create documentation that can be shared with other developers. Documentation comments are a critical tool for ensuring that code is well-documented and easy to understand, and can help developers quickly understand the purpose and functionality of different parts of the code.</p> <p>Example:</p> <pre><code>/// @file This file contains the main function for the program.\n\n#include &lt;iostream&gt;\n\n/// @brief A brief description of the function.\n/// @details This is a detailed description of the function. It can span multiple lines and can\n/// include examples, parameter descriptions, and return value descriptions.\n/// @param arg1 A description of the first argument.\n/// @param arg2 A description of the second argument.\n/// @return A description of the return value.\nint myFunction(int arg1, int arg2) {\n  return arg1 + arg2;\n}\n\nint main(int argc, char *argv[]) {\n  std::cout &lt;&lt; \"Hello, world!\" &lt;&lt; std::endl;\n\n  std::cout &lt;&lt; \"This is a single-line comment\" &lt;&lt; std::endl;\n\n  int var = myFunction();\n\n  return 0;\n}\n</code></pre> </li> </ol>"},{"location":"articles/comment/#12-tags","title":"1.2. Tags","text":"<p>Comment tags in programming are used to annotate code with additional information or notes that can help developers better understand, maintain, and improve the code. These tags usually take the form of a specific syntax, such as a double slash <code>//</code> in most programming languages, and are followed by a specific keyword or label that indicates the type of comment.</p> <p>Comment tags are helpful in managing large codebases or collaborative projects, as they track specific issues, tasks, or requirements.</p> <p>Types of Comment Tags:</p> <ol> <li> <p><code>TODO</code></p> <p>Indicates code that needs to be completed or implemented. The tag usually includes a description of what needs to be done to complete the task.</p> <p>Example:</p> <pre><code>// TODO Implement error handling for edge cases in function `myFunction()` based `Defence Programming Principle`\n</code></pre> </li> <li> <p><code>FIX</code></p> <p>Indicates code that needs to be fixed. The tag usually includes a description of what is wrong with the code.</p> <p>Example:</p> <pre><code>// FIX Modify typo in variable name `userNmae` to `userName` \n</code></pre> </li> <li> <p><code>HACK</code></p> <p>Indicates code that is a quick and dirty solution to a problem. The tag usually includes a description of why the code is a hack and what needs to be done to create a more permanent solution.</p> <p>Example:</p> <pre><code>// HACK Use a global variable to store the current user's information until a proper authentication system can be implemented\n</code></pre> </li> <li> <p><code>OPTIMIZE</code></p> <p>Indicates code that needs to be optimized for performance or memory usage. The tag usually includes a description of how the code can be optimized.</p> <p>Example:</p> <pre><code>// OPTIMIZE Replace nested for-loops with a more efficient algorithm to improve performance\n</code></pre> </li> <li> <p><code>REFACTOR</code></p> <p>Indicates code code that needs to be refactored to improve its structure or organization. The tag usually includes a description of how the code can be refactored.</p> <p>Example:</p> <pre><code>// REFACTOR Extract common functionality from `myFunction()` and `myOtherFunction()` into a separate utility function\n</code></pre> </li> <li> <p><code>REVIEW</code></p> <p>Indicates code that needs to be reviewed by other programmers. The tag usually includes a description of what needs to be reviewed.</p> <p>Example:</p> <pre><code>// REVIEW Verify the implementation of the `login()` function for potential security vulnerabilities\n</code></pre> </li> <li> <p><code>DEBUG</code></p> <p>Indicates code that needs to be debugged. The tag usually includes a description of the problem and any steps that have been taken to debug the code.</p> <p>Example:</p> <pre><code>// DEBUG Use a debugger to identify the source of the segmentation fault in the `run()` function\n</code></pre> </li> <li> <p><code>DOC</code></p> <p>Indicates code that needs to be documented. The tag usually includes a description of what needs to be documented and how it should be documented.</p> <p>Example:</p> <pre><code>// DOC Add documentation to the `calculate()` function to explain its parameters and return value\n</code></pre> </li> <li> <p><code>SECURITY</code></p> <p>Indicates code that needs to be reviewed for security vulnerabilities. The tag usually includes a description of the potential vulnerabilities and how they can be addressed.</p> <p>Example:</p> <pre><code>// SECURITY Review the use of user input in the `executeCommand()` function for potential injection attacks\n</code></pre> </li> <li> <p><code>BUG</code></p> <p>Indicates a piece of code that contains a bug or error that needs to be fixed. The tag usually includes a description of the bug and steps to reproduce it.</p> <p>Example:</p> <pre><code>// BUG Fix the null pointer exception that occurs when calling `myObject-&gt;myMethod()`\n</code></pre> </li> <li> <p><code>XXX</code></p> <p>Indicate code that may be incomplete, incorrect, or not working as intended. It is typically used to highlight issues that can cause errors, crashes, or other problems in the software. It is similar to the <code>TODO</code> tag but typically denotes a more serious issue that requires immediate attention.</p> <p>Example:</p> <pre><code>// XXX Investigate the memory leak that occurs when using the `myLibrary` API\n</code></pre> </li> <li> <p><code>NIT</code></p> <p>Indicates a small, minor issue or nitpick with the code that does not necessarily need to be fixed, but could be improved. The tag usually includes a description of the issue or nitpick.</p> <p>Example:</p> <pre><code>// NIT Replace the magic number 42 with a named constant to improve code readability\n</code></pre> </li> <li> <p><code>NOTE</code></p> <p>Indicates code that contains a note or reminder for future reference. The tag usually includes a description of the note or reminder and may also include suggestions for how to address it. This tag can be useful for team members to communicate important information about the code.</p> <p>Example:</p> <pre><code>// NOTE This function assumes that the input is sorted in ascending order\n</code></pre> </li> </ol>"},{"location":"articles/comment/#2-principle","title":"2. Principle","text":"<ul> <li> <p>Clarity</p> <p>Clear documentation comments help developers understand the code's intended functionality, how it works, and how to use it correctly, which is particularly important in complex projects or when dealing with legacy code.</p> </li> <li> <p>Maintainability</p> <p>Documentation contributes to the code's maintainability, making it easier for developers to update, modify, or fix the code even if they were not the original authors. This ensures that the code remains relevant and can be extended or improved over time without introducing errors.</p> </li> <li> <p>Collaboration</p> <p>Documentation promotes collaboration by allowing team members to understand each other's code, share knowledge, and work together more effectively. It serves as a reference for team members, facilitating communication and coordination in software development projects.</p> </li> <li> <p>Sparingly</p> <p>Comments should only be used where necessary to provide additional context or documentation. Overuse of comments can make the code more difficult to read and understand, as irrelevant or unnecessary information can distract from the code.</p> </li> <li> <p>Consistent formatting</p> <p>Consistent formatting of comments makes the code more readable and easier to understand. Use the same comment characters or symbols and indentation for all comments.</p> </li> <li> <p>Update</p> <p>Comments should be updated as the code evolves to ensure that they remain accurate and up-to-date. Periodically review comments to ensure that they continue to provide accurate and useful information about the code.</p> </li> </ul>"},{"location":"articles/comment/#3-best-practice","title":"3. Best Practice","text":""},{"location":"articles/comment/#31-styles","title":"3.1. Styles","text":"<ul> <li> <p>Context</p> <p>Comments should be used to provide context about the code, explaining its purpose, functionality, and implementation details. Providing context helps other developers understand the code and makes it easier to maintain, especially when dealing with complex projects.</p> </li> <li> <p>Conciseness</p> <p>Comments should be concise and focused, providing only the necessary information about the code. Avoid including unnecessary detail, which can make the code harder to read and understand. Keeping comments concise helps keep the code readable and easier to understand.</p> </li> <li> <p>Clear language</p> <p>Comments should use clear and simple language that is easily understandable to other developers. Avoid using technical jargon or overly complex terms, as it can create confusion among developers who may not have the same level of technical expertise.</p> </li> <li> <p>Consistent formatting</p> <p>Consistent formatting of comments helps make the code more readable and easier to understand. Use the same comment characters or symbols and indentation for all comments. Following consistent formatting makes it easier for developers to scan the code and quickly understand its structure.</p> </li> <li> <p>Meaningful names</p> <p>Using meaningful names for variables, functions, and classes can reduce the need for comments by making the code more self-documenting. Meaningful names that reflect the code's purpose and functionality can help developers quickly understand what the code does without relying solely on comments.</p> </li> <li> <p>Avoid redundancy</p> <p>Avoid including redundant comments that simply repeat what is already obvious from the code. This can make the code harder to read and understand, as well as increase the maintenance effort. Instead, focus on providing useful information that cannot be easily deduced from the code itself.</p> </li> </ul>"},{"location":"articles/comment/#32-tags","title":"3.2. Tags","text":"<ul> <li> <p>Consistency</p> <p>To ensure consistency across the codebase, use the same tags throughout your code. This makes it easier to identify and search for code that needs attention and ensures that everyone on the team is on the same page.</p> </li> <li> <p>Simplicity</p> <p>Use short, descriptive tags that are easy to read and understand. Avoid using complicated tags or acronyms that may not be clear to everyone, keeping in mind that simplicity can help to make the code more readable and understandable.</p> </li> <li> <p>Specificity</p> <p>Use specific tags such as <code>TODO</code>, <code>FIXME</code>, and <code>BUG</code> to highlight specific issues in the code that need attention. This helps developers to prioritize their work and maintain the codebase.</p> </li> <li> <p>Sparingly</p> <p>Overusing comment tags can make the code harder to read and understand, so use tags only when necessary, such as when code needs to be fixed, tested, or documented.</p> </li> <li> <p>Avoiding redundancy</p> <p>Avoid using tags that duplicate existing functionality or are redundant. For example, using both <code>FIXME</code> and <code>BUG</code> tags to indicate the same issue is unnecessary and can make the code harder to read.</p> </li> <li> <p>Regular review</p> <p>Regularly review comment tags to ensure their relevance and necessity. Remove tags that are no longer needed and update or add new tags as necessary.</p> </li> </ul>"},{"location":"articles/comment/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Comment</p> <p>A comment is a section of code that is not executed by the program, but is instead used to provide context or documentation about the code.</p> </li> <li> <p>Comment headers</p> <p>Comment headers are blocks of comments that are used to provide an overview of a file or section of code. They typically include information about the author, date, purpose, and other relevant information.</p> </li> <li> <p>Comment blocks</p> <p>Comment blocks are blocks of comments that are used to provide context or explanations about code. They can be used to provide a detailed explanation of a function, class, or other block of code.</p> </li> <li> <p>Comment tags</p> <p>Comment tags are special keywords that are used to provide additional information about code. They are typically used to mark certain sections of code, such as <code>TODO</code> or <code>FIXME</code>, or to provide additional context about a block of code.</p> </li> </ul>"},{"location":"articles/comment/#5-references","title":"5. References","text":"<ul> <li>Sentenz docs as code article.</li> <li>Wikipedia comment article.</li> </ul>"},{"location":"articles/configuration-management/","title":"Configuration Management","text":"<p>Configuration management is a set of processes and tools used to systematically manage and control the configuration of software, hardware, or any other IT infrastructure. It involves identifying and documenting the configuration item (CI), tracking changes made to these items, and ensuring that the system remains stable and consistent.</p> <ul> <li>1. Category</li> <li>1.1. Ansible</li> <li>1.2. Puppet</li> <li>1.3. Chef</li> <li>1.4. SaltStack</li> <li>1.5. Terraform</li> <li>1.6. Kubernetes</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/configuration-management/#1-category","title":"1. Category","text":""},{"location":"articles/configuration-management/#11-ansible","title":"1.1. Ansible","text":"<p>Ansible is an open-source automation tool used for configuration management, application deployment, and orchestration. Ansible uses a declarative language to define configurations and can work with a wide range of systems, making it a versatile choice. It simplifies the process of managing and controlling systems by providing a simple and human-readable language to define configurations, called YAML (Yet Another Markup Language).</p> <p>Ansible has a large and active community, providing continuous development, support, and a vast collection of community-contributed roles and playbooks. It can be used for managing configurations of various systems, including Linux, Windows, network devices, and cloud platforms.</p> <p>Features of Ansible:</p> <ol> <li> <p>Agentless architecture</p> <p>Ansible operates in an agentless manner, which means it doesn't require any software to be installed on the managed nodes. It communicates with the nodes using SSH or PowerShell, making it lightweight and easy to setup.</p> </li> <li> <p>Declarative language</p> <p>Ansible uses YAML syntax to define configurations, making it easy to understand and write. YAML allows to specify the desired state of a system rather than writing procedural steps, which simplifies configuration management.</p> </li> <li> <p>Playbooks</p> <p>Playbooks are Ansible's configuration files that describe the desired state of a system. Playbooks consist of tasks written in YAML, which Ansible executes on the managed nodes. Playbooks provide a way to automate complex configuration tasks and orchestrate multi-step processes.</p> </li> <li> <p>Inventory management</p> <p>Ansible uses an inventory file to define the managed nodes in the environment. The inventory file can be a simple text file or dynamically generated using external sources like cloud providers or scripts. It allows to group and categorize hosts, making it easy to target specific systems with configurations.</p> </li> <li> <p>Modules</p> <p>Ansible comes with a vast library of pre-built modules that cover a wide range of tasks, such as package management, file manipulation, user management, network configuration, and more. Modules are reusable, idempotent components that Ansible uses to perform actions on the managed nodes.</p> </li> <li> <p>Roles</p> <p>Roles are a way to organize and reuse playbooks and tasks. They provide a modular structure for configuration management, allows to separate different components or layers of the infrastructure into reusable units. Roles make it easier to share and collaborate on configurations.</p> </li> <li> <p>Idempotency</p> <p>Ansible promotes the concept of idempotency, which means that applying a configuration multiple times should have the same result as applying it once. This ensures that the system remains in the desired state regardless of how many times a playbook is executed.</p> </li> <li> <p>Extensibility</p> <p>Ansible is highly extensible and allows to create custom modules, plugins, and extensions to tailor it to specific needs. Integrate Ansible with other tools, such as source control systems, monitoring tools, and cloud platforms, to enhance its capabilities.</p> </li> </ol> <p>Example of Ansible:</p> <ol> <li> <p>Web Server</p> <p>Apache HTTP Server is a free and open-source web server that delivers web content as Web Pages through the internet.</p> <p>An Ansible playbook that installs a package and starts a service on a group of remote servers:</p> <pre><code>---\n- name: Install and start Apache\n  hosts: web_servers\n  become: yes\n  tasks:\n    - name: Update package cache\n      apt:\n        update_cache: yes\n      when: ansible_pkg_mgr == 'apt'\n\n    - name: Install Apache package\n      package:\n        name: apache2\n        state: present\n\n    - name: Start Apache service\n      service:\n        name: apache2\n        state: started\n</code></pre> <p>In the Ansible playbook:</p> <ul> <li>The playbook is named \"Install and start Apache.\"</li> <li>The playbook targets a group of remote servers defined in the Ansible inventory under the group name \"web_servers.\"</li> <li>The <code>become: yes</code> directive enables privilege escalation, allowing the playbook to run with administrative privileges on the remote servers.</li> <li>The playbook consists of three tasks.</li> <li>The first task updates the package cache using the <code>apt</code> module (for systems that use APT package manager, like Ubuntu) to ensure the latest package information is available.</li> <li>The second task installs the Apache package using the <code>package</code> module. It specifies the package name as \"apache2\" and the state as \"present\" to ensure the package is installed.</li> <li>The third task starts the Apache service using the <code>service</code> module. It specifies the service name as \"apache2\" and the state as \"started\" to ensure the service is running.</li> </ul> <p>To execute this playbook, save it to a file (e.g., <code>apache.yml</code>) and run the following command:</p> <pre><code>ansible-playbook apache.yml\n</code></pre> <p>Ansible will connect to the remote servers defined in the \"web_servers\" group and execute the tasks defined in the playbook, installing Apache and starting the service.</p> <p>NOTE The example assumes a setup SSH connectivity and the necessary privileges on the remote servers to perform the tasks. Also, the package manager used in the example is specific to systems using APT. If a different package manager is used, modify the task accordingly.</p> </li> </ol>"},{"location":"articles/configuration-management/#12-puppet","title":"1.2. Puppet","text":"<p>Puppet is an open-source configuration management tool that focuses on automating the provisioning, configuration, and management of software and infrastructure. It provides a declarative language to define configurations and uses a client-server architecture for managing systems.</p> <p>NOTE Puppet need to have a Puppet server setup and the Puppet agent installed and configured on the target systems. The agent will periodically connect to the server to retrieve and apply the configurations. Additionally, the Puppet server and agent communication requires proper certificate management for security</p> <p>Features of Puppet:</p> <ol> <li> <p>Declarative Language</p> <p>Puppet uses a declarative language (Puppet DSL) to define the desired state of the systems. Instead of specifying procedural steps, declare what the system should look like, and Puppet handles the underlying actions to achieve that state. This approach simplifies configuration management and makes it easier to reason about system configurations.</p> </li> <li> <p>Resource Abstraction</p> <p>Puppet treats system components as \"resources\" that need to be managed. Resources can include packages, files, services, users, groups, and more. Puppet provides an extensive collection of built-in resource types that allow to manage various aspects of the infrastructure. Additionally, Puppet allows to create custom resource types to handle specific needs.</p> </li> <li> <p>Cross-Platform Support</p> <p>Puppet supports a wide range of operating systems and platforms, making it suitable for managing diverse infrastructures. Whether working with Linux, Windows, macOS, or various cloud platforms, Puppet provides a consistent approach to configuration management across different environments.</p> </li> <li> <p>Idempotency</p> <p>Puppet follows the principle of idempotency, ensuring that applying a configuration multiple times results in the same desired state. Puppet keeps track of the current state of resources and only applies changes that are necessary to bring the system to the desired state. This ensures that configurations are reliable, consistent, and minimizes unnecessary changes.</p> </li> <li> <p>Module Ecosystem</p> <p>Puppet has a rich ecosystem of modules that provide pre-built configurations for popular software applications, services, and infrastructure components. These modules, available through Puppet Forge, can be easily integrated into configurations, saving time and effort in setting up and managing various components of the infrastructure.</p> </li> <li> <p>Puppet Language Extensions</p> <p>Puppet offers a flexible and extensible language that allows to extend its capabilities. Write custom functions, facts, and types in Ruby or use Puppet's embedded Ruby (ERB) templating to create dynamic configurations. These language extensions provide the flexibility to adapt Puppet to specific requirements or integrate with external systems.</p> </li> <li> <p>Reporting and Auditing</p> <p>Puppet provides reporting and auditing features that enables to track and monitor changes made to infrastructure. It generates reports that show the status of resources, changes applied, and any errors or warnings encountered during configuration runs. These reports help in troubleshooting, compliance auditing, and maintaining a historical record of system configurations.</p> </li> <li> <p>Scalability and Orchestration</p> <p>Puppet can handle large-scale infrastructures with ease. It supports a client-server architecture where Puppet agents (clients) periodically connect to a Puppet master to retrieve configurations. This architecture allows for centralized management, distribution of configurations at scale, and coordination across a large number of systems.</p> </li> </ol> <p>Example of Puppet:</p> <ol> <li> <p>Web Server</p> <p>A Puppet manifest that installs and configures the Apache web server:</p> <pre><code>class apache {\n  package { 'apache2':\n    ensure =&gt; installed,\n  }\n\n  service { 'apache2':\n    ensure  =&gt; running,\n    enable  =&gt; true,\n    require =&gt; Package['apache2'],\n  }\n\n  file { '/var/www/html/index.html':\n    ensure  =&gt; present,\n    content =&gt; 'Hello, World!',\n    require =&gt; Package['apache2'],\n  }\n}\n\ninclude apache\n</code></pre> <p>In this example:</p> <ul> <li>The Puppet manifest defines a class named \"apache\" that represents the Apache configuration.</li> <li>The <code>package</code> resource installs the Apache package (<code>apache2</code>) and ensures it is present on the system.</li> <li>The <code>service</code> resource ensures the Apache service is running and enabled to start on system boot. It depends on the <code>Package['apache2']</code> resource, so it will only execute if the package is successfully installed.</li> <li>The <code>file</code> resource creates an index.html file with the specified content (<code>Hello, World!</code>) at the given path (<code>/var/www/html/index.html</code>). It also depends on the <code>Package['apache2']</code> resource.</li> </ul> <p>Finally, the <code>include apache</code> statement includes the \"apache\" class in the manifest, which ensures that Puppet applies the configurations defined within it.</p> <p>To apply this Puppet manifest, save it to a file (e.g., <code>apache.pp</code>) and run the following command on the Puppet server:</p> <pre><code>puppet apply apache.pp\n</code></pre> <p>Puppet will connect to the target systems (called Puppet agents) and apply the configurations defined in the manifest. It will install Apache, start the service, and create the index.html file with the specified content.</p> </li> </ol>"},{"location":"articles/configuration-management/#13-chef","title":"1.3. Chef","text":"<p>Chef is a powerful configuration management tool that automates the provisioning, configuration, and management of systems and infrastructure. It provides a flexible and scalable approach to infrastructure automation, allowing to define configurations as code and manage them consistently across multiple platforms.</p> <p>Features of Chef:</p> <ol> <li> <p>Infrastructure as Code</p> <p>Chef treats infrastructure configurations as code, using a domain-specific language called the Chef DSL (Ruby-based) or newer versions that support plain Ruby. This allows to define the infrastructure and configurations in a human-readable and version-controlled format, enabling easy collaboration, reproducibility, and scalability.</p> </li> <li> <p>Cookbooks and Recipes</p> <p>Chef organizes configurations into cookbooks, which are self-contained units of configurations. Cookbooks consist of recipes that define the desired state of specific components or services. Recipes specify the resources and actions required to achieve the desired state, such as installing packages, managing files, and configuring services.</p> </li> <li> <p>Resource Abstraction</p> <p>Chef uses a resource-based model to manage system components. Resources represent specific elements like packages, files, services, users, and more. Chef provides a large collection of built-in resources that can be used to manage various aspects of the infrastructure. Also create custom resources to handle specific requirements.</p> </li> <li> <p>Chef Server</p> <p>Chef uses a client-server architecture. The Chef server acts as a central repository for storing and distributing configurations, cookbooks, and other related data. The Chef client, running on managed nodes, connects to the Chef server to retrieve configurations and apply them. The server-client model allows for centralized management, version control, and scaling to large infrastructures.</p> </li> <li> <p>Chef Solo and Chef Zero</p> <p>In addition to the client-server model, Chef offers standalone modes called Chef Solo and Chef Zero. These modes allow to use Chef without a central server. Chef Solo runs configurations locally on individual nodes, while Chef Zero allows for a server-like experience without the need for a separate server.</p> </li> <li> <p>Community Cookbooks</p> <p>Chef has a vibrant community that contributes to a vast ecosystem of community cookbooks. These cookbooks provide pre-built configurations for popular software, services, and infrastructure components. Leveraging community cookbooks saves time and effort by reusing tested and proven configurations and enables to benefit from the collective knowledge of the Chef community.</p> </li> <li> <p>Testing and Continuous Delivery</p> <p>Chef supports testing frameworks like Test Kitchen, InSpec, and ChefSpec, which help ensure the correctness of configurations and validate desired states. These testing tools facilitate the adoption of continuous delivery practices by allowing to automate testing and validation as part of the deployment pipeline.</p> </li> <li> <p>Integrations and Extensibility</p> <p>Chef integrates with other tools and services in the ecosystem, such as version control systems (e.g., Git), cloud platforms (e.g., AWS, Azure), and monitoring systems. Chef can be extended through plugins, custom resources, and community-developed extensions to meet specific requirements or integrate with external systems.</p> </li> </ol> <p>Example of Chef:</p> <ol> <li> <p>Web Server</p> <p>A Chef recipe that installs and configures the Apache web server:</p> <pre><code>package 'apache2' do\n  action :install\nend\n\nservice 'apache2' do\n  action [:enable, :start]\nend\n\ntemplate '/var/www/html/index.html' do\n  source 'index.html.erb'\n  mode '0644'\nend\n</code></pre> <p>In the example:</p> <ul> <li>The <code>package</code> resource installs the Apache package (<code>apache2</code>) using the default package manager of the underlying operating system.</li> <li>The <code>service</code> resource ensures that the Apache service is enabled and started.</li> <li>The <code>template</code> resource deploys a customized <code>index.html</code> file to the specified location (<code>/var/www/html/index.html</code>). It uses an ERB (Embedded Ruby) template (<code>index.html.erb</code>) to dynamically generate the content of the file.</li> </ul> <p>To apply this Chef recipe:</p> <ul> <li>Save the recipe to a file, such as <code>default.rb</code>.</li> <li>setup a Chef workstation with the Chef Development Kit (ChefDK) installed.</li> <li>setup a Chef server or use Chef Solo/Zero if the prefer a standalone mode.</li> <li>Upload the recipe to the Chef server or copy it to the target node if using Chef Solo/Zero.</li> <li>Run the Chef client on the target node to apply the configuration:</li> </ul> <pre><code>sudo chef-client\n</code></pre> <p>Chef will converge the system to the desired state defined in the recipe by installing the Apache package, starting the Apache service, and deploying the customized index.html file.</p> <p>NOTE This example assumes a client-server setup with a Chef server. If Chef Solo or Chef Zero are used, run the <code>chef-client</code> command locally on the target node.</p> <p>Additionally, provide the <code>index.html.erb</code> template file, which could contain embedded Ruby code for dynamic content generation. The template could be stored in the <code>templates/default</code> directory relative to the recipe file.</p> </li> </ol>"},{"location":"articles/configuration-management/#14-saltstack","title":"1.4. SaltStack","text":"<p>SaltStack (Salt) is an open-source infrastructure automation and configuration management tool. Salt follows a master-minion architecture, where a central Salt master manages and controls multiple Salt minions (managed nodes).</p> <p>SaltStack is a powerful tool for infrastructure automation, remote execution, and configuration management. It excels in managing large-scale, dynamic environments and enables rapid deployment and management of complex infrastructures using a Python-based configuration language.</p> <p>Features of SaltStack:</p> <ol> <li> <p>Remote Execution</p> <p>Salt allows for executing commands and running scripts on remote systems, providing powerful remote execution capabilities. It uses a secure communication protocol called ZeroMQ to efficiently communicate with minions and execute commands in parallel, enabling rapid and scalable operations across a large number of nodes.</p> </li> <li> <p>Configuration Management</p> <p>Salt provides robust configuration management features similar to other tools like Puppet and Chef. It allows to define and enforce the desired state of the infrastructure by managing configuration files, packages, services, users, and more. Salt states, written in YAML or Jinja, describe the desired state of resources and can be used to ensure consistency and repeatability in system configurations.</p> </li> <li> <p>Event-Driven Infrastructure Automation</p> <p>Salt operates on an event-driven model, where events and changes in the infrastructure trigger automated responses. This approach allows to define reactive policies and automate tasks based on specific events or conditions. It facilitates real-time monitoring, auto-remediation, and event-driven orchestration.</p> </li> <li> <p>Salt States and Formulas</p> <p>Salt states provide a way to define the desired state of resources in a declarative manner. States can be organized into reusable units called formulas, which encapsulate configurations for specific applications or components. Formulas can be shared and applied across multiple systems, promoting consistency and simplifying management.</p> </li> <li> <p>Orchestration</p> <p>Salt includes powerful orchestration capabilities that allows to define complex workflows and coordinate tasks across multiple systems. Orchestration in Salt can be used for provisioning, application deployment, and other multi-step processes. It provides granular control over the order of execution, parallelization, and error handling.</p> </li> <li> <p>Salt Cloud</p> <p>Salt Cloud is a module in SaltStack that enables cloud infrastructure management. It provides integrations with popular cloud platforms, allowing to manage and provision virtual machines and resources in public or private clouds. Salt Cloud automates the process of spinning up instances, managing networks, and configuring them using SaltStack.</p> </li> <li> <p>Salt Pillar</p> <p>Salt Pillar is a secure and encrypted data store used to store sensitive information such as passwords, private keys, and other secrets. It allows to securely share data between the Salt master and minions without exposing it in the state files or formulas. Salt Pillar supports various backends like YAML, JSON, and key-value stores.</p> </li> <li> <p>Extensibility</p> <p>Salt is highly extensible and provides various mechanisms for customization and integration with other tools and systems. It supports custom modules, execution modules, and states, allowing to extend its functionality and integrate with external systems. Additionally, Salt provides APIs and SDKs for programmatic access and integration with other automation workflows.</p> </li> </ol> <p>Example of SaltStack:</p> <ol> <li> <p>Web Server</p> <p>A SaltStack configuration for managing an Apache web server.</p> <ul> <li>Create a Salt state file for Apache:</li> </ul> <p>Create a file named <code>apache.sls</code> in the SaltStack states directory (e.g., <code>/srv/salt</code>):</p> <pre><code>apache:\n  pkg.installed\n\n/etc/apache2/sites-available/000-default.conf:\n  file.managed:\n    - source: salt://apache/files/000-default.conf\n    - template: jinja\n\napache.service:\n  service.running:\n    - enable: True\n    - require:\n      - pkg: apache\n      - file: /etc/apache2/sites-available/000-default.conf\n</code></pre> <ul> <li>Create the Apache configuration file:</li> </ul> <p>Create a file named <code>000-default.conf</code> in the SaltStack files directory (e.g., <code>/srv/salt/apache/files</code>):</p> <pre><code>&lt;VirtualHost *:80&gt;\n  ServerAdmin webmaster@localhost\n  DocumentRoot /var/www/html\n  ErrorLog ${APACHE_LOG_DIR}/error.log\n  CustomLog ${APACHE_LOG_DIR}/access.log combined\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>Apply the SaltStack configuration:</li> </ul> <p>On the Salt master, run the following command to apply the configuration to the minions:</p> <pre><code>salt 'minion1' state.apply apache\n</code></pre> <p>Replace <code>'minion1'</code> with the identifier of the target minion.</p> <p>This configuration will perform the following actions:</p> <ul> <li>Install the Apache package on the minion using the <code>pkg.installed</code> state.</li> <li>Deploy the <code>000-default.conf</code> configuration file to the <code>/etc/apache2/sites-available/</code> directory on the minion using the <code>file.managed</code> state. The file will be templated using Jinja.</li> <li>Start and enable the Apache service on the minion using the <code>service.running</code> state. It requires the Apache package to be installed and the configuration file to be present.</li> </ul> <p>By applying this configuration, SaltStack will ensure that the Apache package is installed, the configuration file is properly deployed, and the Apache service is running on the target minion.</p> <p>NOTE Make sure to have a SaltStack infrastructure setup with a master and at least one minion before applying this configuration. Adjust the file paths and package names as per the operating system and Apache setup.</p> </li> </ol>"},{"location":"articles/configuration-management/#15-terraform","title":"1.5. Terraform","text":"<p>Terraform is an open-source infrastructure as code (IaC) tool for provisioning and managing cloud resources and infrastructure in a declarative manner. It allows to define and automate the creation, modification, and destruction of infrastructure components across various cloud providers and platforms.</p> <p>Features of Terraform:</p> <ol> <li> <p>Declarative Configuration</p> <p>Terraform uses a declarative language called HashiCorp Configuration Language (HCL) to define infrastructure configurations. HCL describes the desired state of the infrastructure rather than specifying the steps to achieve it. Terraform then determines the necessary actions to bring the infrastructure to the desired state.</p> </li> <li> <p>Multi-Cloud Support</p> <p>Terraform supports multiple cloud providers, including Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and many others. It provides a consistent workflow and syntax, allowing to manage resources across different cloud environments using a unified approach.</p> </li> <li> <p>Infrastructure Graph</p> <p>Terraform creates a dependency graph of the defined resources and their relationships. This graph enables Terraform to determine the correct order of provisioning resources and efficiently manage the creation and deletion of resources. It also helps in visualizing and understanding the dependencies between infrastructure components.</p> </li> <li> <p>Resource Provisioning and Management</p> <p>Terraform supports a wide range of resource types offered by cloud providers, including virtual machines, networks, storage, load balancers, databases, and more. It allows to define and manage these resources, specify their attributes, and control their lifecycle, such as creating, modifying, and destroying resources as needed.</p> </li> <li> <p>Plan and Apply</p> <p>Terraform follows a two-step process</p> <p>planning and applying. During the planning phase, Terraform examines the configuration files and creates an execution plan that shows the proposed changes to the infrastructure. Review this plan before proceeding. In the apply phase, Terraform applies the changes and provisions or modifies the resources accordingly.</p> </li> <li> <p>State Management</p> <p>Terraform maintains a state file that tracks the current state of the infrastructure. This file records the resources created and their configurations. The state file is used to determine the delta between the desired state and the actual state and to perform operations like resource updates and deletions. Terraform supports remote backends for storing the state file securely and allows collaboration among team members.</p> </li> <li> <p>Infrastructure Collaboration</p> <p>Terraform supports collaboration by allowing multiple team members to work on the same infrastructure code. It provides mechanisms for remote state storage and locking, enabling safe concurrent modifications to the infrastructure configuration. This allows for team collaboration and version-controlled infrastructure changes.</p> </li> <li> <p>Modules and Reusability</p> <p>Terraform supports the use of modules, which are reusable units of infrastructure configurations. Modules can encapsulate a set of resources and their configurations, providing an abstraction layer for reuse across projects. Modules can be shared with others or published to the Terraform Registry, fostering a vibrant ecosystem of community-contributed modules.</p> </li> </ol> <p>Example of</p> <ol> <li> <p>AWS EC2</p> <p>A Terraform configuration that provisions an AWS EC2 instance.</p> <ul> <li>setup the Terraform configuration file:</li> </ul> <p>Create a file named <code>main.tf</code> with the following content:</p> <pre><code>provider \"aws\" {\n  region = \"us-west-2\"\n}\n\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0c94855ba95c71c99\"\n  instance_type = \"t2.micro\"\n}\n</code></pre> <p>In the example:</p> <ul> <li>The <code>provider</code> block specifies the AWS provider and sets the region to \"us-west-2\".</li> <li> <p>The <code>aws_instance</code> resource provisions an EC2 instance.</p> <ul> <li>The <code>ami</code> attribute specifies the Amazon Machine Image (AMI) ID.</li> <li>The <code>instance_type</code> attribute defines the EC2 instance type.</li> </ul> </li> <li> <p>Initialize and apply the Terraform configuration:</p> </li> </ul> <p>Open a terminal and navigate to the directory containing the <code>main.tf</code> file. Run the following commands:</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>The <code>terraform init</code> command initializes the Terraform working directory, downloading the necessary provider plugins.</p> <p>The <code>terraform apply</code> command applies the Terraform configuration, provisions the EC2 instance based on the defined resources, and prompts for confirmation. Enter \"yes\" to proceed.</p> <ul> <li>Verify the provisioned resources:</li> </ul> <p>Once the Terraform apply completes, it will display the provisioned resources. In this case, it will output the details of the provisioned EC2 instance.</p> <p>Also verify the resources in the AWS Management Console, specifically in the EC2 service, and find the newly created instance.</p> <ul> <li>Clean up the provisioned resources:</li> </ul> <p>To clean up the provisioned resources and avoid any unintended costs, run:</p> <pre><code>terraform destroy\n</code></pre> <p>The <code>terraform destroy</code> command will prompt for confirmation before destroying the provisioned resources. Enter \"yes\" to proceed.</p> <p>NOTE Make sure to have AWS credentials configured on the system, either through environment variables or using the AWS CLI configuration (<code>aws configure</code>).</p> <p>Terraform allows to define more complex configurations with networking, load balancers, security groups, and other resources. Also parameterize the configuration using variables, use modules for reusability, and leverage various Terraform features to manage and orchestrate the infrastructure.</p> </li> </ol>"},{"location":"articles/configuration-management/#16-kubernetes","title":"1.6. Kubernetes","text":"<p>Kubernetes is an open-source container orchestration platform developed by Google that automates the deployment, scaling, and management of containerized applications. It provides a robust and scalable infrastructure for running and managing containers across multiple hosts, allowing organizations to build and manage highly available and resilient applications. It allows users to define and manage configurations using Kubernetes manifests (YAML files) and provides mechanisms for scaling, rolling updates, and automated deployments.</p> <p>NOTE Kubernetes has become the de facto standard for container orchestration and has widespread adoption in both cloud-native and on-premises environments. It enables organizations to achieve scalability, resilience, and flexibility in deploying and managing containerized applications.</p> <p>Features of Kubernetes:</p> <ol> <li> <p>Container Orchestration</p> <p>Kubernetes enables the management of containers at scale. It automates container deployment, scaling, and load balancing across multiple hosts, ensuring efficient utilization of resources and high availability of applications. Kubernetes abstracts away the underlying infrastructure details, allowing developers to focus on application logic rather than infrastructure management.</p> </li> <li> <p>Scalability and Elasticity</p> <p>Kubernetes allows applications to scale horizontally by adding or removing instances of containers based on demand. It supports auto-scaling based on CPU utilization or custom metrics. Kubernetes automatically manages the distribution of workloads and load balancing across the available resources.</p> </li> <li> <p>Service Discovery and Load Balancing</p> <p>Kubernetes provides service discovery and load balancing capabilities. It allows applications to discover and communicate with other services using DNS-based service names. Kubernetes also distributes incoming traffic to containers within a service through built-in load balancers.</p> </li> <li> <p>Self-Healing and Fault Tolerance</p> <p>Kubernetes automatically monitors the health of containers and services. If a container fails or becomes unresponsive, Kubernetes restarts it or replaces it with a new instance. It continuously monitors the application state and takes necessary actions to maintain desired application state and availability.</p> </li> <li> <p>Rolling Updates and Rollbacks</p> <p>Kubernetes supports rolling updates, enabling applications to be updated without any downtime. It gradually replaces older versions of containers with new versions, ensuring smooth transitions. If any issues occur during the update, Kubernetes supports rollbacks to the previous version of the application.</p> </li> <li> <p>Storage Orchestration</p> <p>Kubernetes provides storage orchestration capabilities, allowing applications to easily manage and mount persistent storage volumes. It supports different types of storage, such as local storage, network-attached storage (NAS), and cloud storage, and provides mechanisms for managing storage requirements for containers.</p> </li> <li> <p>Configuration and Secret Management</p> <p>Kubernetes enables the management of configuration and secret data for applications. It allows the external configuration to be decoupled from the container image and provides mechanisms to manage and update configurations without redeploying the entire application.</p> </li> <li> <p>Ecosystem and Extensibility</p> <p>Kubernetes has a rich ecosystem and a wide range of community-contributed extensions, plugins, and tools. It provides an extensible architecture, allowing integration with various cloud providers, logging and monitoring systems, and other DevOps tools. This extensibility makes Kubernetes adaptable to different use cases and allows for customization and integration with existing systems.</p> </li> </ol> <p>Example of Kubernetes:</p> <ol> <li> <p>Web Server</p> <ul> <li>A Kubernetes configuration (in YAML format) that deploys an Apache web server using a Deployment and a Service:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apache-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: apache\n  template:\n    metadata:\n      labels:\n        app: apache\n    spec:\n      containers:\n        - name: apache-container\n          image: httpd:latest\n          ports:\n            - containerPort: 80\n</code></pre> <p>In the example:</p> <ul> <li>The <code>Deployment</code> resource defines the desired state of the Apache web server deployment.<ul> <li><code>replicas: 3</code> specifies that we want three instances of the Apache web server.</li> <li><code>selector</code> defines the label selector to match the Pods controlled by this Deployment.</li> <li><code>template</code> defines the Pod template used to create the Pods.</li> <li><code>labels</code> specify the labels for the Pods created from this template.</li> <li><code>containers</code> define the container specifications.<ul> <li><code>name</code> sets the name of the container.</li> <li><code>image</code> specifies the Docker image to use (in this case, <code>httpd:latest</code>).</li> <li><code>ports</code> define the ports exposed by the container.</li> </ul> </li> </ul> </li> </ul> <p>To create the Apache deployment, apply the configuration using the <code>kubectl apply</code> command:</p> <pre><code>kubectl apply -f apache.yaml\n</code></pre> <p>This will create the Apache Deployment and start the specified number of Pods.</p> <ul> <li>Expose the deployed Apache Pods using a Service:</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: apache-service\nspec:\n  selector:\n    app: apache\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n</code></pre> <p>In the example:</p> <ul> <li>The <code>Service</code> resource defines a service to expose the Apache Pods.<ul> <li><code>selector</code> specifies the label selector to identify the Pods to be included in the service.</li> <li><code>ports</code> define the port configurations for the service.</li> <li><code>protocol</code> specifies the network protocol (TCP in this case).</li> <li><code>port</code> sets the port number on the service.</li> <li><code>targetPort</code> specifies the port number on the Pods.</li> <li><code>type: LoadBalancer</code> makes the service externally accessible using a cloud provider's load balancer (assuming the provider supports it).</li> </ul> </li> </ul> <p>To create the Apache service, apply the configuration using <code>kubectl apply</code>:</p> <pre><code>kubectl apply -f apache-service.yaml\n</code></pre> <p>This will create the service, which will expose the Apache Pods on port 80.</p> <p>Verify the deployment and service using <code>kubectl</code> commands like <code>kubectl get deployments</code>, <code>kubectl get pods</code>, and <code>kubectl get services</code>. The service will have an external IP assigned if the cloud provider supports load balancers.</p> <p>With this configuration, Kubernetes will ensure that the desired number of Apache Pods are running and provide load balancing for incoming traffic to the Apache service.</p> <p>NOTE Ensure that have a Kubernetes cluster setup and the <code>kubectl</code> command-line tool configured to communicate with the cluster before applying these configurations. Adjust the YAML files as needed to match the environment and requirements.</p> </li> </ol>"},{"location":"articles/configuration-management/#2-principles","title":"2. Principles","text":"<p>Configuration management provide a foundation for effective and efficient management of configurations. By following these principles, organizations can establish effective configuration management practices that ensure consistency, reliability, and stability across their systems and environments.</p> <ul> <li> <p>Centralized Configuration Repository</p> <p>Establish a centralized repository or database to store configuration information. This repository serves as a single source of truth and enables easy access and management of configurations. It should support version control and allow for tracking changes over time.</p> </li> <li> <p>Configuration as Code</p> <p>Treat configurations as code artifacts and manage them using version control systems. This approach ensures that configurations are versioned, reviewed, and auditable. Storing configurations as code also enables automation, repeatability, and consistency across environments.</p> </li> <li> <p>Standardization</p> <p>Establish standards and best practices for configurations to ensure consistency and reduce complexity. Define naming conventions, configuration formats, and guidelines for documenting configurations. Standardization simplifies maintenance, troubleshooting, and collaboration across teams.</p> </li> <li> <p>Change Management</p> <p>Implement a change management process to govern configuration changes. Changes should be requested, reviewed, approved, and tracked through a defined workflow. This process ensures that modifications are documented, assessed for impacts, and applied in a controlled manner to minimize risks and maintain stability.</p> </li> <li> <p>Version Control and Auditing</p> <p>Apply version control practices to configurations, allowing for easy rollback, comparison, and auditing of changes. Version control systems track modifications, provide historical context, and support collaboration among team members. Regular auditing of configurations ensures compliance, identifies unauthorized changes, and facilitates troubleshooting.</p> </li> <li> <p>Automation</p> <p>Leverage automation tools and processes to manage and enforce configurations. Automation reduces manual effort, human errors, and variability in configurations. It allows for consistent and efficient deployment, provisioning, and enforcement of configurations across systems and environments.</p> </li> <li> <p>Testing and Validation</p> <p>Establish testing and validation procedures to ensure the correctness and integrity of configurations. Test configurations in isolated environments or staging environments before deploying to production. Use validation mechanisms, such as syntax checks, linting, and automated tests, to identify errors or inconsistencies in configurations.</p> </li> <li> <p>Change Tracking and Reporting</p> <p>Maintain a comprehensive record of configuration changes, including the who, what, and when of each change. Establish reporting mechanisms to track and analyze configuration data, including compliance status, drift, and inventory. Reporting enables visibility into the state of configurations, aids in compliance audits, and supports decision-making processes.</p> </li> <li> <p>Continuous Improvement</p> <p>Embrace a culture of continuous improvement in configuration management. Regularly assess and refine processes, tools, and workflows. Incorporate feedback from users, stakeholders, and monitoring systems to identify areas for improvement. Adopt industry best practices and stay up to date with emerging technologies and standards in configuration management.</p> </li> </ul>"},{"location":"articles/configuration-management/#3-best-practice","title":"3. Best Practice","text":"<p>By following these best practices, organizations can establish a solid foundation for effective configuration management, enabling consistency, reliability, and scalability in managing their systems and environments.</p> <ul> <li> <p>Define a Configuration Management Strategy</p> <p>Establish a clear strategy and roadmap for configuration management within the organization. Define goals, objectives, and desired outcomes. Identify the scope and boundaries of configuration management efforts, including the systems, environments, and teams involved. This strategy will guide the approach and ensure consistency across the organization.</p> </li> <li> <p>Establish a Configuration Management Plan</p> <p>Create a formal plan that outlines the processes, tools, and responsibilities for configuration management. Define roles and responsibilities for configuration management activities, such as change management, version control, and release management. The plan should address the entire lifecycle of configurations, from creation and maintenance to retirement.</p> </li> <li> <p>Use Version Control Systems</p> <p>Store configurations in version control systems (VCS) to track changes, enable collaboration, and maintain an audit trail. Use branching and tagging strategies to manage different versions of configurations, allowing for easy rollback, comparison, and collaboration among team members. Regularly commit changes to the VCS and encourage a culture of code reviews and documentation.</p> </li> <li> <p>Document Configurations</p> <p>Document configurations comprehensively to ensure clarity, understanding, and ease of maintenance. Include details such as the purpose of configurations, dependencies, versions, and relationships with other systems or components. Keep configuration documentation up to date as changes are made, and make it easily accessible to relevant teams and stakeholders.</p> </li> <li> <p>Implement Change Management</p> <p>Establish a structured change management process to govern configuration changes. Implement a formal change request system that captures change details, approvals, and impact assessments. This process helps mitigate risks, ensure proper testing and validation, and maintain stability across environments. Regularly review and refine the change management process based on lessons learned.</p> </li> <li> <p>Automate Configuration Deployment</p> <p>Utilize automation tools and scripts to deploy configurations consistently and reliably. Automation reduces manual effort, eliminates human errors, and enables fast and repeatable deployment of configurations across systems and environments. Leverage tools like configuration management frameworks, infrastructure as code, and continuous integration/continuous deployment (CI/CD) pipelines to automate configuration deployment.</p> </li> <li> <p>Validate and Test Configurations</p> <p>Implement testing and validation procedures to verify the correctness and integrity of configurations. Use automated testing frameworks and tools to validate configurations against predefined criteria, including syntax, consistency, security, and compliance requirements. Regularly test configurations in staging or test environments before deploying them to production.</p> </li> <li> <p>Monitor and Track Configuration Changes</p> <p>Implement mechanisms to track and monitor configuration changes. Use configuration management tools or systems to capture and report on configuration drift, monitor compliance, and detect unauthorized changes. Leverage monitoring and alerting systems to proactively identify discrepancies or anomalies in configurations.</p> </li> <li> <p>Regularly Review and Audit Configurations</p> <p>Conduct regular reviews and audits of configurations to ensure compliance, security, and efficiency. Review configurations for accuracy, completeness, and adherence to established standards. Perform periodic audits to identify unauthorized changes, potential risks, and opportunities for improvement. Incorporate findings from reviews and audits into continuous improvement efforts.</p> </li> <li> <p>Foster Collaboration and Communication</p> <p>Encourage collaboration and communication among teams involved in configuration management. Establish channels for sharing knowledge, best practices, and lessons learned. Foster a culture of open communication, where team members can discuss and resolve configuration-related challenges and issues effectively.</p> </li> </ul>"},{"location":"articles/configuration-management/#4-terminology","title":"4. Terminology","text":"<p>These terms provide a foundational understanding of the key concepts and terminology used in configuration management. Familiarity with these terms is essential for effective communication and implementation of configuration management practices.</p> <ul> <li> <p>Configuration Item (CI)</p> <p>A configuration item is a component or asset that is managed and tracked as part of the configuration management process. It can include hardware, software, documentation, network devices, or any other tangible or intangible item that contributes to the functioning of a system.</p> </li> <li> <p>Configuration Management Database (CMDB)</p> <p>A CMDB is a centralized repository that stores information about configuration items and their relationships. It serves as a comprehensive source of configuration data, allowing for tracking, analysis, and reporting on configurations, their versions, and dependencies.</p> </li> <li> <p>Baseline</p> <p>A baseline is a reference point or snapshot of a specific configuration that serves as a standard for comparison and change control. It represents a stable and approved configuration state that can be used for configuration audits, compliance checks, and as a starting point for new deployments.</p> </li> <li> <p>Configuration Control</p> <p>Configuration control is the process of managing changes to configurations in a controlled manner. It involves assessing the impact of changes, obtaining appropriate approvals, implementing changes, and ensuring documentation and tracking of changes for future reference.</p> </li> <li> <p>Configuration Item Relationship</p> <p>Configuration item relationships define the associations and dependencies between different configuration items. Relationships can include hierarchical relationships (parent-child), dependencies, associations, or other types of connections that describe how different items relate to each other.</p> </li> <li> <p>Configuration Management Plan</p> <p>A configuration management plan is a formal document that outlines the strategy, processes, tools, and responsibilities for configuration management within an organization. It serves as a guide for managing configurations and provides a framework for consistent and controlled configuration management practices.</p> </li> <li> <p>Configuration Management System (CMS)</p> <p>A configuration management system refers to the collection of tools, processes, and resources used to manage and control configurations. It includes CMDBs, version control systems, change management systems, and other tools and processes that support configuration management activities.</p> </li> <li> <p>Change Management</p> <p>Change management is a process for managing and controlling changes to configurations. It involves requesting, reviewing, approving, and implementing changes in a controlled manner to minimize risks, ensure proper testing, and maintain stability.</p> </li> <li> <p>Configuration Drift</p> <p>Configuration drift refers to the unintended changes or inconsistencies that occur between a desired or baseline configuration and the actual state of a system or component. It can result from unauthorized changes, manual interventions, or deviations from established standards, and it is important to monitor and rectify configuration drift to maintain consistency and compliance.</p> </li> <li> <p>Configuration Item Versioning</p> <p>Configuration item versioning involves managing and tracking different versions of configuration items over time. It allows for comparing and reverting to previous versions, tracking changes, and maintaining an audit trail of modifications made to configurations.</p> </li> </ul>"},{"location":"articles/continuous-pipelines/","title":"Continuous Pipelines","text":"<p>Continuous Pipelines refer to the process of automating the software development workflow from code creation to deployment. This process involves multiple stages such as code integration, build, testing, and deployment to production. The pipeline runs continuously, ensuring that every change made to the code is automatically built, tested, and deployed, providing quick feedback to the development team and enabling faster delivery of new features to customers. By automating this process, organizations can improve efficiency, reduce errors, and increase the speed of delivering software updates.</p> <ul> <li>1. Category</li> <li>1.1. Continuous Verification</li> <li>1.2. Continuous Integration</li> <li>1.3. Continuous Delivery</li> <li>1.4. Continuous Deployment</li> <li>1.5. Continuous Build</li> <li>1.6. Continuous Testing</li> <li>1.7. Continuous Feedback</li> <li>1.8. Continuous Monitoring</li> <li>1.9. Continuous Optimization</li> <li>1.10. Continuous Compliance</li> <li>1.11. Continuous Security</li> <li>1.12. Continuous Documentation</li> <li>1.13. Continuous Training</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. References</li> </ul>"},{"location":"articles/continuous-pipelines/#1-category","title":"1. Category","text":""},{"location":"articles/continuous-pipelines/#11-continuous-verification","title":"1.1. Continuous Verification","text":"<p>Continuous Verification refers to the practice of continuously verifying the quality and functionality of software applications and systems. This can involve automating the process of testing and validating code changes, and can help organizations to catch and fix bugs and other issues before they are deployed to production. Continuous Verification helps to ensure that software is of high quality and reliable, and can improve the overall stability and user experience of applications.</p>"},{"location":"articles/continuous-pipelines/#12-continuous-integration","title":"1.2. Continuous Integration","text":"<p>Continuous Integration (CI) refers to the practice of automating the process of integrating code changes from multiple developers into a single version control repository. This process typically involves building and testing the code changes as soon as they are committed to the repository, and providing feedback to developers on the quality and stability of their changes. The goal of CI is to catch issues early in the development process and promote collaboration among team members.</p>"},{"location":"articles/continuous-pipelines/#13-continuous-delivery","title":"1.3. Continuous Delivery","text":"<p>Continuous Delivery (CD) is a set of practices and techniques that aim to automate the process of delivering code changes from the development environment to production. This process typically involves testing and verifying code changes at multiple stages, such as build, test, and deployment, before they are released to production. CD helps to ensure that code changes are deployed quickly, with minimal manual intervention, and with confidence in their quality and stability.</p>"},{"location":"articles/continuous-pipelines/#14-continuous-deployment","title":"1.4. Continuous Deployment","text":"<p>Continuous Continuous Deployment (CD) is an extension of Continuous Delivery that automates the deployment of code changes to production as soon as they pass all necessary tests and approvals. This practice is typically used in organizations that have high levels of automation and a strong focus on rapid and frequent delivery of code changes.</p>"},{"location":"articles/continuous-pipelines/#15-continuous-build","title":"1.5. Continuous Build","text":"<p>Continuous Build refers to the practice of automatically building and compiling software applications and systems, whenever changes are committed to the source code repository. This can involve automating the process of building, compiling, and packaging software, and can help organizations to ensure that software is always up-to-date and ready for deployment. Continuous Build helps to speed up the software development process, reduce errors and inconsistencies, and improve the overall efficiency of software development and deployment.</p>"},{"location":"articles/continuous-pipelines/#16-continuous-testing","title":"1.6. Continuous Testing","text":"<p>Continuous Testing refers to the practice of automating the process of testing code changes throughout the software development lifecycle. This process typically involves the use of test automation tools and scripts that run automatically as code changes are committed, built, and deployed. Continuous Testing helps to ensure that code changes are thoroughly tested and validated before they are released to production, reducing the risk of defects and improving the overall quality of the software.</p>"},{"location":"articles/continuous-pipelines/#17-continuous-feedback","title":"1.7. Continuous Feedback","text":"<p>Continuous Feedback refers to the practice of providing real-time feedback to developers on the quality and performance of their code changes. This can include feedback from automated testing tools, performance metrics, and user feedback, among other sources. The goal of Continuous Feedback is to provide developers with quick and actionable insights into the quality and stability of their code changes, and to promote continuous improvement in the software development process.</p>"},{"location":"articles/continuous-pipelines/#18-continuous-monitoring","title":"1.8. Continuous Monitoring","text":"<p>Continuous Monitoring refers to the practice of monitoring the performance and stability of software applications in production, and proactively addressing any issues that may arise. This process typically involves collecting and analyzing performance and log data, as well as monitoring system health and availability. Continuous Monitoring helps to ensure that software applications are performing optimally in production, and helps to minimize downtime and resolve issues quickly.</p>"},{"location":"articles/continuous-pipelines/#19-continuous-optimization","title":"1.9. Continuous Optimization","text":"<p>Continuous Optimization refers to the practice of continuously improving and refining the Continuous Pipeline to increase efficiency, reduce errors, and improve the speed of delivery. This can involve changes to the pipeline itself, as well as improvements to the underlying infrastructure and tools, and can involve a combination of technical and organizational changes. The goal of Continuous Optimization is to improve the overall effectiveness and efficiency of the software development process, and to support the delivery of high-quality software to customers.</p>"},{"location":"articles/continuous-pipelines/#110-continuous-compliance","title":"1.10. Continuous Compliance","text":"<p>Continuous Compliance refers to the practice of ensuring that software applications and systems comply with relevant regulations and standards throughout their lifecycle. This can involve automating the process of verifying compliance with data privacy, security, and other regulatory requirements, and can help organizations to reduce the risk of non-compliance and penalties.</p>"},{"location":"articles/continuous-pipelines/#111-continuous-security","title":"1.11. Continuous Security","text":"<p>Continuous Security refers to the practice of integrating security into the software development lifecycle, and continuously monitoring and verifying the security of applications and systems. This can involve automated security testing, security scans, and regular security audits, and helps organizations to identify and remediate security vulnerabilities before they can be exploited.</p>"},{"location":"articles/continuous-pipelines/#112-continuous-documentation","title":"1.12. Continuous Documentation","text":"<p>Continuous Documentation refers to the practice of maintaining up-to-date and accurate documentation of software applications and systems, and ensuring that documentation is automatically updated as code changes are committed and deployed. This helps to ensure that developers and other stakeholders have access to the most current and accurate information about the software, and can help to reduce errors and improve collaboration.</p>"},{"location":"articles/continuous-pipelines/#113-continuous-training","title":"1.13. Continuous Training","text":"<p>Continuous Training refers to the practice of providing ongoing training and development opportunities for developers, DevOps teams, and other stakeholders involved in the software development process. This can include training on new technologies, tools, and methodologies, as well as opportunities for hands-on experience and collaboration. Continuous Training helps organizations to build a skilled and knowledgeable workforce, and to continuously improve the software development process.</p>"},{"location":"articles/continuous-pipelines/#2-principle","title":"2. Principle","text":"<p>Principles provide a theoretical foundation for the design and implementation of effective Continuous Pipelines, to continuously improve the quality and efficiency of the software development and deployment efforts.</p> <ul> <li> <p>Automation</p> <p>Automation is a key principle of Continuous Pipelines, and involves automating as many steps in the software development and deployment process as possible. By automating tasks like building, testing, and deploying software, organizations can reduce the risk of errors, save time, and improve the overall efficiency of the software development process.</p> </li> <li> <p>Feedback</p> <p>Feedback is another key principle of Continuous Pipelines, and involves providing continuous feedback to developers and other stakeholders throughout the software development and deployment process. By providing rapid feedback on code changes, test results, and other metrics, organizations can help developers to identify and address issues quickly, and continuously improve the quality of the software.</p> </li> <li> <p>Collaboration</p> <p>Collaboration is also a key principle of Continuous Pipelines, and involves fostering a culture of collaboration and communication among developers, DevOps teams, and other stakeholders involved in the software development process. By working together and sharing knowledge and expertise, teams can improve the quality and efficiency of the software development process.</p> </li> <li> <p>Continuous Improvement</p> <p>Continuous Improvement is a key principle of Continuous Pipelines, and involves continuously analyzing and improving the software development and deployment process. By regularly reviewing metrics, identifying areas for improvement, and implementing changes, organizations can continuously improve the quality and efficiency of their software development efforts.</p> </li> <li> <p>Standardization</p> <p>Standardization is another key principle of Continuous Pipelines, and involves establishing and following standardized processes and procedures for software development and deployment. By standardizing processes and procedures, organizations can reduce the risk of errors, improve efficiency, and ensure that software is developed and deployed in a consistent and controlled manner.</p> </li> <li> <p>Flexibility</p> <p>Flexibility is also a key principle of Continuous Pipelines, and involves designing software development and deployment processes that are flexible and adaptable to changing requirements and conditions. By designing processes that can be easily modified and adapted, organizations can respond quickly to changes in the software development environment and continuously improve the quality and efficiency of their software development efforts.</p> </li> </ul>"},{"location":"articles/continuous-pipelines/#3-best-practice","title":"3. Best Practice","text":"<p>Best practices to build and maintain a successful Continuous Pipeline that enables to develop and deploy high-quality software applications and systems efficiently and effectively.</p> <ul> <li> <p>Maintain a culture of collaboration</p> <p>Continuous Pipelines rely on collaboration between developers, operations teams, and other stakeholders. Organizations should cultivate a culture of collaboration by establishing open channels of communication, encouraging cross-functional teamwork, and promoting knowledge sharing and learning.</p> </li> <li> <p>Automate everything</p> <p>Automation is a key driver of efficiency and reliability in Continuous Pipelines. Organizations should strive to automate as many aspects of the software development and deployment process as possible, including testing, building, and deployment, in order to reduce manual effort, increase consistency, and improve quality.</p> </li> <li> <p>Focus on testing</p> <p>Testing is a critical aspect of Continuous Pipelines. Organizations should adopt a testing strategy that emphasizes early and frequent testing, including unit testing, integration testing, and end-to-end testing, to ensure that software is thoroughly tested and validated throughout the development process.</p> </li> <li> <p>Maintain visibility and traceability</p> <p>Continuous Pipelines generate a large volume of data and information. Organizations should maintain visibility into the entire pipeline, including all changes and updates, and establish traceability between changes and the corresponding tests and deployments.</p> </li> <li> <p>Practice continuous improvement</p> <p>Continuous Pipelines are built on the principle of continuous improvement. Organizations should constantly assess and evaluate the pipeline, seeking opportunities for improvement in areas such as speed, quality, and efficiency, and implement changes to address identified issues and optimize the pipeline.</p> </li> <li> <p>Embrace a DevOps mindset</p> <p>Continuous Pipelines require a DevOps mindset, which emphasizes collaboration, automation, and continuous improvement. Organizations should encourage DevOps practices such as cross-functional teams, shared responsibility, and continuous learning, to create a culture of collaboration and innovation.</p> </li> <li> <p>Invest in tools and infrastructure</p> <p>Continuous Pipelines require robust tools and infrastructure to support automation, testing, and deployment. Organizations should invest in high-quality tools and infrastructure, and ensure that they are properly integrated and configured to support the entire pipeline.</p> </li> </ul>"},{"location":"articles/continuous-pipelines/#4-references","title":"4. References","text":"<ul> <li>GitLab CI/CD article.</li> <li>Atlassian CI article.</li> <li>Atlassian CD article.</li> <li>Microsoft CD vs CD article.</li> <li>GitHub CI/CD article.</li> </ul>"},{"location":"articles/crlf-vs-lf/","title":"CRLF vs LF","text":"<p>CRLF and LF are line ending characters that are used to represent the end of a line of text in text files.</p> <p>CRLF (Carriage Return and Line Feed) is a combination of the characters <code>carriage return</code> (ASCII 13, \\r) and <code>line feed</code> (ASCII 10, \\n), used to represent a line break in Windows based systems.</p> <p>LF (Line Feed) is the character <code>line feed</code> (ASCII 10, \\n), used to represent a line break in Unix and Unix-based systems, such as Linux and macOS.</p> <p>The use of line ending characters is important for text processing, as it helps to distinguish between the end of a line of text and the end of a file. The use of different line ending characters can lead to compatibility issues, especially when transferring files between different operating systems. This can be resolved by standardizing the line ending representation to a specific format, such as using only LF characters.</p> <ul> <li>1. Normalizing Line Endings</li> <li>2. References</li> </ul>"},{"location":"articles/crlf-vs-lf/#1-normalizing-line-endings","title":"1. Normalizing Line Endings","text":"<p>In a Git-based version control software (VCS), create a .gitattributes file in the root of the repo to normalize line endings. This configuration-based approach ensures that line endings remain consistent across the code base, regardless of which operating systems or local Git settings developers use, because this file takes priority.</p> <p>The following <code>.gitattributes</code> config normalizes line endings to <code>LF</code> for all text files checked into a repo while leaving local line endings untouched in the working tree:</p> <pre><code># Set the default behavior, in case don't have core.autocrlf set.\n* text=auto\n\n# Declare files that will always have LF line endings on checkout.\n*.sh text eol=lf\n\n# Declare files that will always have CRLF line endings on checkout.\n*.sln text eol=crlf\n*.bat text eol=crlf\n\n# Denote all files that are truly binary and should not be modified.\n*.png binary\n*.jpg binary\n</code></pre>"},{"location":"articles/crlf-vs-lf/#2-references","title":"2. References","text":"<ul> <li>Git end-of-line conversion article.</li> <li>Blog CRLF vs LF article.</li> <li>OWASP CRLF Injection article.</li> </ul>"},{"location":"articles/cross-compiler/","title":"Cross Compiler","text":"<p>A cross compiler is a compiler capable of creating executable code for a platform other than the one on which the compiler is running. For example, a compiler that runs on a PC but generates code that runs on Android smartphone is a cross compiler.</p> <ul> <li>1. Naming Convention</li> <li>2. Glossary</li> <li>3. References</li> </ul>"},{"location":"articles/cross-compiler/#1-naming-convention","title":"1. Naming Convention","text":"<p>The naming rule of the cross-compilation toolchain is: <code>[arch]-[vendor]-[os]-[abi]</code>.</p> <ul> <li> <p><code>arch</code></p> <p>Refers to target architecture: arm, mips, x86, i686.</p> </li> <li> <p><code>vendor</code></p> <p>Refers to toolchain supplier: apple.</p> </li> <li> <p><code>os</code></p> <p>Refers to the target operating system: darwin, linux, none (bare metal systems).</p> </li> <li> <p><code>eabi</code></p> <p>Refers to Embedded Application Binary Interface: eabi, gnueabi, gnueabihf.</p> </li> </ul> <p>Illustrations as follows:</p> <ul> <li>arm-none-eabi</li> </ul> <p>This tool chain targets for ARM architecture (including ARM Linux boot and kernel), has no vendor (generic), does not target an operating system and complies with the ARM EABI.</p> <p>Generally suitable for ARM7, Cortex-M and Cortex-R core chips.</p> <ul> <li>arm-none-linux-gnueabi, arm-linux-gnueabi</li> </ul> <p>This toolchain targets the ARM architecture, has no vendor (generic), creates binaries that run on the Linux operating system, and uses the GNU EABI. It is used to target ARM-based Linux systems.</p> <p>Used for Linux systems based on ARM architecture, and can be used to compile u-boot, Linux kernel, linux applications of ARM architecture, etc. arm-none-linux-gnueabi is based on GCC, uses the Glibc library, and is a compiler optimized. Generally, ARM9, ARM11, Cortex-A kernels and Linux operating system.</p> <ul> <li>i686-apple-darwin10-gcc-4.2.1</li> </ul> <p>This toolchain targets the Intel i686 architecture, the vendor is Apple, and the target OS is Darwin version 10 with the  of GCC version 4.2.1.</p> <ul> <li>arm-ostl-linux-gnueabi</li> </ul> <p>This toolchain targets the ARM architecture, the vendor is STMicroelectronics for OpenSTLinux in STM32MP, creates binaries that run on the Linux operating system, and uses the GNU EABI. It is used to target ARM-based Linux systems.</p> <ul> <li>arm-eabi</li> </ul> <p>Android  ARM compiler.</p> <ul> <li>arm-linux-gnueabi-gcc and arm-linux-gnueabihf-gcc</li> </ul> <p>The two cross-compilers are applicable to two different architectures of armel and armhf. The two architectures of armel and armhf adopt different strategies for floating-point operations (arms with fpu can support these two floating-point operations strategies).</p> <p>In fact, these two cross-compilers are just different default values \u200b\u200bof the gcc option -mfloat-abi. The gcc option -mfloat-abi has three values, soft, softfp, and hard (the latter two of which require the fpu floating point unit in the arm, soft and the latter two are compatible, but the two modes of softfp and hard are not compatible with each other ): soft: FPU is not used for floating point calculation, even if there is an FPU floating point unit, it is not used, but the software mode is used.</p> <p>softfp: The default value adopted by the armel architecture (corresponding compiler is arm-linux-gnueabi-gcc) is calculated by fpu, but the parameters are passed by ordinary registers, so that when interrupting, only ordinary registers need to be saved, and the interrupt load is small. But the parameters need to be converted to floating point and then calculated.</p> <p>hard: The default value adopted by the armhf architecture (corresponding compiler arm-linux-gnueabihf-gcc) is calculated by fpu, and the parameters are also passed by the floating-point register in the fpu, eliminating the need for conversion. The performance is the best, but the interrupt load high.</p>"},{"location":"articles/cross-compiler/#2-glossary","title":"2. Glossary","text":"<p>Definitions of terms.</p> <ul> <li> <p>ABI</p> <p>Application Binary Interface (ABI) for the ARM Architecture. In the computer, the application binary interface describes the low-level interface between the application (or other types) and the operating system or other applications.</p> </li> <li> <p>EABI</p> <p>Embedded Application Binary Interface (EABI). The embedded application binary interface specifies the file format, data type, register usage, stack organization optimization, and standard conventions of parameters in an embedded software. Developers using their own assembly language can also use EABI as an interface with the assembly language generated by a compatible compiler. The main difference with ABI is that ABI is used on the computer and EABI is used on the embedded platform (e.g. ARM, MIPS).</p> </li> <li> <p>amd64</p> <p>AMD64 is AMD\u2019s 64-bit extension of Intel\u2019s x86 architecture, and is also referred to as x86_64 (or x86-64).</p> </li> <li> <p>arm64</p> <p>ARM64 is the 64-bit extension of the ARM CPU architecture.</p> </li> <li> <p>x86_64</p> <p>x86_64 (or x86-64) refers to a 64-bit instruction set invented by AMD as an extension of Intel\u2019s x86 architecture. AMD calls its x86_64 architecture, AMD64, and Intel calls its implementation, Intel 64.</p> </li> </ul>"},{"location":"articles/cross-compiler/#3-references","title":"3. References","text":"<ul> <li>Actorsfit arm cross compiler article.</li> </ul>"},{"location":"articles/cryptography/","title":"Cryptography","text":"<p>Cryptography is the practice of securing communication from third-party interference, where third-party is anyone who is not authorized to access the communication. Cryptography uses mathematical algorithms to convert the original message, known as plaintext, into an unreadable format called ciphertext.</p> <p>The ciphertext can only be read by someone who possesses the key used to encrypt the message, and this key is kept secret by the authorized parties. Cryptography is used for various purposes, such as confidentiality, integrity, authentication, and non-repudiation.</p> <ul> <li>1. Category</li> <li>1.1. Symmetric Cryptography</li> <li>1.2. Asymmetric Cryptography</li> <li>1.3. Hash Functions<ul> <li>1.3.1. Password Hashing Algorithms</li> </ul> </li> <li>1.4. Steganography</li> <li>1.5. Cryptanalysis</li> <li>1.6. Cryptographic Protocols</li> <li>1.7. Homomorphic Encryption</li> <li>1.8. Zero-Knowledge Proofs</li> <li>1.9. Cryptographic Tokens</li> <li>1.10. Key Management</li> <li>1.11. Cryptographic Hardware</li> <li>1.12. Digital Signatures</li> <li>1.13. Key Exchange</li> <li>1.14. Public Key Infrastructure</li> <li>1.15. Random Number Generation</li> <li>1.16. Side-Channel Attacks</li> <li>1.17. Multi-Party Computation</li> <li>1.18. Obfuscation</li> <li>1.19. One-Time Pad</li> <li>1.20. Perfect Forward Secrecy</li> <li>1.21. Blockchain Technology</li> <li>1.22. Quantum Cryptography</li> <li>1.23. Post-Quantum Cryptography</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/cryptography/#1-category","title":"1. Category","text":"<p>Cryptography encompasses a wide range of techniques and protocols that are used to secure communication and data storage in various applications. Cryptography is a constantly evolving field, and new techniques and protocols are being developed to meet the ever-increasing demand for digital security and privacy.</p>"},{"location":"articles/cryptography/#11-symmetric-cryptography","title":"1.1. Symmetric Cryptography","text":"<p>Symmetric cryptography, also known as Secret Key Cryptography, uses the same key for both encryption and decryption. The key is kept secret between the sender and receiver, and the encryption process is fast and efficient.</p> <p>In symmetric cryptography, the plaintext is transformed into ciphertext using an algorithm that applies the secret key to the plaintext. The same secret key is then used to decrypt the ciphertext back into plaintext.</p> <p>The challenge is to securely share the key between the sender and receiver, which can be difficult to manage in large-scale systems.</p> <p>Common symmetric cryptography algorithms:</p> <ul> <li> <p>Data Encryption Standard (DES)</p> <p>An older block cipher that uses a 56-bit key.</p> <p>NOTE Considered insecure and should not be used.</p> </li> <li> <p>Advanced Encryption Standard (AES)</p> <p>A widely-used block cipher that supports key sizes of 128, 192, or 256 bits.</p> </li> <li> <p>Triple DES (3DES)</p> <p>A block cipher that applies DES three times to each block of plaintext, using two or three keys.</p> </li> <li> <p>Blowfish</p> <p>A block cipher that supports key sizes of up to 448 bits and is designed to be computationally efficient.</p> </li> </ul>"},{"location":"articles/cryptography/#12-asymmetric-cryptography","title":"1.2. Asymmetric Cryptography","text":"<p>Asymmetric cryptography, also known as public key cryptography, uses two different keys for encryption and decryption. The public key is used for encryption, and the private key is used for decryption. The private key is kept secret by the owner, while the public key can be shared publicly. This allows for secure communication between two parties without the need for a shared secret key.</p> <p>The disadvantage of asymmetric cryptography is its computational complexity, but is more secure, as the private key is never shared.</p> <p>Common asymmetric cryptography algorithms:</p> <ul> <li> <p>RSA (Rivest, Shamir, and Adleman)</p> <p>A widely-used algorithm that is based on the difficulty of factoring large prime numbers.</p> </li> <li> <p>Elliptic Curve Cryptography (ECC)</p> <p>A newer algorithm that is based on the mathematics of elliptic curves and is designed to be more efficient than RSA.</p> </li> <li> <p>Diffie-Hellman</p> <p>A key exchange algorithm that allows two parties to securely share a secret key over an insecure channel.</p> </li> </ul>"},{"location":"articles/cryptography/#13-hash-functions","title":"1.3. Hash Functions","text":"<p>Hash functions are used to generate a unique fixed-size output value, called a hash, from any input data of arbitrary size, which is usually a string of digits and letters. Hash functions are commonly used in cryptography for message authentication, digital signatures, password storage, and data integrity verification. Some popular hash functions include <code>Secure Hash Algorithm (SHA)</code> and <code>Message Digest (MD)</code>.</p> <p>Commonly used hash functions include MD5, SHA-1, SHA-256, and SHA-3. However, MD5 and SHA-1 are considered insecure and should not be used for cryptographic purposes. SHA-256 and SHA-3 are currently the most widely used hash functions.</p> <p>Family of hash functions:</p> <ul> <li> <p>MD</p> <p>An older family of hash functions developed by Ron Rivest. Includes several members, such as <code>MD2</code>, <code>MD4</code>, and <code>MD5</code>.</p> <p>NOTE Considered insecure and should not be used.</p> </li> <li> <p>SHA-1</p> <p>SHA-1 is a cryptographic hash function that produces a 160-bit hash value. It was designed by the National Security Agency (NSA) and published by the National Institute of Standards and Technology (NIST) in 1995. <code>SHA-1</code> is the only members of the SHA-1 family.</p> <p>NOTE Considered insecure and should not be used.</p> </li> <li> <p>SHA-2</p> <p>A family of hash functions developed by the National Security Agency (NSA). Includes several members, such as <code>SHA-224</code>, <code>SHA-256</code>, <code>SHA-384</code>, <code>SHA-512</code>, <code>SHA-512/224</code> and <code>SHA-512/256</code>. Widely used for digital signatures, message authentication, and password storage. Provides a high level of security and is considered to be resistant to attacks.</p> </li> <li> <p>SHA-3</p> <p>A family of hash functions developed as an alternative to SHA-2. Includes several members, such as <code>SHA3-224</code>, <code>SHA3-256</code>, <code>SHA3-384</code>, and <code>SHA3-512</code>. Designed to be more secure and resistant to attacks than SHA-2.</p> </li> <li> <p>BLAKE</p> <p>A family of hash functions designed by Jean-Philippe Aumasson and others. Includes several members, such as <code>BLAKE2b</code> and <code>BLAKE2s</code>. Known for their high performance and security.</p> </li> <li> <p>Keccak</p> <p>A family of hash functions developed by Guido Bertoni, Joan Daemen, and Gilles Van Assche. Includes several members, such as <code>Keccak-256</code> and <code>Keccak-512</code>. Chosen as the winner of the NIST hash function competition and is now the basis for SHA-3.</p> </li> <li> <p>Whirlpool</p> <p>A family of hash functions developed by Vincent Rijmen and Paulo Barreto. Includes <code>Whirlpool-0</code> and <code>Whirlpool-T</code>. Not widely used, but considered secure.</p> </li> <li> <p>Skein</p> <p>A family of hash functions developed by Bruce Schneier, Niels Ferguson, Stefan Lucks, Doug Whiting, Mihir Bellare, Tadayoshi Kohno, and Jon Callas. Includes several members, such as <code>Skein-256</code> and <code>Skein-512</code>. Known for their high performance and security.</p> </li> </ul> <p>Features of hash functions:</p> <ul> <li> <p>Deterministic</p> <p>Given the same input, a hash function always produces the same output.</p> </li> <li> <p>Fixed Output Size</p> <p>The output of a hash function is always of a fixed size, regardless of the input size.</p> </li> <li> <p>One-Way</p> <p>It should be computationally infeasible to determine the input message from the hash value.</p> </li> <li> <p>Collision Resistance</p> <p>It should be computationally infeasible to find two input messages that produce the same hash value.</p> </li> <li> <p>Uniformity</p> <p>A small change in the input message should produce a significant change in the hash value.</p> </li> <li> <p>Non-Reversibility</p> <p>It should be computationally infeasible to determine the input message from the hash value.</p> </li> </ul>"},{"location":"articles/cryptography/#131-password-hashing-algorithms","title":"1.3.1. Password Hashing Algorithms","text":"<p>Password hashing algorithms are a specific type of hash function that are designed to store and verify passwords securely. Passwords are often the first line of defense in protecting user accounts and sensitive information, so it is important to store them securely.</p> <p>Password hashing algorithms are designed to be slow and computationally intensive, making them more difficult to crack through brute-force attacks. It is important to choose a strong password hash function and to use a unique salt for each user's password to ensure maximum security. The purpose of password hashing is to protect user passwords in case a database containing the password hashes is compromised.</p> <p>Common password hashing algorithms:</p> <ul> <li> <p>PBKDF2</p> <p>PBKDF2 (Password-Based Key Derivation Function 2) is a key derivation function that is used to securely hash passwords. It is based on a hash function, such as SHA-1 or SHA-256, and uses a salt value and an iteration count to produce a derived key from a password. While PBKDF2 is considered secure, it is not as memory-hard as newer algorithms like Bcrypt, Scrypt, and Argon2.</p> </li> <li> <p>Bcrypt</p> <p>Bcrypt is a password hashing algorithm that is designed to be slow and computationally expensive. Bcrypt is resistant to brute-force attacks and dictionary attacks, and is considered a secure choice for password hashing.</p> </li> <li> <p>Scrypt</p> <p>Scrypt is password hashing algorithm that is designed to be memory-hard. It uses a large amount of memory to make it more difficult to implement parallel attacks, and is resistant to attacks using ASICs or GPUs. Scrypt is considered more secure than bcrypt, but is also slower and more memory-intensive.</p> </li> <li> <p>Argon2</p> <p>Argon2 is a password hashing algorithm that was selected as the winner of the Password Hashing Competition in 2015. It is designed to be both memory-hard and CPU-hard, and is considered to be the most secure password hashing algorithm currently available.</p> </li> </ul> <p>Features of password hashing algorithms:</p> <ul> <li> <p>Salt</p> <p>Password hashing algorithms use a salt, which is a random value added to the password before hashing. This helps prevent attackers from using precomputed hash values, such as rainbow tables, to quickly crack passwords.</p> </li> <li> <p>Slow</p> <p>Password hashing algorithms are intentionally slow to make brute-force attacks more difficult. This slows down attackers who try to guess passwords by hashing them repeatedly until they find a match.</p> </li> <li> <p>Strong</p> <p>Blowfish, SHA-256 or SHA-3 are commonly used for password storage, since MD5 and SHA-1 are considered insecure.</p> </li> <li> <p>Non-Reversible</p> <p>Password hashing algorithms are non-reversible, meaning that it is computationally infeasible to determine the password from the hash value. This helps ensure that even if an attacker gains access to the password hash values, they cannot easily determine the passwords themselves.</p> </li> <li> <p>Unique</p> <p>Password hashing algorithms generate unique hash values for each password. This helps ensure that even if two users have the same password, their hash values will be different, making it more difficult for an attacker to attack multiple accounts at once.</p> </li> </ul>"},{"location":"articles/cryptography/#14-steganography","title":"1.4. Steganography","text":"<p>Steganography is the practice of hiding a message within another message or image. Unlike cryptography, which only secures the message, steganography hides the message from the observer. The hidden message is often referred to as the payload. Steganography is often used in conjunction with cryptography to provide additional security.</p>"},{"location":"articles/cryptography/#15-cryptanalysis","title":"1.5. Cryptanalysis","text":"<p>Cryptanalysis is the study of methods to break cryptographic systems. Cryptanalysts use various techniques such as mathematical analysis, brute force attacks, and side-channel attacks to break cryptographic algorithms. Cryptanalysis is an essential aspect of cryptography, as it helps to identify weaknesses in cryptographic systems and improve their security.</p>"},{"location":"articles/cryptography/#16-cryptographic-protocols","title":"1.6. Cryptographic Protocols","text":"<p>Cryptographic protocols are sets of rules and procedures that govern the secure exchange of information between parties. Cryptographic protocols are used in a wide range of applications, including secure communication, authentication, key exchange, and digital signatures.</p> <p>The design and implementation of these protocols can be complex, and there have been instances where vulnerabilities in protocols have been discovered and exploited by attackers. It is important to use trusted and well-vetted protocols, and to regularly update and patch systems to address any known vulnerabilities.</p> <p>Common cryptographic protocols:</p> <ul> <li> <p>Secure Sockets Layer (SSL)/Transport Layer Security (TLS)</p> <p>These protocols are used to secure communication between clients and servers. They provide encryption and authentication to ensure that the information being transmitted is protected from eavesdropping and tampering.</p> </li> <li> <p>Internet Protocol Security (IPsec)</p> <p>This protocol is used to secure communication between network devices, such as routers and switches. It provides authentication and encryption to ensure that the information being transmitted is protected from attacks such as spoofing and man-in-the-middle attacks.</p> </li> <li> <p>Kerberos</p> <p>This protocol is used for authentication in a network environment. It provides a secure way for users to authenticate to network services and systems without transmitting passwords in plain text.</p> </li> <li> <p>Pretty Good Privacy (PGP)</p> <p>This protocol is used for secure email communication. It provides encryption and digital signatures to ensure that emails are protected from eavesdropping and tampering.</p> </li> <li> <p>Secure Multipurpose Internet Mail Extensions (S/MIME)</p> <p>This protocol is also used for secure email communication. It provides encryption and digital signatures to ensure that emails are protected from eavesdropping and tampering, and also provides a way to verify the identity of the sender.</p> </li> </ul>"},{"location":"articles/cryptography/#17-homomorphic-encryption","title":"1.7. Homomorphic Encryption","text":"<p>Homomorphic encryption is a type of encryption that allows computations to be performed on ciphertexts without decrypting them. This means that data can be processed and analyzed while still being encrypted, preserving its confidentiality. Homomorphic encryption has the potential to transform the way data is processed and analyzed, as it allows sensitive data to be analyzed without the need to expose it.</p>"},{"location":"articles/cryptography/#18-zero-knowledge-proofs","title":"1.8. Zero-Knowledge Proofs","text":"<p>Zero-knowledge proofs are a type of protocol that allows one party to prove to another that they know a particular secret without revealing any information about that secret. Zero-knowledge proofs are used in various applications, such as authentication, identification, and access control.</p>"},{"location":"articles/cryptography/#19-cryptographic-tokens","title":"1.9. Cryptographic Tokens","text":"<p>Cryptographic tokens are physical devices that store cryptographic keys and are used for authentication and access control. Cryptographic tokens are often used in two-factor authentication systems and are considered to be more secure than traditional password-based systems.</p>"},{"location":"articles/cryptography/#110-key-management","title":"1.10. Key Management","text":"<p>Key management is the process of generating, storing, and distributing cryptographic keys used in cryptographic systems. Key management is essential for maintaining the security of cryptographic systems, as a compromised key can compromise the entire system.</p> <p>Categories of key management:</p> <ul> <li> <p>Key Generation</p> <p>This involves creating cryptographic keys using a secure random number generator. The keys should be of sufficient length and complexity to resist attacks.</p> </li> <li> <p>Key Distribution</p> <p>This involves securely transferring cryptographic keys from the key generator to the users who need them. This may involve using secure channels or protocols, such as Transport Layer Security (TLS), to ensure the confidentiality and integrity of the keys during transit.</p> </li> <li> <p>Key Storage</p> <p>This involves storing cryptographic keys securely to prevent unauthorized access. Keys should be stored in a tamper-proof hardware security module (HSM) or a secure software key store. The keys should be protected with strong access controls, such as multi-factor authentication.</p> </li> <li> <p>Key Usage</p> <p>This involves using cryptographic keys securely in cryptographic operations, such as encryption, decryption, and digital signatures. The keys should be used only for their intended purpose and should be protected from misuse or unauthorized access.</p> </li> <li> <p>Key Revocation</p> <p>This involves revoking cryptographic keys that are no longer needed or that may have been compromised. Keys should be revoked as soon as possible to prevent their use in future attacks. Revocation should be done securely to ensure that the revoked keys cannot be used again.</p> </li> <li> <p>Key Recovery</p> <p>This involves recovering cryptographic keys in case they are lost or damaged. This may involve using backup keys or key recovery agents to recreate the lost keys.</p> </li> </ul>"},{"location":"articles/cryptography/#111-cryptographic-hardware","title":"1.11. Cryptographic Hardware","text":"<p>Cryptographic hardware refers to specialized hardware devices designed to perform cryptographic operations. Cryptographic hardware is often used in high-security applications where software-based solutions may not provide adequate security.</p>"},{"location":"articles/cryptography/#112-digital-signatures","title":"1.12. Digital Signatures","text":"<p>Digital signatures are used to provide authentication, integrity, and non-repudiation to electronic documents and messages. A digital signature is generated using a private key and can be verified using the corresponding public key. Digital signatures are widely used in e-commerce, online contracts, and electronic voting systems.</p>"},{"location":"articles/cryptography/#113-key-exchange","title":"1.13. Key Exchange","text":"<p>Key exchange is the process of securely sharing cryptographic keys between two or more parties. Key exchange protocols ensure that the keys are shared only between the intended parties and are not intercepted by attackers. Key exchange protocols are an essential component of many cryptographic systems, such as SSL/TLS and IPSec.</p> <p>Protocols for key exchange:</p> <ul> <li> <p>Diffie-Hellman Key Exchange</p> <p>This is a popular key exchange protocol that allows two parties to establish a shared secret key over an insecure communication channel.</p> </li> <li> <p>Elliptic Curve Diffie-Hellman (ECDH)</p> <p>This is a variant of the Diffie-Hellman Key Exchange that uses elliptic curves to generate the shared secret key.</p> </li> <li> <p>RSA Key Exchange</p> <p>This is a key exchange protocol based on the RSA encryption algorithm.</p> </li> </ul>"},{"location":"articles/cryptography/#114-public-key-infrastructure","title":"1.14. Public Key Infrastructure","text":"<p>Public Key Infrastructure (PKI) is a set of technologies, protocols, and services used to manage digital certificates and public-private key pairs. PKI is used to enable secure communication and authentication over a network or the internet.</p> <p>PKI functions:</p> <ul> <li> <p>Certificate authority (CA)</p> <p>A trusted third-party organization that issues digital certificates to verify the identity of users, devices, or services. The CA uses its private key to sign the digital certificate, which contains the public key of the certificate holder.</p> </li> <li> <p>Registration authority (RA)</p> <p>An entity that verifies the identity of the certificate holder before issuing a digital certificate on behalf of the CA.</p> </li> <li> <p>Certificate management</p> <p>The process of issuing, renewing, revoking, and managing digital certificates.</p> </li> <li> <p>Certificate revocation</p> <p>The process of invalidating a digital certificate before its expiration date, usually due to a compromise of the private key or other security concerns.</p> </li> </ul>"},{"location":"articles/cryptography/#115-random-number-generation","title":"1.15. Random Number Generation","text":"<p>Random number generation is a crucial component of cryptography, used in various applications such as key generation, encryption, and digital signature schemes. A random number generator must be unpredictable and produce a statistically random sequence of numbers. A compromised or faulty random number generator can lead to vulnerabilities in cryptographic systems that can be exploited by attackers.</p> <p>Types of random number generators:</p> <ul> <li> <p>Pseudo-Random Number Generators (PRNGs)</p> <p>These are algorithms that generate a sequence of numbers that appear to be random but are actually generated using a deterministic process. PRNGs use a <code>seed</code> value as the starting point to generate the sequence of numbers. While PRNGs are not truly random, they are used widely in various applications, such as simulations and games, where the sequence of numbers generated only needs to be unpredictable and not truly random.</p> </li> <li> <p>True Random Number Generators (TRNGs)</p> <p>These generate a sequence of numbers that are truly random and unpredictable. TRNGs use physical sources of randomness, such as atmospheric noise, thermal noise, or radioactive decay, to generate random numbers. TRNGs are typically slower and more complex than PRNGs, but they provide a higher level of security.</p> </li> </ul>"},{"location":"articles/cryptography/#116-side-channel-attacks","title":"1.16. Side-Channel Attacks","text":"<p>Side-channel attacks are a type of cryptanalytic attack that exploits weaknesses in the physical implementation of cryptographic systems, such as power consumption, electromagnetic radiation, or sound. Side-channel attacks can be used to extract cryptographic keys or other sensitive information from a system. Side-channel attacks are often used in combination with other cryptanalytic techniques to break cryptographic systems.</p>"},{"location":"articles/cryptography/#117-multi-party-computation","title":"1.17. Multi-Party Computation","text":"<p>Multi-party computation is a type of secure computation that allows multiple parties to jointly compute a function on their private inputs without revealing their inputs to each other. Multi-party computation is used in various applications, such as secure auctions, collaborative data analysis, and private data sharing.</p>"},{"location":"articles/cryptography/#118-obfuscation","title":"1.18. Obfuscation","text":"<p>Obfuscation is a technique used to hide the meaning of code or data without affecting its functionality. Obfuscation is often used in software protection to make it difficult for attackers to reverse engineer code or steal intellectual property.</p>"},{"location":"articles/cryptography/#119-one-time-pad","title":"1.19. One-Time Pad","text":"<p>The one-time pad is a cryptographic technique that uses a random key of the same length as the message to encrypt and decrypt the message. The one-time pad is theoretically unbreakable if the key is kept secret and used only once, but it is impractical for most applications due to the large size of the key.</p>"},{"location":"articles/cryptography/#120-perfect-forward-secrecy","title":"1.20. Perfect Forward Secrecy","text":"<p>Perfect forward secrecy is a property of cryptographic systems that ensures that past communication cannot be compromised even if the secret key is compromised in the future. Perfect forward secrecy is achieved by generating a new key for each communication session, ensuring that compromise of one key does not compromise past or future communications.</p>"},{"location":"articles/cryptography/#121-blockchain-technology","title":"1.21. Blockchain Technology","text":"<p>Blockchain technology is a distributed ledger technology that uses cryptography to secure transactions and data. Blockchain technology is used in various applications, such as cryptocurrencies, supply chain management, and digital identity management. Blockchain technology relies heavily on cryptographic techniques, such as digital signatures, hash functions, and symmetric and asymmetric encryption.</p>"},{"location":"articles/cryptography/#122-quantum-cryptography","title":"1.22. Quantum Cryptography","text":"<p>Quantum cryptography uses principles of quantum mechanics to ensure secure communication between two parties. It is based on the fact that any attempt to measure a quantum system disturbs the system, and this disturbance can be detected by the communicating parties. Quantum cryptography is still in the experimental stage and is not yet widely used.</p>"},{"location":"articles/cryptography/#123-post-quantum-cryptography","title":"1.23. Post-Quantum Cryptography","text":"<p>Post-quantum cryptography is a type of cryptography that is designed to be resistant against attacks from quantum computers. Quantum computers have the potential to break many of the current cryptographic algorithms, making post-quantum cryptography an area of active research. Some popular post-quantum cryptography algorithms include lattice-based cryptography and code-based cryptography.</p>"},{"location":"articles/cryptography/#2-principle","title":"2. Principle","text":"<p>Cryptography is based on several fundamental principles that form the basis of modern cryptographic systems.  Principles form the foundation of modern cryptographic systems and are essential for ensuring the security and privacy of our digital lives.</p> <ul> <li> <p>Confidentiality</p> <p>Confidentiality is the principle of keeping data or information hidden from unauthorized access. Confidentiality is achieved through encryption, which scrambles the original data so that it can only be read by those with the proper decryption key.</p> </li> <li> <p>Integrity</p> <p>Integrity is the principle of ensuring that data or information is not tampered with or altered in any way during transmission or storage. Integrity is achieved through message authentication, which uses cryptographic hash functions to ensure that the message has not been altered.</p> </li> <li> <p>Authenticity</p> <p>Authenticity is the principle of ensuring that the identity of the sender or recipient of a message is verified. Authenticity is achieved through digital signatures, which use public key cryptography to sign a message and verify the identity of the sender.</p> </li> <li> <p>Non-Repudiation</p> <p>Non-repudiation is the principle of ensuring that the sender of a message cannot deny having sent it. Non-repudiation is achieved through digital signatures, which provide evidence of the origin of a message and the identity of the sender.</p> </li> <li> <p>Key Management</p> <p>Key management is the process of generating, storing, and distributing cryptographic keys used in cryptographic systems. Key management is essential for maintaining the security of cryptographic systems, as a compromised key can compromise the entire system.</p> </li> <li> <p>Complexity</p> <p>Complexity is the principle of making cryptographic algorithms difficult to break. Cryptographic algorithms should be designed in such a way that they require significant computing power and time to break, even with the most advanced technology.</p> </li> <li> <p>Availability</p> <p>Availability is the principle of ensuring that the cryptographic system is available and accessible to authorized users when needed. Availability is achieved through system design, redundancy, and backup procedures.</p> </li> <li> <p>Kerckhoffs' Principle</p> <p>Kerckhoffs' Principle is a fundamental principle of modern cryptography that states that a cryptographic system should be secure even if the attacker knows everything about the system, except for the secret key. This principle emphasizes the importance of strong and secure key management.</p> </li> </ul>"},{"location":"articles/cryptography/#3-best-practice","title":"3. Best Practice","text":"<p>Best practices ensure the security of cryptographic systems and protect against potential threats and attacks.</p> <ul> <li> <p>Use Strong and Up-to-Date Algorithms</p> <p>Use cryptographic algorithms that are strong and up-to-date. Avoid using deprecated algorithms that are known to be vulnerable to attacks.</p> </li> <li> <p>Keep Cryptographic Systems Up-to-Date</p> <p>Keep cryptographic systems up-to-date with the latest software updates, patches, and security fixes. This can help protect against known vulnerabilities and ensure that the system is secure against new and emerging threats.</p> </li> <li> <p>Use Strong Keys</p> <p>Use strong and complex keys for encryption and decryption. The keys should be long enough to make brute-force attacks unfeasible.</p> </li> <li> <p>Use Key Management Best Practices</p> <p>Implement key management best practices, including key generation, storage, distribution, and revocation. Keys should be generated securely and distributed only to authorized personnel.</p> </li> <li> <p>Implement Access Control</p> <p>Implement access control mechanisms to restrict access to cryptographic systems to authorized personnel only. Access should be granted based on the principle of least privilege.</p> </li> <li> <p>Use Secure Channels for Key Exchange</p> <p>Use secure channels for key exchange to prevent interception and man-in-the-middle attacks. Secure channels can include public key infrastructure, secure socket layer (SSL), and virtual private networks (VPN).</p> </li> <li> <p>Implement End-to-End Encryption</p> <p>Implement end-to-end encryption to ensure that data is encrypted throughout the communication process, from the sender to the receiver. End-to-end encryption can prevent interception and man-in-the-middle attacks.</p> </li> <li> <p>Use Digital Signatures for Authentication</p> <p>Use digital signatures to authenticate digital documents and messages. Digital signatures provide evidence of the identity of the signer and ensure that the document has not been altered.</p> </li> <li> <p>Protect Against Side-Channel Attacks</p> <p>Protect against side-channel attacks, which exploit physical characteristics of the system to obtain information about the keys used in the system. Side-channel attacks can include power analysis, electromagnetic analysis, and acoustic analysis.</p> </li> <li> <p>Perform Regular Security Audits</p> <p>Perform regular security audits to identify potential vulnerabilities and weaknesses in cryptographic systems. Audits can include penetration testing, vulnerability scanning, and code review.</p> </li> <li> <p>Document and Enforce Security Policies</p> <p>Document and enforce security policies that specify the roles and responsibilities of personnel with access to cryptographic systems. Security policies should include procedures for key management, access control, and incident response.</p> </li> <li> <p>Use Random Number Generators</p> <p>Use high-quality random number generators to generate cryptographic keys and other random values. Pseudo-random number generators can be vulnerable to predictability attacks if the underlying algorithm is weak or if the seed is predictable.</p> </li> <li> <p>Protect Against Key Compromise</p> <p>Protect against key compromise by storing keys securely and minimizing the number of copies of keys in circulation. Keys should be encrypted and stored on secure hardware, such as hardware security modules (HSMs).</p> </li> <li> <p>Use Strong Passwords</p> <p>Use strong passwords to protect cryptographic systems and prevent unauthorized access. Passwords should be long, complex, and unique to each user.</p> </li> <li> <p>Monitor System Activity</p> <p>Monitor system activity to detect potential security breaches and anomalies. System logs can be used to track user activity, detect unauthorized access attempts, and identify potential vulnerabilities.</p> </li> <li> <p>Plan for Incident Response</p> <p>Plan for incident response by developing procedures for detecting, investigating, and responding to security incidents. Incident response plans should include procedures for notifying affected parties, preserving evidence, and restoring systems to a secure state.</p> </li> <li> <p>Perf Perform risk assessments to identify potential threats and vulnerabilities to cryptographic systems. Risk assessments can help identify areas of weakness and guide the development of security policies and procedures.</p> </li> <li> <p>Use Multiple Layers of Protection</p> <p>Use multiple layers of protection, including encryption, access control, and intrusion detection systems. By using multiple layers of protection, you can reduce the likelihood of a successful attack and mitigate the impact of a security breach.</p> </li> <li> <p>Follow Industry Standards and Best Practices</p> <p>Follow industry standards and best practices for cryptographic systems. Standards bodies such as the National Institute of Standards and Technology (NIST) provide guidelines and recommendations for cryptographic algorithms and key management.</p> </li> <li> <p>Limit Exposure of Sensitive Information</p> <p>Limit the exposure of sensitive information by implementing data classification and access control policies. Sensitive information should be encrypted and access should be restricted to authorized personnel only.</p> </li> <li> <p>Test and Verify</p> <p>Test and verify the security of cryptographic systems through penetration testing, vulnerability scanning, and code review. Testing can help identify potential vulnerabilities and weaknesses in the system and guide the development of security policies and procedures.</p> </li> <li> <p>Plan for Disaster Recovery</p> <p>Plan for disaster recovery by implementing backup and recovery procedures for cryptographic systems. Backup copies of cryptographic keys and other sensitive data should be stored in secure locations and regularly tested to ensure that they can be restored in the event of a disaster.</p> </li> <li> <p>Train Personnel</p> <p>Train personnel on the proper use of cryptographic systems, including key management, access control, and incident response procedures. Personnel should also be trained on common threats and vulnerabilities, and how to detect and respond to security incidents.</p> </li> <li> <p>Stay Informed</p> <p>Stay informed about new and emerging threats to cryptographic systems and keep up-to-date with the latest trends and developments in cryptography. This can help to stay ahead of potential threats and adapt security policies and procedures accordingly.</p> </li> </ul>"},{"location":"articles/cryptography/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Cryptography</p> <p>The practice of secure communication in the presence of third parties.</p> </li> <li> <p>Encryption</p> <p>The process of converting plaintext into ciphertext to make it unreadable to unauthorized parties.</p> </li> <li> <p>Decryption</p> <p>The process of converting ciphertext back into plaintext using a decryption key.</p> </li> <li> <p>Cryptosystem</p> <p>A system used for encryption and decryption of messages.</p> </li> <li> <p>Key</p> <p>A secret value used to encrypt or decrypt a message.</p> </li> <li> <p>Symmetric Key Cryptography</p> <p>A type of cryptography where the same key is used for encryption and decryption.</p> </li> <li> <p>Asymmetric Key Cryptography</p> <p>A type of cryptography where different keys are used for encryption and decryption.</p> </li> <li> <p>Public Key Cryptography</p> <p>A type of asymmetric cryptography where a public key is used for encryption and a private key is used for decryption.</p> </li> <li> <p>Private Key Cryptography</p> <p>A type of asymmetric cryptography where a private key is used for encryption and a public key is used for decryption.</p> </li> <li> <p>Hash Function</p> <p>A mathematical function that converts an input into a fixed-size output called a hash value.</p> </li> <li> <p>Digital Signature</p> <p>A cryptographic technique used to ensure the authenticity, integrity, and non-repudiation of digital messages or documents.</p> </li> <li> <p>Certificate</p> <p>A digital document that contains information about the identity of a person or organization and their public key.</p> </li> <li> <p>SSL/TLS</p> <p>Secure Sockets Layer/Transport Layer Security, a protocol used for secure communication over the internet.</p> </li> <li> <p>Key Exchange</p> <p>The process of securely exchanging keys between two parties over an insecure channel.</p> </li> <li> <p>Brute Force Attack</p> <p>A type of attack where an attacker tries all possible combinations of keys until the correct key is found.</p> </li> <li> <p>Man-in-the-Middle Attack</p> <p>An attack where an attacker intercepts communication between two parties and can eavesdrop, modify, or manipulate the messages.</p> </li> <li> <p>Side-Channel Attack</p> <p>An attack where an attacker exploits weaknesses in the physical implementation of a cryptosystem, such as measuring power consumption or electromagnetic radiation.</p> </li> <li> <p>Digital Certificate</p> <p>A digital document that contains information about the identity of a person, organization, or device, and their public key. Digital certificates are used for authentication and secure communication over the internet.</p> </li> <li> <p>Cryptanalysis</p> <p>The study of cryptographic systems with the goal of breaking them or finding weaknesses that can be exploited.</p> </li> <li> <p>One-time Pad</p> <p>A type of encryption where a random key is used only once to encrypt a message. The one-time pad is considered unbreakable if used correctly.</p> </li> <li> <p>Key Management</p> <p>The process of generating, storing, distributing, and revoking cryptographic keys.</p> </li> <li> <p>Message Authentication Code (MAC)</p> <p>A short piece of information that is used to verify the authenticity and integrity of a message.</p> </li> <li> <p>Nonce</p> <p>A random number used only once in a cryptographic protocol to prevent replay attacks.</p> </li> <li> <p>Salting</p> <p>The process of adding a random value to data before hashing it, to prevent attacks such as rainbow table attacks.</p> </li> <li> <p>Rainbow Tables</p> <p>A type of precomputed hash table used in cryptography to crack password hashes.</p> </li> </ul>"},{"location":"articles/data-management/","title":"Data Management","text":"<p>Data Management plans help to ensure that your data are well-organized, managed, and prepared for preservation into the future. Use a data management plan to document the planned research effort, the expected outputs, and the plan for documenting and archiving data.</p> <ul> <li>1. Folder Structure</li> <li>1.1. Project Naming Convention</li> <li>2. File Structure</li> <li>2.1. File Naming Convention</li> <li>3. References</li> </ul>"},{"location":"articles/data-management/#1-folder-structure","title":"1. Folder Structure","text":"<ul> <li>00_Governance <p>Includes scoping documents, static documents for regular reference but minimal change.</p> </li> <li>00_Contract     &gt; Includes Customer Contracts, Non Disclosure Agreements (NDA) and Suplier Contracts.</li> <li>01_Request     &gt; Includes Market Reports.</li> <li>02_Order     &gt; Includes Executive Summary and Project Approval.</li> <li>01_Management <p>Includes planning and financial documents, living documents linked to regular reporting requirements and shared with internal and external stakeholders according to agreed mechanisms.</p> </li> <li>00_Schedule     &gt; Includes Gantt Charts.</li> <li>01_Patent     &gt; Includes Customer Patent, Product Patent or Technology Patent.</li> <li>02_Finance     &gt; Includes Cost calculation or Budget.</li> <li>99_Sharing     &gt; Contains files that are shared with external parties. Here a state is frozen so that it is no longer processed after sharing.</li> <li>02_Requirement <p>Contains all the requirements of the project throughout its life cycle.</p> </li> <li>HW</li> <li>SW</li> <li>03_Development <p>Includes technical documents related to deliverables, day-to-day project activities, documents generally used only by internal project teams to create a result.</p> </li> <li>00_Asset     &gt; Contains living documents with regular changes.</li> <li>01_Research     &gt; Contains static documents like data sheets and research papers.</li> <li>04_Communication <p>Includes internal and external information exchange.</p> </li> <li>Meetings</li> <li>Reports</li> <li>E-Mails</li> <li>05_Result <p>Includes generated output of the project. This includes lessons learned and documents that are required for manufacturing / mass production.</p> </li> </ul>"},{"location":"articles/data-management/#11-project-naming-convention","title":"1.1. Project Naming Convention","text":"<p>The naming rule is: <code>[YYYY]_[customer]_[title]_[sequence]</code>.</p> <ul> <li> <p><code>YYYY</code></p> <p>Refers to the year of project launch: 2022.</p> </li> <li> <p><code>customer</code></p> <p>Refers to the name of the customer or company: Apple.</p> </li> <li> <p><code>title</code></p> <p>Refers to the title of the project: iPhone-13.</p> </li> <li> <p><code>sequence</code></p> <p>Refers to the sequential number, default 1: 1.</p> </li> </ul> <p>Illustrations as follows:</p> <ul> <li>2022_Apple_iPhone-13_1</li> </ul>"},{"location":"articles/data-management/#2-file-structure","title":"2. File Structure","text":"<p>A successful file structure organizes your data and code with the goal of repeatability, making it easier for you and your collaborators to revisit, revise and develop your project. File structures are not fixed entities, but rather build a framework that communicates the function and purpose of elements within a project by separating concerns into a hierarchy of folders and using consistent, chronological, and descriptive names.</p>"},{"location":"articles/data-management/#21-file-naming-convention","title":"2.1. File Naming Convention","text":"<p>The naming rule is: <code>[YYYYMMDD]_[type]_[scope]_[author]_[title]_[subtitle]_[version]</code>.</p> <ul> <li> <p><code>YYYYMMDD</code></p> <p>Refers to the creation date according to the ISO 8601 date format: 20220401.</p> </li> <li> <p><code>type</code></p> <p>Refers to the document abbreviation or a document no.: mm or 1234.</p> <ul> <li>mm (meeting minute)</li> </ul> </li> <li> <p><code>scope</code></p> <p>Refers to the scope of the document: internal.</p> <ul> <li>internal</li> <li>cu (customer)</li> <li>su (supplier)</li> </ul> </li> <li> <p><code>author</code></p> <p>Refers to the name, department, category of the document author: s.</p> <ul> <li>s (sentenz)</li> </ul> </li> <li> <p><code>title</code></p> <p>Refers to the title of the project: iphone-13.</p> </li> <li> <p><code>subtitle</code></p> <p>Refers to the subtitle of the project: mini.</p> </li> <li> <p><code>version</code></p> <p>Refers to the version of the document, default v01: v01.</p> </li> </ul> <p>Illustrations as follows:</p> <ul> <li>20220401_mm_internal_s_apple_iphone-13_mini_v01</li> </ul>"},{"location":"articles/data-management/#3-references","title":"3. References","text":"<ul> <li>Axiom data management article.</li> <li>Broad file structure article.</li> </ul>"},{"location":"articles/data-serialization-formats/","title":"Data Serialization Formats","text":"<p>Data serialization formats are used to convert complex data structures into a format that can be easily transmitted over a network or stored in a persistent storage medium.</p> <ul> <li>1. Category</li> <li>1.1. Text-based<ul> <li>1.1.1. JSON</li> <li>1.1.2. XML</li> <li>1.1.3. YAML</li> <li>1.1.4. TOML</li> </ul> </li> <li>1.2. Binary<ul> <li>1.2.1. Protocol Buffers</li> <li>1.2.2. MessagePack</li> <li>1.2.3. Avro</li> <li>1.2.4. BSON</li> </ul> </li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/data-serialization-formats/#1-category","title":"1. Category","text":"<p>Serialization formats are optimized for specific use cases, programming languages, and platforms. Some formats may be more suitable for certain types of data, such as numeric data or text data, while others may be better suited for large or complex data sets. The choice of serialization format will depend on a range of factors, such as the requirements for data transfer and storage, the available tools and libraries, and the constraints of the system being used.</p>"},{"location":"articles/data-serialization-formats/#11-text-based","title":"1.1. Text-based","text":"<p>Text-based data serialization formats encode data in a human-readable text format, such as JSON, XML, YAML, and TOML. Text-based formats are widely used for data interchange and storage, as they are easy to read and write, and can be edited using a simple text editor. They are also platform-independent, meaning they can be used on any platform that supports the format.</p>"},{"location":"articles/data-serialization-formats/#111-json","title":"1.1.1. JSON","text":"<p>JSON (JavaScript Object Notation) is a text-based data serialization format that is commonly used for web applications and REST APIs. It is based on a subset of the JavaScript programming language, and is designed to be lightweight, easy to read and write, and easy to parse. JSON supports a variety of data types, including strings, numbers, boolean values, arrays, and objects.</p> <pre><code>{\n  \"name\": \"John Doe\",\n  \"age\": 35,\n  \"isMarried\": false,\n  \"hobbies\": [\"reading\", \"writing\", \"running\"],\n  \"address\": {\n    \"street\": \"123 Main St\",\n    \"city\": \"Anytown\",\n    \"state\": \"CA\",\n    \"zip\": \"12345\"\n  }\n}\n</code></pre>"},{"location":"articles/data-serialization-formats/#112-xml","title":"1.1.2. XML","text":"<p>XML (Extensible Markup Language) is a text-based data serialization format that is commonly used for data exchange and storage. It is based on a set of rules for encoding documents in a format that is both human-readable and machine-readable. XML supports a variety of data types, including text, numbers, boolean values, arrays, and objects.</p> <pre><code>&lt;person&gt;\n  &lt;name&gt;John Doe&lt;/name&gt;\n  &lt;age&gt;35&lt;/age&gt;\n  &lt;isMarried&gt;false&lt;/isMarried&gt;\n  &lt;hobbies&gt;\n    &lt;hobby&gt;reading&lt;/hobby&gt;\n    &lt;hobby&gt;writing&lt;/hobby&gt;\n    &lt;hobby&gt;running&lt;/hobby&gt;\n  &lt;/hobbies&gt;\n  &lt;address&gt;\n    &lt;street&gt;123 Main St&lt;/street&gt;\n    &lt;city&gt;Anytown&lt;/city&gt;\n    &lt;state&gt;CA&lt;/state&gt;\n    &lt;zip&gt;12345&lt;/zip&gt;\n  &lt;/address&gt;\n&lt;/person&gt;\n</code></pre>"},{"location":"articles/data-serialization-formats/#113-yaml","title":"1.1.3. YAML","text":"<p>YAML (YAML Ain't Markup Language): YAML is a text-based data serialization format that is designed to be human-readable and easy to write. It is often used for configuration files and data exchange between systems. YAML uses indentation and whitespace to define the structure of the data being serialized. It supports a variety of data types, including strings, numbers, boolean values, arrays, and objects.</p> <pre><code>name: John Doe\nage: 35\nisMarried: false\nhobbies:\n  - reading\n  - writing\n  - running\naddress:\n  street: 123 Main St\n  city: Anytown\n  state: CA\n  zip: '12345' \n</code></pre>"},{"location":"articles/data-serialization-formats/#114-toml","title":"1.1.4. TOML","text":"<p>TOML (Tom's Obvious, Minimal Language) is a text-based data serialization format that is designed to be easy to read and write, while still being expressive enough to handle complex data structures. It is often used for configuration files and data exchange between systems. TOML uses a key-value pair syntax to define the structure of the data being serialized. It supports a variety of data types, including strings, numbers, boolean values, arrays, and objects.</p> <pre><code>name = \"John Doe\"\nage = 35\nisMarried = false\n\n[hobbies]\nhobbies = [\"reading\", \"writing\", \"running\"]\n\n[address]\nstreet = \"123 Main St\"\ncity = \"Anytown\"\nstate = \"CA\"\nzip = \"12345\"\n</code></pre>"},{"location":"articles/data-serialization-formats/#12-binary","title":"1.2. Binary","text":"<p>Binary data serialization formats encode data in binary format, which is more compact and efficient than text-based formats. Examples of binary serialization formats include Protocol Buffers, Apache Thrift, BSON, and Avro. Binary formats are often used for high-performance computing and data-intensive applications, where efficiency and speed are critical.</p>"},{"location":"articles/data-serialization-formats/#121-protocol-buffers","title":"1.2.1. Protocol Buffers","text":"<p>Protocol Buffers is a binary data serialization format that is designed to be compact, efficient, and extensible. It was developed by Google and is used extensively in Google's own software systems. Protocol Buffers uses a language-agnostic schema to define the structure of the data being serialized.</p> <pre><code>message Person {\n  string name = 1;\n  int32 age = 2;\n  bool is_married = 3;\n  repeated string hobbies = 4;\n  Address address = 5;\n}\n\nmessage Address {\n  string street = 1;\n  string city = 2;\n  string state = 3;\n  string zip = 4;\n}\n</code></pre>"},{"location":"articles/data-serialization-formats/#122-messagepack","title":"1.2.2. MessagePack","text":"<p>MessagePack is a binary data serialization format that is designed to be fast, compact, and efficient. It is often used for real-time data exchange and storage, and supports a wide range of programming languages. MessagePack uses a simple binary format to encode data, with support for a variety of data types, including integers, floating-point numbers, boolean values, arrays, and maps.</p> <pre><code>\\x85\\xa4name\\xa7John Doe\\xa3age\\x23\\x23\\x00\\xa9isMarried\\xc3\\xa7hobbies\\x93\\xa7reading\\xa7writing\\xa6running\\xa7address\\x84\\xa6street\\xab123 Main St\\xa4city\\xa7Anytown\\xa5state\\xa2CA\\xa3zip\\xcd\\x00\\x01\\xe2\n</code></pre>"},{"location":"articles/data-serialization-formats/#123-avro","title":"1.2.3. Avro","text":"<p>Avro is a binary data serialization format that is designed to be efficient, extensible, and schema-based. It is often used for high-performance data processing and storage, and is widely used in the Apache Hadoop ecosystem. Avro uses a schema to define the structure of the data being serialized, and supports a variety of data types, including strings, integers, floating-point numbers, boolean values, arrays, and maps.</p> <pre><code>{\n  \"namespace\": \"example.avro\",\n  \"type\": \"record\",\n  \"name\": \"Person\",\n  \"fields\": [\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"age\", \"type\": \"int\"},\n    {\"name\": \"isMarried\", \"type\": \"boolean\"},\n    {\"name\": \"hobbies\", \"type\": {\"type\": \"array\", \"items\": \"string\"}},\n    {\"name\": \"address\", \"type\": {\n      \"type\": \"record\",\n      \"name\": \"Address\",\n      \"fields\": [\n        {\"name\": \"street\", \"type\": \"string\"},\n        {\"name\": \"city\", \"type\": \"string\"},\n        {\"name\": \"state\", \"type\": \"string\"},\n        {\"name\": \"zip\", \"type\": \"string\"}\n      ]\n    }}\n  ]\n}\n</code></pre>"},{"location":"articles/data-serialization-formats/#124-bson","title":"1.2.4. BSON","text":"<p>BSON (Binary JSON) is a binary data serialization format that is designed to be efficient, flexible, and easy to parse. It is often used for data storage and transmission in high-performance systems like MongoDB. BSON uses a binary encoding to represent data, with support for a variety of data types, including strings, integers, floating-point numbers, boolean values, arrays, and objects.</p> <pre><code>\\x48\\x00\\x00\\x00\\x04name\\x00\\x0BJane Doe\\x00\\x05age\\x00\\x1F\\x00\\x00\\x00\\x08is_married\\x00\\x01\\x08hobbies\\x00\\x1C\\x00\\x00\\x00\\x020\\x00\\x07reading\\x00\\x07writing\\x00\\x06running\\x00\\x08address\\x00\\x1E\\x00\\x00\\x00\\x03street\\x00\\x0F123 Main St\\x00\\x04city\\x00\\x07Anytown\\x00\\x05state\\x00\\x02CA\\x00\\x03zip\\x00\\x05\\x31\\x32\\x33\\x34\\x35\\x00\n</code></pre>"},{"location":"articles/data-serialization-formats/#2-principle","title":"2. Principle","text":"<p>Principles are essential to ensure that data serialization formats are reliable, flexible, and scalable for use in a wide range of systems and applications.</p> <ul> <li> <p>Compactness</p> <p>Serialization formats should be designed to produce compact representations of data, which minimize storage space and transmission bandwidth. Compactness is particularly important in distributed systems, where large amounts of data may need to be transmitted across networks with limited bandwidth.</p> </li> <li> <p>Portability</p> <p>Serialization formats should be designed to be portable across different platforms, programming languages, and operating systems. This helps ensure that data can be exchanged between different systems without requiring custom code or data translation.</p> </li> <li> <p>Efficiency</p> <p>Data serialization formats should be efficient in terms of memory usage and processing time. This is especially important for large-scale systems where data needs to be transmitted and processed quickly.</p> </li> <li> <p>Flexibility</p> <p>Data serialization formats should be flexible enough to support a wide range of data types and structures. This includes support for nested structures, arrays, and other complex data structures.</p> </li> <li> <p>Interoperability</p> <p>Serialization formats should be designed to be interoperable with other data formats, protocols, and systems. This helps ensure that data can be integrated with other systems and data sources, and can be used in a variety of applications and contexts.</p> </li> <li> <p>Readability</p> <p>Data serialization formats should be readable and easy to understand by humans. This is important for debugging, troubleshooting, and maintaining systems.</p> </li> <li> <p>Security</p> <p>Data serialization formats should be designed to be secure, with support for data encryption, digital signatures, and other security measures.</p> </li> <li> <p>Versioning</p> <p>Data serialization formats should support versioning, to enable changes to be made to the data structure without breaking existing systems or data.</p> </li> <li> <p>Extensibility</p> <p>Serialization formats should be designed to support extensions and evolution over time, as new data types and requirements emerge. This helps ensure that data can continue to be used and exchanged in the face of changing technology and requirements.</p> </li> <li> <p>Robustness</p> <p>Serialization formats should be designed to be robust and resilient in the face of errors, edge cases, and unexpected input. This helps ensure that data can be exchanged reliably and without data loss or corruption.</p> </li> </ul>"},{"location":"articles/data-serialization-formats/#3-best-practice","title":"3. Best Practice","text":"<p>Best practices are based on principles and are essential for ensuring the reliability, performance, and scalability of data serialization formats in a wide range of systems and applications.</p> <ul> <li> <p>Choose the appropriate format</p> <p>When selecting a data serialization format, it's important to choose the one that best fits the specific use case. This requires an understanding of the data structure, size, and performance requirements of the system. It's also important to consider factors such as interoperability, security, and extensibility.</p> </li> <li> <p>Use schema validation</p> <p>Schema validation is a process that verifies that the data conforms to a predefined structure or schema. This can help ensure data quality and prevent errors in the system. Some data serialization formats, such as JSON and XML, support schema validation.</p> </li> <li> <p>Avoid unnecessary data duplication</p> <p>Data serialization formats should be designed to minimize unnecessary data duplication. This can be achieved by using references or pointers to existing data, rather than duplicating the data itself. This can help reduce storage requirements and improve performance.</p> </li> <li> <p>Use efficient encoding</p> <p>Efficient encoding is essential for minimizing memory usage and maximizing performance. Some data serialization formats, such as CBOR and MessagePack, use binary encoding, which is more efficient than text encoding.</p> </li> <li> <p>Consider backward and forward compatibility</p> <p>When designing a data serialization format, it's important to consider backward and forward compatibility. Backward compatibility ensures that new versions of the data can be read by older systems, while forward compatibility ensures that older versions of the data can be read by newer systems. This requires careful attention to versioning and data structure evolution.</p> </li> <li> <p>Use appropriate data types</p> <p>Data serialization formats should use appropriate data types to accurately represent the data. This includes support for numeric types, date and time types, and other specialized data types as needed.</p> </li> <li> <p>Use compression</p> <p>Compression can be used to reduce the size of the serialized data, which can help improve performance and reduce storage requirements. Some data serialization formats, such as Protocol Buffers and Apache Avro, support built-in compression.</p> </li> <li> <p>Use encryption</p> <p>To protect sensitive data, it's important to use encryption when transmitting data over untrusted networks. This can be done using standard encryption protocols such as SSL or TLS, or by using custom encryption schemes.</p> </li> <li> <p>Use versioning</p> <p>As data serialization formats evolve over time, it's important to use versioning to ensure that older and newer systems can still communicate effectively. This can be done by including a version identifier in the serialized data, or by using a version-specific schema.</p> </li> </ul>"},{"location":"articles/data-serialization-formats/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Serialization</p> <p>The process of converting data from its native format into a format that can be transmitted or stored, typically in a binary or text-based format.</p> </li> <li> <p>Deserialization</p> <p>The process of converting serialized data back into its original format.</p> </li> <li> <p>Data format</p> <p>The specific structure and syntax used to encode and decode data, such as JSON, XML, or Protocol Buffers.</p> </li> <li> <p>Schema</p> <p>A formal specification of the structure and data types used in a particular serialization format. Schemas can be used to validate input data and ensure compatibility between different systems.</p> </li> <li> <p>Protocol</p> <p>A set of rules and standards governing how data is transmitted and received between different systems, often including a specific serialization format.</p> </li> <li> <p>Encoding</p> <p>The process of converting data from one representation to another, such as from text to binary.</p> </li> <li> <p>Decoding</p> <p>The process of converting encoded data back into its original representation.</p> </li> <li> <p>Wire format</p> <p>The specific binary format used to transmit data over a network, such as the format used by HTTP or TCP/IP.</p> </li> <li> <p>Type mapping</p> <p>The process of mapping data types between different programming languages or serialization formats, to ensure that data can be transmitted and decoded correctly.</p> </li> <li> <p>RPC (Remote Procedure Call)</p> <p>A protocol that allows a program to call a function or method on a remote system, using a specific serialization format and communication protocol.</p> </li> <li> <p>Marshalling</p> <p>The process of converting data from its native programming language into a format that can be serialized and transmitted, often using a library or API provided by the serialization format.</p> </li> <li> <p>Unmarshalling</p> <p>The process of converting serialized data back into its native programming language, often using a library or API provided by the serialization format.</p> </li> <li> <p>Payload</p> <p>The actual data being transmitted or stored, as opposed to any additional metadata or headers.</p> </li> <li> <p>Endianness</p> <p>Refers to the order in which bytes are stored and transmitted, which can vary between different systems and architectures. Endianness can affect how data is serialized and deserialized, and must be taken into account to ensure compatibility between different systems.</p> </li> <li> <p>Canonicalization</p> <p>The process of converting data into a canonical form, which can help ensure compatibility between different systems and prevent security vulnerabilities such as XML injection attacks.</p> </li> <li> <p>Tag or key</p> <p>A unique identifier used to label or identify data elements within a serialized message, such as a field name in a JSON object or an element name in an XML document.</p> </li> <li> <p>Value</p> <p>The actual data associated with a tag or key, which can take various forms depending on the serialization format and data type.</p> </li> <li> <p>Message</p> <p>A complete serialized data structure, which may contain multiple tags or keys and their associated values.</p> </li> <li> <p>Pretty-printing</p> <p>A technique used to format serialized data in a human-readable way, by adding whitespace and line breaks to separate different elements of the message. This can make it easier to debug and troubleshoot serialization issues, but can also increase message size.</p> </li> <li> <p>Backward compatibility</p> <p>The ability to deserialize data from an older version of a schema or serialization format, often achieved through techniques such as versioning and data migration.</p> </li> <li> <p>Forward compatibility</p> <p>The ability to deserialize data from a newer version of a schema or serialization format, often achieved through techniques such as schema evolution and data transformation.</p> </li> <li> <p>Metadata</p> <p>Additional information included with a serialized message, such as timestamps, source and destination addresses, or authentication tokens.</p> </li> <li> <p>Streaming</p> <p>The process of transmitting or receiving large serialized messages in a piecemeal fashion, often using a specialized streaming API or protocol. Streaming can improve performance and reduce memory usage in certain scenarios, but can also add complexity and overhead.</p> </li> </ul>"},{"location":"articles/database/","title":"Database","text":"<p>A database is a collection of structured data that is organized and stored in a way that enables efficient retrieval and manipulation of the data. A database can be used to store and manage data for a variety of purposes, such as maintaining a record of transactions, storing customer information, or tracking inventory.</p> <p>Databases are typically managed by a database management system (DBMS), which provides a set of tools and interfaces for creating, modifying, and querying the data stored in the database. This can include a graphical user interface for managing the database, or a set of command-line tools and programming APIs for accessing and manipulating the data programmatically.</p> <ul> <li>1. Category</li> <li>1.1. Database<ul> <li>1.1.1. Relational Databases</li> <li>1.1.2. Object-Relational Databases</li> <li>1.1.3. NoSQL Databases</li> <li>1.1.4. Columnar Databases</li> <li>1.1.5. Graph Databases</li> <li>1.1.6. Time-series Databases</li> <li>1.1.7. In-Memory Databases</li> <li>1.1.8. Cloud Databases</li> <li>1.1.9. Hybrid Databases</li> <li>1.1.10. Distributed Databases</li> <li>1.1.11. Embedded Databases</li> <li>1.1.12. XML Databases</li> <li>1.1.13. Key-Value Databases</li> <li>1.1.14. Document Databases</li> <li>1.1.15. Geospatial Databases</li> <li>1.1.16. Edge Databases</li> <li>1.1.17. Streaming Databases</li> <li>1.1.18. Machine Learning Databases</li> <li>1.1.19. Event-Driven Databases</li> <li>1.1.20. NewSQL Databases</li> <li>1.1.21. Multi-Model Databases</li> </ul> </li> <li>1.2. Management<ul> <li>1.2.1. DBMS</li> <li>1.2.2. Transaction</li> <li>1.2.3. ACID</li> <li>1.2.4. CRUD</li> <li>1.2.5. HTAP</li> <li>1.2.6. OLTP</li> <li>1.2.7. OLAP</li> <li>1.2.8. FDW</li> <li>1.2.9. Streaming</li> <li>1.2.10. Geospatial</li> <li>1.2.11. Time Series</li> <li>1.2.12. Distributed Tables</li> <li>1.2.13. Schema Design</li> </ul> </li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/database/#1-category","title":"1. Category","text":""},{"location":"articles/database/#11-database","title":"1.1. Database","text":""},{"location":"articles/database/#111-relational-databases","title":"1.1.1. Relational Databases","text":"<p>Relational databases store data in a structured format using tables and relationships. The data is organized into rows and columns, and relationships are established between tables using keys.</p> <p>DBMS tools: MySQL, Oracle Database, Microsoft SQL Server.</p>"},{"location":"articles/database/#112-object-relational-databases","title":"1.1.2. Object-Relational Databases","text":"<p>Object-Relational databases combine the features of relational databases and object-oriented programming. They store data in tables as well as objects, allowing for more complex data structures and relationships.</p> <p>DBMS tools: PostgreSQL, Oracle Database.</p>"},{"location":"articles/database/#113-nosql-databases","title":"1.1.3. NoSQL Databases","text":"<p>NoSQL databases do not use a fixed schema, and instead store data in a variety of formats, such as key-value pairs, documents, or graph structures. They are designed for scalability and handling large amounts of unstructured data.</p> <p>DBMS tools: MongoDB, Cassandra, Amazon DynamoDB.</p>"},{"location":"articles/database/#114-columnar-databases","title":"1.1.4. Columnar Databases","text":"<p>Columnar databases store data in columns rather than rows, providing faster query performance for large datasets, improving data performance and scalability. They are typically used in data warehousing and analytics.</p> <p>DBMS tools: Apache Cassandra, Amazon Redshift, Google Bigtable.</p>"},{"location":"articles/database/#115-graph-databases","title":"1.1.5. Graph Databases","text":"<p>Graph databases store data as nodes and relationships, providing efficient querying and navigation of complex relationships. They are commonly used in social network analysis, recommendation systems, and fraud detection.</p> <p>DBMS tools: Neo4j, Amazon Neptune, OrientDB.</p>"},{"location":"articles/database/#116-time-series-databases","title":"1.1.6. Time-series Databases","text":"<p>Time-series databases are specialized databases optimized for storing and retrieving time-stamped data. They are commonly used for monitoring and analyzing sensor data, financial data, and IoT data.</p> <p>DBMS tools: InfluxDB, TimescaleDB, OpenTSDB.</p>"},{"location":"articles/database/#117-in-memory-databases","title":"1.1.7. In-Memory Databases","text":"<p>In-Memory databases store data in RAM, providing extremely fast query performance. They are used for real-time applications, such as gaming, financial trading, and telecommunications.</p> <p>DBMS tools: Redis, Apache Ignite, MemSQL.</p>"},{"location":"articles/database/#118-cloud-databases","title":"1.1.8. Cloud Databases","text":"<p>Cloud databases are databases that run on cloud computing platforms, such as Amazon Web Services, Microsoft Azure, or Google Cloud Platform. They provide scalable, on-demand access to databases, allowing for easy provisioning and management of resources.</p> <p>DBMS tools: Amazon RDS, Microsoft Azure SQL Database, Google Cloud SQL.</p>"},{"location":"articles/database/#119-hybrid-databases","title":"1.1.9. Hybrid Databases","text":"<p>Hybrid databases are databases that combine the features of multiple database categories, allowing for a variety of data formats and structures. They are designed to handle the demands of modern applications, which often require a combination of structured and unstructured data.</p> <p>DBMS tools: Amazon DocumentDB, Microsoft Azure Cosmos DB, Google Cloud Firestore.</p>"},{"location":"articles/database/#1110-distributed-databases","title":"1.1.10. Distributed Databases","text":"<p>Distributed databases are databases that are split across multiple servers, providing increased scalability and fault tolerance. They are commonly used for large-scale, data-intensive applications, such as e-commerce, online gaming, and scientific simulations.</p> <p>DBMS tools: Apache Cassandra, Amazon DynamoDB, Microsoft Azure Cosmos DB.</p>"},{"location":"articles/database/#1111-embedded-databases","title":"1.1.11. Embedded Databases","text":"<p>Embedded databases are databases that run directly on an application, rather than on a separate server. They are typically used for small-scale, standalone applications, such as mobile devices or embedded systems.</p> <p>DBMS tools: SQLite, Berkeley DB, H2.</p>"},{"location":"articles/database/#1112-xml-databases","title":"1.1.12. XML Databases","text":"<p>XML databases store data in the XML format, providing a flexible and extensible data structure. They are commonly used for data exchange and web services, as well as for document management.</p> <p>DBMS tools: eXist-db, BaseX, Apache MarkLogic.</p>"},{"location":"articles/database/#1113-key-value-databases","title":"1.1.13. Key-Value Databases","text":"<p>Key-value databases are databases that store data as key-value pairs, allowing for simple and efficient data storage and retrieval. They often support advanced key-value operations, such as key-value updates, key-value deletions, and key-value search, and they are often used in applications that require simple and efficient data storage and retrieval.</p> <p>DBMS tools: Amazon DynamoDB, Riak, Redis.</p>"},{"location":"articles/database/#1114-document-databases","title":"1.1.14. Document Databases","text":"<p>Document databases are databases that store data as semi-structured or unstructured documents, allowing for the flexible storage and retrieval of data elements. They often support advanced document operations, such as document queries, document updates, and document indexing, and they are often used in applications that require the flexible storage and retrieval of semi-structured or unstructured data elements.</p> <p>DBMS tools: MongoDB, Amazon DocumentDB, Couchbase.</p>"},{"location":"articles/database/#1115-geospatial-databases","title":"1.1.15. Geospatial Databases","text":"<p>Geospatial databases are databases that are optimized for storing and retrieving geospatial data, such as locations, maps, and geographic information. They often support advanced geospatial operations, such as spatial indexing, geospatial queries, and geospatial analytics, and they are often used in applications that require geospatial data processing and analysis.</p> <p>DBMS tools: PostGIS, Oracle Spatial and Graph, MongoDB Geospatial.</p>"},{"location":"articles/database/#1116-edge-databases","title":"1.1.16. Edge Databases","text":"<p>Edge databases are databases that run on edge devices, such as IoT devices, and are designed for low-latency data processing and storage. They often support advanced edge operations, such as local data processing, data aggregation, and data synchronization, and they are often used in applications that require low-latency data processing and storage at the edge.</p> <p>DBMS tools: Amazon Greengrass, Microsoft Azure IoT Edge, Google Cloud IoT Edge.</p>"},{"location":"articles/database/#1117-streaming-databases","title":"1.1.17. Streaming Databases","text":"<p>Streaming databases are databases that are designed for real-time data ingestion, processing, and analysis of large amounts of data. They often support advanced streaming operations, such as real-time data analysis, event processing, and data pipelines, and they are often used in applications that require real-time data processing and analysis.</p> <p>DBMS tools: Apache Kafka, Apache Flink, Amazon Kinesis.</p>"},{"location":"articles/database/#1118-machine-learning-databases","title":"1.1.18. Machine Learning Databases","text":"<p>Machine learning databases are databases that provide built-in machine learning algorithms and support for data science workflows. They often support advanced machine learning operations, such as data preprocessing, model training, and model deployment, and they are often used in applications that require machine learning and data science capabilities.</p> <p>DBMS tools: Google BigQuery ML, Amazon SageMaker, Databricks.</p>"},{"location":"articles/database/#1119-event-driven-databases","title":"1.1.19. Event-Driven Databases","text":"<p>Event-driven databases are databases that support event-driven architectures and real-time data processing. They often support advanced event-processing operations, such as event streams, event sourcing, and event notifications, and they are often used in applications that require real-time data processing and event-driven architectures.</p> <p>DBMS tools: Apache Kafka, Apache Pulsar, Apache Flink.</p>"},{"location":"articles/database/#1120-newsql-databases","title":"1.1.20. NewSQL Databases","text":"<p>NewSQL databases are databases that combine the scalability and performance of NoSQL databases with the transactional consistency and durability of relational databases. They often support advanced SQL operations, and they are often used in applications that require high performance and strong consistency guarantees.</p> <p>DBMS tools: Google Spanner, CockroachDB, NuoDB.</p>"},{"location":"articles/database/#1121-multi-model-databases","title":"1.1.21. Multi-Model Databases","text":"<p>Multi-model databases are databases that support multiple data models, such as relational, document-oriented, graph, and key-value. This allows for greater flexibility in data modeling and the ability to handle a variety of data structures in a single database. Multi-model databases are often used in applications that require the ability to store and retrieve different types of data in a unified way.</p> <p>DBMS tools: ArangoDB, Amazon DocumentDB, OrientDB.</p>"},{"location":"articles/database/#12-management","title":"1.2. Management","text":"<p>Database Management involves organizing, storing, and maintaining data within a database system. It encompasses design principles, technologies, and methodologies to ensure data reliability, efficiency, and security. The tasks involved in database management include defining data elements, creating and maintaining the database schema, enforcing data integrity and security, optimizing performance, and implementing backup and recovery mechanisms.</p>"},{"location":"articles/database/#121-dbms","title":"1.2.1. DBMS","text":"<p>A database management system (DBMS) is a software system that allows users to define, create, maintain, and control access to databases. A DBMS manages the organization, manipulation, storage, and retrieval of data, and provides an interface for interaction with the data.</p> <p>Key functions of a DBMS include:</p> <ul> <li> <p>Data Storage</p> <p>A DBMS provides a centralized repository for storing data, which can be organized into tables and other structures.</p> </li> <li> <p>Data Retrieval</p> <p>A DBMS allows users to retrieve specific data from the database, either through queries or by browsing the data directly.</p> </li> <li> <p>Data Manipulation</p> <p>A DBMS provides a set of tools for modifying and updating data in the database, including adding new records, deleting existing records, and modifying existing data.</p> </li> <li> <p>Data Security</p> <p>A DBMS provides a variety of security features to protect the data in the database, including authentication, authorization, and access control.</p> </li> <li> <p>Data Consistency</p> <p>A DBMS provides a set of features to ensure the consistency and reliability of the data in the database, including transaction management and data validation.</p> </li> </ul>"},{"location":"articles/database/#122-transaction","title":"1.2.2. Transaction","text":"<p>A transaction in database systems refers to a unit of work that is executed within a database management system (DBMS). The primary objective of transactions is to maintain the consistency and integrity of data within a database, by ensuring that the database remains in a consistent state even in the face of failures or errors. Transactions achieve this objective by implementing the Atomicity, Consistency, Isolation, and Durability (ACID) properties, which guarantee that either all the operations within a transaction are completed successfully, or none of them are completed at all.</p>"},{"location":"articles/database/#123-acid","title":"1.2.3. ACID","text":"<p>ACID stands for Atomicity, Consistency, Isolation, and Durability. It is a set of properties that ensure the consistency and reliability of database transactions.</p> <p>ACID properties ensure that database transactions are reliable, consistent, and resistant to failures or data loss.</p> <ul> <li> <p>Atomicity</p> <p>Atomicity property ensures that a transaction is treated as a single unit of work, either completing fully or rolling back completely in the event of a failure. This ensures that the database remains in a consistent state, even if part of the transaction fails.</p> </li> <li> <p>Consistency</p> <p>Consistency property ensures that a transaction brings the database from one consistent state to another, without introducing any invalid data. This is typically achieved through the use of constraints and data validation.</p> </li> <li> <p>Isolation</p> <p>Isolation property ensures that concurrent transactions do not interfere with each other, and that each transaction is executed as if it were the only transaction taking place. This prevents data corruption and other issues that could arise from multiple transactions accessing the same data simultaneously.</p> </li> <li> <p>Durability</p> <p>Durability property ensures that once a transaction is committed, its effects are permanent and cannot be lost in the event of a failure. This is achieved through the use of transaction logs, backups, and other measures.</p> </li> </ul>"},{"location":"articles/database/#124-crud","title":"1.2.4. CRUD","text":"<p>CRUD stands for Create, Read, Update, and Delete. It is a set of basic operations that can be performed on a database, and is used to manage data in a database.</p> <p>The CRUD operations are the foundation of most database management systems, and are used to perform the basic functions of managing data in a database.</p> <ul> <li> <p>Create</p> <p>Create operation allows a user to add new data to a database. This can include creating new records, tables, or other structures in the database.</p> </li> <li> <p>Read</p> <p>Read operation allows a user to retrieve data from a database. This can include reading specific records, performing searches, or retrieving data for reporting purposes.</p> </li> <li> <p>Update</p> <p>Update operation allows a user to modify existing data in a database. This can include updating existing records, adding new fields to tables, or changing the structure of the database.</p> </li> <li> <p>Delete</p> <p>Delete operation allows a user to remove data from a database. This can include deleting individual records, tables, or other structures in the database.</p> </li> </ul>"},{"location":"articles/database/#125-htap","title":"1.2.5. HTAP","text":"<p>HTAP (Hybrid Transactional/Analytical Processing) is a term used to describe a database architecture that combines the characteristics of both transactional (OLTP) and analytical (OLAP) processing. HTAP databases are designed to handle both transactional and analytical workloads in real-time, providing a single source of truth for data that is both transactional and analytical.</p>"},{"location":"articles/database/#126-oltp","title":"1.2.6. OLTP","text":"<p>OLTP (Online Transactional Processing) is a database design approach that focuses on optimizing transactional processing, such as inserting, updating, or deleting data. OLTP databases are designed to handle large numbers of concurrent transactions, and are optimized for high-speed and low latency.</p>"},{"location":"articles/database/#127-olap","title":"1.2.7. OLAP","text":"<p>OLAP (Online Analytical Processing) is a database design approach that focuses on optimizing analytical processing, such as complex queries and reporting. OLAP databases are designed to handle large amounts of data and complex calculations, and are optimized for high-performance data analysis.</p>"},{"location":"articles/database/#128-fdw","title":"1.2.8. FDW","text":"<p>FDW (Foreign Data Wrapper) is a foreign data wrapper is a database component that enables data stored in external databases to be treated as if it were part of the local database. FDWs provide a convenient way to access data stored in different databases, and can be used to create a single virtual database from multiple sources.</p>"},{"location":"articles/database/#129-streaming","title":"1.2.9. Streaming","text":"<p>Streaming refers to the process of continuously receiving and processing data in real-time. In database systems, streaming can be used to handle large amounts of data that are generated in real-time, such as IoT data or social media data.</p>"},{"location":"articles/database/#1210-geospatial","title":"1.2.10. Geospatial","text":"<p>Geospatial databases are databases that are designed to store and manage geographic data, such as location-based data, maps, and satellite imagery. Geospatial databases are optimized for the storage and management of large amounts of data with geographic attributes, and are widely used in industries such as transportation, logistics, and real estate.</p>"},{"location":"articles/database/#1211-time-series","title":"1.2.11. Time Series","text":"<p>Time series databases are databases that are designed to store and manage time-based data, such as sensor data, stock prices, and weather data. Time series databases are optimized for the storage and management of large amounts of time-based data, and are widely used in industries such as finance, IoT, and weather forecasting.</p>"},{"location":"articles/database/#1212-distributed-tables","title":"1.2.12. Distributed Tables","text":"<p>Distributed tables are tables in a database that are spread across multiple nodes in a distributed database system. Distributed tables provide a way to scale databases horizontally, allowing for the storage and management of large amounts of data across multiple nodes. This provides increased performance and scalability, as well as improved fault tolerance.</p>"},{"location":"articles/database/#1213-schema-design","title":"1.2.13. Schema Design","text":"<p>Schema design is a critical component of database architecture and design, as it defines the structure of the data in a database. Schema design involves creating tables, columns, and relationships to represent the data requirements of an application. The schema defines the structure of the data and the relationships between the data elements, and it is used to ensure that the data is stored in a consistent and organized manner.</p>"},{"location":"articles/database/#2-principle","title":"2. Principle","text":"<ul> <li> <p>Atomicity</p> <p>The principle of atomicity ensures that a database transaction is either completed in its entirety or not completed at all, ensuring data consistency and integrity.</p> </li> <li> <p>Consistency</p> <p>The principle of consistency ensures that a database conforms to a set of rules or constraints, such as uniqueness, referential integrity, and domain integrity, ensuring data accuracy and reliability.</p> </li> <li> <p>Isolation</p> <p>The principle of isolation ensures that multiple database transactions are executed as if they were executing independently, ensuring data consistency and avoiding concurrent access conflicts.</p> </li> <li> <p>Durability</p> <p>The principle of durability ensures that once a database transaction is committed, its effects are permanent and will persist even in the event of a system failure, ensuring data reliability and availability.</p> </li> <li> <p>Normalization</p> <p>The principle of normalization is the process of organizing a database into a set of tables with well-defined relationships, reducing data redundancy and increasing data accuracy and consistency.</p> </li> <li> <p>Referential Integrity</p> <p>The principle of referential integrity is the requirement that every foreign key in a database must have a corresponding primary key in another table, ensuring data accuracy and consistency.</p> </li> <li> <p>Data Abstraction</p> <p>The principle of data abstraction is the process of hiding the implementation details of a database from the users, providing a higher-level view of the data and simplifying database access.</p> </li> <li> <p>Scalability</p> <p>The principle of scalability is the ability of a database to handle increasing amounts of data and users as the system grows, ensuring data reliability and availability.</p> </li> <li> <p>Performance</p> <p>The principle of performance is the ability of a database to efficiently retrieve and manipulate data, ensuring that database operations are executed quickly and effectively.</p> </li> <li> <p>Security</p> <p>The principle of security is the protection of database data from unauthorized access and manipulation, ensuring data privacy and confidentiality.</p> </li> <li> <p>Backup and Recovery</p> <p>The principle of backup and recovery is the process of creating and maintaining backups of database data, ensuring data availability and reliability in the event of a system failure or data loss.</p> </li> <li> <p>Data Encryption</p> <p>The principle of data encryption is the process of converting sensitive data into an unreadable format, ensuring data privacy and confidentiality.</p> </li> <li> <p>ata Access Control</p> <p>The principle of data access control is the process of controlling access to database data based on user permissions, ensuring data security and confidentiality.</p> </li> <li> <p>Data Modeling</p> <p>The principle of data modeling is the process of creating a conceptual representation of a database, ensuring data consistency and accuracy.</p> </li> <li> <p>Data Integration</p> <p>The principle of data integration is the process of combining data from multiple sources into a single, unified view, ensuring data accuracy and consistency.</p> </li> <li> <p>Data Migration</p> <p>The principle of data migration is the process of transferring data from one database to another, ensuring data availability and reliability.</p> </li> <li> <p>Query Optimization</p> <p>The principle of query optimization is the process of improving the efficiency of database queries, ensuring fast and efficient data retrieval.</p> </li> <li> <p>Indexing</p> <p>The principle of indexing is the process of creating an index of data elements, allowing for fast and efficient data retrieval.</p> </li> <li> <p>Partitioning</p> <p>The principle of partitioning is the process of dividing a database into smaller, more manageable parts, improving database performance and scalability.</p> </li> <li> <p>Replication</p> <p>The principle of replication is the process of creating multiple copies of a database, ensuring data availability and reliability.</p> </li> <li> <p>Redundancy</p> <p>The principle of redundancy is the process of creating multiple backups of database data, ensuring data availability and reliability in the event of a system failure.</p> </li> <li> <p>Fault Tolerance</p> <p>The principle of fault tolerance is the ability of a database to continue functioning in the event of a system failure or error, ensuring data availability and reliability.</p> </li> <li> <p>High Availability</p> <p>The principle of high availability is the ability of a database to be accessible and operational at all times, ensuring data availability and reliability.</p> </li> <li> <p>Load Balancing</p> <p>The principle of load balancing is the process of distributing database workloads across multiple servers, improving database performance and scalability.</p> </li> <li> <p>Data Warehousing</p> <p>The principle of data warehousing is the process of storing large amounts of data for analysis and reporting purposes, improving data accuracy and consistency.</p> </li> <li> <p>Data Marts</p> <p>The principle of data marts is the process of creating smaller, more focused data warehouses, improving data accuracy and consistency.</p> </li> <li> <p>Data Mining</p> <p>The principle of data mining is the process of discovering patterns and trends in large amounts of data, improving data accuracy and consistency.</p> </li> <li> <p>Business Intelligence</p> <p>The principle of business intelligence is the process of using data analysis and reporting to support business decision-making, improving data accuracy and consistency.</p> </li> <li> <p>Data Visualization</p> <p>The principle of data visualization is the process of presenting data in graphical and visual format, improving data accuracy and consistency.</p> </li> <li> <p>Big Data</p> <p>The principle of big data is the process of handling and analyzing large amounts of data, improving data accuracy and consistency.</p> </li> <li> <p>NoSQL</p> <p>The principle of NoSQL is a type of database that does not use a fixed schema and allows for flexible data modeling, improving data scalability and performance.</p> </li> </ul>"},{"location":"articles/database/#3-best-practice","title":"3. Best Practice","text":"<ul> <li> <p>Define Clear Objectives</p> <p>Determine the specific purpose and requirements of the database before starting the design process.</p> </li> <li> <p>Normalize Data</p> <p>Normalize the data to reduce redundancy and improve data accuracy and consistency.</p> </li> <li> <p>Choose the Right Database Technology</p> <p>Choose the database technology that best fits the requirements of the database, taking into consideration factors such as scalability, performance, and reliability.</p> </li> <li> <p>Design for Performance</p> <p>Design the database to optimize performance, taking into consideration factors such as indexing, query optimization, and caching.</p> </li> <li> <p>Implement Security Measures</p> <p>Implement security measures to protect sensitive data and prevent unauthorized access to the database.</p> </li> <li> <p>Monitor and Tune the Database</p> <p>Regularly monitor and tune the database to ensure optimal performance and prevent issues such as bottlenecks and deadlocks.</p> </li> <li> <p>Plan for Data Backup and Recovery</p> <p>Plan for data backup and recovery to ensure the availability and integrity of the database in case of disaster or other unexpected events.</p> </li> <li> <p>Document the Database</p> <p>Document the database design, schema, and data to help ensure accuracy and consistency over time.</p> </li> <li> <p>Consider Scalability</p> <p>Consider scalability when designing the database, ensuring that the database can easily be expanded to accommodate future growth.</p> </li> <li> <p>Continuously Evaluate and Improve</p> <p>Continuously evaluate and improve the database to ensure that it remains efficient and effective over time.</p> </li> <li> <p>Implement Access Controls</p> <p>Implement access controls to ensure that only authorized users can access the database and specific data.</p> </li> <li> <p>Use Unique Identifiers</p> <p>Use unique identifiers, such as primary keys, to ensure that data is accurately referenced and linked.</p> </li> <li> <p>Use Transactions</p> <p>Use transactions to ensure data integrity and consistency, even in the face of errors or unexpected events.</p> </li> <li> <p>Regularly Test and Verify Data</p> <p>Regularly test and verify data to ensure accuracy and completeness, and to identify and fix any issues.</p> </li> <li> <p>Maintain Data History</p> <p>Maintain data history to track changes and provide a historical view of the data.</p> </li> <li> <p>Plan for Disaster Recovery</p> <p>Plan for disaster recovery to ensure that the database can quickly and easily be recovered in the event of a disaster.</p> </li> <li> <p>Use Automated Tools</p> <p>Use automated tools, such as backups and monitoring software, to simplify and automate database management tasks.</p> </li> <li> <p>Implement Data Validation</p> <p>Implement data validation to ensure that data is entered correctly and meets specific criteria.</p> </li> <li> <p>Use Standard Data Formats</p> <p>Use standard data formats, such as XML or JSON, to simplify data exchange and integration.</p> </li> <li> <p>Monitor Database Health</p> <p>Monitor the health of the database to identify and address performance or reliability issues.</p> </li> <li> <p>Encrypt Sensitive Data</p> <p>Encrypt sensitive data to protect it from unauthorized access and to ensure privacy.</p> </li> <li> <p>Document Data Sources</p> <p>Document data sources to help ensure data accuracy and consistency and to simplify data integration.</p> </li> <li> <p>Use Triggers</p> <p>Use triggers to automate database actions and to ensure data consistency and accuracy.</p> </li> <li> <p>Implement Version Control</p> <p>Implement version control to track changes and ensure that multiple users can work on the database simultaneously.</p> </li> <li> <p>Plan for Data Archiving</p> <p>Plan for data archiving to help ensure data accuracy and consistency over time, and to reduce the size of the database.</p> </li> <li> <p>Use Stored Procedures</p> <p>Use stored procedures to improve database performance and to simplify complex database operations.</p> </li> <li> <p>Implement Data Partitioning</p> <p>Implement data partitioning to improve database performance and scalability.</p> </li> <li> <p>Use Data Warehousing</p> <p>Use data warehousing to improve data accuracy, consistency, and performance, especially for large amounts of data.</p> </li> <li> <p>Implement Automated Reporting</p> <p>Implement automated reporting to simplify the generation of reports and to improve data accuracy.</p> </li> <li> <p>Evaluate the Database Regularly</p> <p>Evaluate the database regularly to identify and address performance, reliability, and security issues, and to make improvements where necessary.</p> </li> <li> <p>Use Materialized Views</p> <p>Use materialized views to improve database performance and to simplify complex database queries.</p> </li> <li> <p>Implement Data Replication</p> <p>Implement data replication to ensure high availability and to improve database performance.</p> </li> <li> <p>Use Real-time Analytics</p> <p>Use real-time analytics to quickly analyze data and make decisions in real-time.</p> </li> <li> <p>Consider Multi-tenancy</p> <p>Consider multi-tenancy when designing the database, to ensure that multiple tenants can use the database simultaneously and securely.</p> </li> <li> <p>Implement Disaster Tolerance</p> <p>Implement disaster tolerance to ensure that the database remains available even in the face of a disaster.</p> </li> <li> <p>Use Compression</p> <p>Use compression to reduce the size of the database and to improve database performance.</p> </li> <li> <p>Implement Data Sharding</p> <p>Implement data sharding to improve database performance and scalability, especially for large amounts of data.</p> </li> <li> <p>Use Cloud Databases</p> <p>Use cloud databases to take advantage of the scalability, security, and reliability offered by cloud computing.</p> </li> <li> <p>Implement Auditing</p> <p>Implement auditing to track database activity, improve security, and comply with regulations.</p> </li> <li> <p>Optimize Queries</p> <p>Optimize queries to improve database performance and reduce response time.</p> </li> </ul>"},{"location":"articles/database/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Relational Database</p> <p>A type of database that organizes data into tables and uses relationships between the tables to store and retrieve data.</p> </li> <li> <p>Table</p> <p>A collection of related data organized into rows and columns in a relational database.</p> </li> <li> <p>Column</p> <p>A vertical data element in a table, representing a specific type of data.</p> </li> <li> <p>Row</p> <p>A horizontal data element in a table, representing a single instance of data.</p> </li> <li> <p>Primary Key</p> <p>A unique identifier for each row in a table, used to establish relationships between tables.</p> </li> <li> <p>Foreign Key</p> <p>A reference to a primary key in another table, used to establish relationships between tables.</p> </li> <li> <p>Index</p> <p>A data structure used to improve database performance by allowing for fast searching and retrieval of data.</p> </li> <li> <p>Query</p> <p>A request for data from a database, usually expressed in a database query language.</p> </li> <li> <p>Transaction</p> <p>A sequence of database operations that are treated as a single unit of work, either committed or rolled back.</p> </li> <li> <p>Stored Procedure</p> <p>A precompiled set of database operations that can be executed repeatedly.</p> </li> <li> <p>Trigger</p> <p>An event-driven procedure that is executed automatically when a specific database event occurs.</p> </li> <li> <p>View</p> <p>A virtual table that is derived from the data in one or more tables.</p> </li> <li> <p>Materialized View</p> <p>A pre-computed view of data, stored in a table for fast retrieval.</p> </li> <li> <p>Partition</p> <p>A division of a database table into smaller pieces for improved performance and scalability.</p> </li> <li> <p>Data Warehousing</p> <p>The practice of storing data in a centralized repository for fast and efficient analysis.</p> </li> <li> <p>NoSQL</p> <p>A type of database that does not use the relational model and is optimized for handling large amounts of unstructured data.</p> </li> <li> <p>Object-Relational Database</p> <p>A type of database that combines the features of relational databases and object-oriented databases.</p> </li> <li> <p>Cloud Database</p> <p>A database hosted on a remote server and accessed through the internet, typically provided as a service by a cloud computing provider.</p> </li> <li> <p>Scalability</p> <p>The ability of a database to handle increasing amounts of data and traffic without impacting performance.</p> </li> <li> <p>Availability</p> <p>The ability of a database to remain accessible to users at all times, even during maintenance or failures.</p> </li> <li> <p>Replication</p> <p>The process of copying data from one database to another, typically used for backup and disaster recovery purposes.</p> </li> <li> <p>Sharding</p> <p>The process of dividing a database into multiple smaller databases, typically to improve performance and scalability.</p> </li> <li> <p>Backup</p> <p>A copy of a database used for recovery purposes in case the original database is lost or damaged.</p> </li> <li> <p>Recovery</p> <p>The process of restoring a database to a previous state after a failure or disaster.</p> </li> <li> <p>Data Integrity</p> <p>The accuracy and consistency of data stored in a database, maintained through the use of constraints and transactions.</p> </li> <li> <p>Data Security</p> <p>The measures taken to protect sensitive data stored in a database from unauthorized access, modification, or theft.</p> </li> <li> <p>ACID Properties</p> <p>A set of properties that ensure the consistency and reliability of database transactions, including Atomicity, Consistency, Isolation, and Durability.</p> </li> <li> <p>Denormalization</p> <p>The process of adding redundant data to a database to improve performance, typically used in data warehousing and NoSQL databases.</p> </li> <li> <p>Query Optimization</p> <p>The process of improving the performance of database queries by reducing the amount of data that must be processed.</p> </li> <li> <p>Normalization</p> <p>The process of organizing data into separate tables to eliminate redundant data and improve data integrity.</p> </li> <li> <p>Data Modeling</p> <p>The process of creating a blueprint or representation of data and its relationships, used to design and build a database.</p> </li> <li> <p>ER Model</p> <p>An entity-relationship model, used in data modeling to represent the relationships between entities in a database.</p> </li> <li> <p>Data Types</p> <p>The classification of data into specific types, such as text, numbers, or dates, used to define the structure of a database.</p> </li> <li> <p>Concurrency Control</p> <p>The management of access to a database by multiple users, ensuring data consistency and avoiding conflicts.</p> </li> <li> <p>Locking</p> <p>The process of preventing other users from accessing data while it is being updated or modified.</p> </li> <li> <p>Transactions Log</p> <p>A record of all database transactions, used for recovery purposes in case of a failure.</p> </li> <li> <p>Encryption</p> <p>The process of converting data into a secure format that can only be decrypted by authorized users.</p> </li> <li> <p>Data Migration</p> <p>The process of moving data from one database to another, typically for the purpose of upgrading or changing systems.</p> </li> <li> <p>Performance Tuning</p> <p>The process of improving the performance of a database by optimizing its configuration, queries, and indexing.</p> </li> <li> <p>Data Dictionary</p> <p>A repository of metadata, including information about the structure, relationships, and constraints of data in a database.</p> </li> <li> <p>Indexing</p> <p>The process of creating a data structure that allows for faster and more efficient searches of data in a database.</p> </li> <li> <p>Foreign Key</p> <p>A field in a database table that refers to the primary key of another table, used to establish relationships between tables.</p> </li> <li> <p>Primary Key</p> <p>A unique identifier for each record in a database table, used to uniquely identify and access data.</p> </li> <li> <p>Normalization Forms</p> <p>The series of rules used to eliminate redundant data and improve data integrity, such as 1NF, 2NF, 3NF, and BCNF.</p> </li> <li> <p>Stored Procedures</p> <p>Pre-compiled sets of SQL statements that can be stored in a database and executed repeatedly.</p> </li> <li> <p>Triggers</p> <p>Automated actions that are triggered by specific events in a database, such as data changes or updates.</p> </li> <li> <p>Views</p> <p>A virtual table in a database that is created by querying data from one or more other tables.</p> </li> <li> <p>Query Language</p> <p>The language used to interact with and manipulate data in a database, such as SQL (Structured Query Language).</p> </li> <li> <p>Data Warehousing</p> <p>The process of storing and managing large amounts of data for analysis and reporting purposes.Data Mining: The process of discovering and extracting insights from large amounts of data.</p> </li> </ul>"},{"location":"articles/deployment-strategies/","title":"Deployment Strategies","text":"<p>Deployment strategies are the various approaches or techniques used to deploy software applications or updates to production environments.</p> <ul> <li>1. Category</li> <li>1.1. Blue/Green Deployment</li> <li>1.2. Red/Black Deployment</li> <li>1.3. Rolling Deployment</li> <li>1.4. Canary Deployment</li> <li>1.5. A/B Testing</li> <li>1.6. Shadow Deployment</li> <li>1.7. Immutable Deployment</li> <li>1.8. Feature Toggle Deployment</li> <li>1.9. Rollback Deployment</li> <li>1.10. Immutable Infrastructure Deployment</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/deployment-strategies/#1-category","title":"1. Category","text":""},{"location":"articles/deployment-strategies/#11-bluegreen-deployment","title":"1.1. Blue/Green Deployment","text":"<p>In Blue/Green deployment strategy, two identical production environments (blue and green) are maintained. The new version of the application is deployed to the green environment, and the traffic is gradually shifted from the blue environment to the green environment. This approach ensures that the new version of the application is thoroughly tested before being made available to users. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime.</p>"},{"location":"articles/deployment-strategies/#12-redblack-deployment","title":"1.2. Red/Black Deployment","text":"<p>In Red/Black deployment strategy, two identical production environments (red and black) are maintained, with the current production environment being the red environment. The new version of the application is deployed to the black environment, and once it is tested and deemed stable, the traffic is switched to the black environment, making it the new production environment. This approach ensures that the new version of the application is thoroughly tested before being made available to users. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime by enabling a smooth transition from the old to the new version.</p>"},{"location":"articles/deployment-strategies/#13-rolling-deployment","title":"1.3. Rolling Deployment","text":"<p>In rolling deployment strategy, the new version of the application is gradually deployed to a subset of servers in the production environment, and the process continues until all servers have been updated. This approach ensures that the application is continuously available to users and that any issues that arise can be detected and resolved quickly. The theoretical basis for this deployment strategy is that it minimizes the risk of downtime and ensures that the application remains available even during the deployment process.</p>"},{"location":"articles/deployment-strategies/#14-canary-deployment","title":"1.4. Canary Deployment","text":"<p>In Canary deployment strategy, the new version of the application is deployed to a small subset of users or servers to test its functionality and performance before gradually rolling it out to the entire production environment. This approach ensures that any issues with the new version are detected and resolved before it is made available to all users. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime.</p>"},{"location":"articles/deployment-strategies/#15-ab-testing","title":"1.5. A/B Testing","text":"<p>In A/B Testing deployment strategy, two different versions of the application are deployed simultaneously to different subsets of users. This approach allows for the comparison of the two versions, making it possible to determine which version performs better. The theoretical basis for this deployment strategy is that it ensures high availability and enables continuous improvement of the application.</p>"},{"location":"articles/deployment-strategies/#16-shadow-deployment","title":"1.6. Shadow Deployment","text":"<p>In Shadow deployment strategy, the new version of the application is deployed to a separate production environment, and a portion of the production traffic is routed to it. The results of the new deployment are then compared to the current production environment to ensure that the new version is performing as expected. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime.</p>"},{"location":"articles/deployment-strategies/#17-immutable-deployment","title":"1.7. Immutable Deployment","text":"<p>In Immutable deployment strategy, a new environment is created for each new version of the application instead of updating the existing environment. The new environment is created from a base image that includes the entire application stack. This approach helps to reduce the risk of issues arising from configuration changes or updates. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime by isolating the new version of the application from the existing environment.</p>"},{"location":"articles/deployment-strategies/#18-feature-toggle-deployment","title":"1.8. Feature Toggle Deployment","text":"<p>In Feature deployment strategy, new features are deployed to production but kept disabled until they are tested and deemed stable. This approach allows for the gradual rollout of new features while minimizing the risk of issues. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime by enabling developers to deploy new features without disrupting the production environment.</p>"},{"location":"articles/deployment-strategies/#19-rollback-deployment","title":"1.9. Rollback Deployment","text":"<p>In Rollback deployment strategy, the new version of the application is deployed to the production environment, but in the event of any issues or failures, the deployment can be rolled back to the previous version. This approach ensures that the application remains available to users, even in the event of issues or failures. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime by allowing for quick rollback to a previous version.</p>"},{"location":"articles/deployment-strategies/#110-immutable-infrastructure-deployment","title":"1.10. Immutable Infrastructure Deployment","text":"<p>In Immutable Infrastructure deployment strategy, a new infrastructure is created for each new version of the application using code, making it easy to manage and deploy. This approach ensures that the infrastructure is consistent across all environments and reduces the risk of issues arising from manual configuration changes. The theoretical basis for this deployment strategy is that it ensures high availability and reduces the risk of downtime by ensuring that the infrastructure is reliable and consistent.</p>"},{"location":"articles/deployment-strategies/#2-principle","title":"2. Principle","text":"<p>By adhering to these principles, deployment strategies can be designed and implemented to ensure high availability, reliability, and stability of the application while minimizing disruption and downtime for users.</p> <ul> <li> <p>Continuous Integration</p> <p>Continuous integration is the practice of frequently integrating code changes into a central repository and ensuring that automated tests are run to detect issues as early as possible. This principle ensures that issues are caught early in the development process, reducing the risk of problems occurring during deployment.</p> </li> <li> <p>Continuous Delivery</p> <p>Continuous delivery is the practice of automating the deployment process so that code changes can be quickly and safely released to production. This principle ensures that deployments are quick and efficient, minimizing downtime and disruption to users.</p> </li> <li> <p>Automation</p> <p>Automation is the practice of using tools and processes to automate the deployment process. This principle ensures that deployments are consistent, repeatable, and reliable, reducing the risk of human error.</p> </li> <li> <p>Rollback Capability</p> <p>Rollback capability is the ability to quickly and easily revert to a previous version of the application in the event of issues or failures. This principle ensures that the application remains available to users even in the event of issues or failures.</p> </li> <li> <p>Testing</p> <p>Testing is the practice of running automated tests to ensure that the application is functioning correctly and that new code changes do not introduce issues. This principle ensures that the application is stable and reliable before being released to production.</p> </li> <li> <p>Monitoring</p> <p>Monitoring is the practice of tracking the performance and health of the application in real-time. This principle ensures that issues can be quickly detected and addressed before they become serious problems.</p> </li> <li> <p>Incremental Changes</p> <p>Incremental changes is the practice of making small, incremental changes to the application rather than large, sweeping changes. This principle ensures that deployments are less disruptive and that issues are easier to isolate and troubleshoot.</p> </li> </ul>"},{"location":"articles/deployment-strategies/#3-best-practice","title":"3. Best Practice","text":"<p>By following these best practices, deployment strategies can be designed and implemented to ensure that deployments are consistent, reliable, and efficient, while minimizing disruption and downtime for users.</p> <ul> <li> <p>Use Version Control System</p> <p>Use a version control system to manage code changes and ensure that all changes are properly tracked and documented.</p> </li> <li> <p>Use Continuous Pipelines</p> <p>Use a continuous pipelines to manage the deployment process and ensure that all steps are properly coordinated and executed. Continuous integration (CI) and continuous deployment (CD) are two closely related practices that involve automating the testing, building, and deployment of code changes. By implementing CI/CD, you can ensure that code changes are tested and deployed quickly and reliably. Theoretical basis for this practice is that it ensures fast and efficient deployments with reduced risk of errors.</p> </li> <li> <p>Use Automated Deployment Tools</p> <p>Automation is one of the key principles of effective deployment strategies. By using automated deployment tools, you can ensure that deployments are consistent, repeatable, and reliable. These tools can also help to reduce the risk of human error and speed up the deployment process. Theoretical basis for this practice is that it ensures efficient, accurate and reliable deployments.</p> </li> <li> <p>Implement Rollback Capability</p> <p>Rollback capability is the ability to quickly and easily revert to a previous version of the application in the event of issues or failures. By implementing rollback capability, you can ensure that the application remains available to users even in the event of issues or failures. Theoretical basis for this practice is that it enables quick and easy recovery from failures.</p> </li> <li> <p>Use Feature Flags</p> <p>Feature flags are a mechanism for controlling the visibility of new features in the application. By using feature flags, you can gradually roll out new features to users, monitor their performance, and quickly revert to the previous version if necessary. Theoretical basis for this practice is that it enables gradual release of new features while minimizing risk.</p> </li> <li> <p>Use Zero-Downtime Deployment</p> <p>Use blue/green or canary deployments to ensure that changes are gradually rolled out to users, minimizing the risk of downtime or issues.</p> </li> <li> <p>Monitor Performance and Health</p> <p>Monitoring the performance and health of the application in real-time is critical for detecting issues and addressing them before they become serious problems. By using monitoring tools, you can track key performance indicators (KPIs) and respond quickly to issues as they arise. Theoretical basis for this practice is that it enables proactive detection and resolution of issues.</p> </li> <li> <p>Test the deployment process</p> <p>Test the deployment process itself to ensure that it is working as expected and that deployments are successful.</p> </li> <li> <p>Test Deployments in Staging Environment</p> <p>Before deploying changes to the production environment, it is recommended to test them in a staging environment that closely mimics the production environment. This allows you to identify any issues before deploying to production, reducing the risk of downtime or other issues. Theoretical basis for this practice is that it enables thorough testing of changes before they go live.</p> </li> <li> <p>Use Configuration Management</p> <p>Use configuration management to manage the configuration of the application and ensure that change</p> </li> </ul>"},{"location":"articles/deployment-strategies/#4-terminology","title":"4. Terminology","text":"<p>Understanding and using this terminology, developers and operations teams can more effectively communicate and implement deployment strategies.</p> <ul> <li> <p>Deployment Pipeline</p> <p>A series of steps that code changes go through in order to be deployed to production. This pipeline typically includes steps such as building, testing, and deploying the code changes.</p> </li> <li> <p>Rollback</p> <p>The ability to quickly and easily revert to a previous version of the application in the event of issues or failures.</p> </li> <li> <p>Canary Analysis</p> <p>A technique for evaluating the performance and stability of a new version of the application by gradually increasing the percentage of users who are redirected to the new version while monitoring performance metrics and error rates.</p> </li> <li> <p>Configuration Management</p> <p>The practice of using tools and processes to manage the configuration of the application and infrastructure. This includes settings such as environment variables, database connections, and server configurations.</p> </li> <li> <p>Environment</p> <p>A set of servers, services, and infrastructure components that are used to host and run the application. Examples include development, testing, staging, and production environments.</p> </li> <li> <p>Zero-Downtime Deployment</p> <p>A deployment strategy where updates are made to the application without any interruption to user traffic. This is typically achieved using strategies such as blue/green or canary deployments.</p> </li> </ul>"},{"location":"articles/deployment-strategies/#5-references","title":"5. References","text":"<p>Google deployment strategies article.</p>"},{"location":"articles/docs-as-code/","title":"Docs as Code","text":"<p>Docs as Code is a set of principles and practices that aims to treat documentation as code, and apply software development practices to documentation.</p> <p>Docs as Code treats documentation as a codebase, with the same level of rigor and discipline as software development. This involves using version control systems (such as Git), markup languages (such as Markdown), and static site generators (such as Jekyll) to create and manage documentation.</p> <ul> <li>1. Category</li> <li>1.1. Markup Languages<ul> <li>1.1.1. Markdown</li> <li>1.1.2. AsciiDoc</li> <li>1.1.3. reStructuredText</li> </ul> </li> <li>1.2. UML<ul> <li>1.2.1. Structural Diagrams</li> <li>1.2.2. Behavioral Diagrams</li> <li>1.2.3. Interaction Diagrams</li> <li>1.2.4. Tools</li> </ul> </li> <li>1.3. Static Site Generators</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/docs-as-code/#1-category","title":"1. Category","text":""},{"location":"articles/docs-as-code/#11-markup-languages","title":"1.1. Markup Languages","text":""},{"location":"articles/docs-as-code/#111-markdown","title":"1.1.1. Markdown","text":"<p>Markdown is a lightweight markup language that is widely used for creating documentation. It is easy to learn and read, and allows for easy formatting and structuring of text. Markdown files typically have the extension <code>.md</code>.</p> <p>Markdown syntax to format text and can be converted to HTML, PDF, or other formats.</p> <p>Example:</p> <pre><code># Heading 1\n\n## Heading 2\n\n### Heading 3\n\nThis is a paragraph of text.\n\n* This is a bullet point.\n* This is another bullet point.\n\n1. This is a numbered list.\n2. This is another item in the numbered list.\n</code></pre>"},{"location":"articles/docs-as-code/#112-asciidoc","title":"1.1.2. AsciiDoc","text":"<p>AsciiDoc is markup language that is designed for creating technical documentation. It is more powerful than Markdown and allows for more advanced formatting and structuring. AsciiDoc files typically have the extension <code>.adoc</code> or <code>.asciidoc</code>.</p> <p>AsciiDoc can be converted to HTML, PDF, EPUB, and other formats.</p> <p>Example:</p> <pre><code>= Heading 1\n\n== Heading 2\n\n=== Heading 3\n\nThis is a paragraph of text.\n\n* This is a bullet point.\n* This is another bullet point.\n\n1. This is a numbered list.\n2. This is another item in the numbered list.\n</code></pre>"},{"location":"articles/docs-as-code/#113-restructuredtext","title":"1.1.3. reStructuredText","text":"<p>reStructuredText is a markup language that is used for creating technical documentation. It is more powerful than Markdown and allows for more advanced formatting and structuring. reStructuredText files typically have the extension <code>.rst</code>.</p> <p>reStructuredText can be converted to HTML, LaTeX, and other formats. reStructuredText supports features, including tables, footnotes, and citations.</p> <p>Example:</p> <pre><code>Heading 1\n=========\n\nHeading 2\n---------\n\nHeading 3\n^^^^^^^^^\n\nThis is a paragraph of text.\n\n* This is a bullet point.\n* This is another bullet point.\n\n1. This is a numbered list.\n2. This is another item in the numbered list.\n</code></pre>"},{"location":"articles/docs-as-code/#12-uml","title":"1.2. UML","text":"<p>Unified Modeling Language (UML) is a graphical modeling language used to visualize, specify, and document software systems. It includes a variety of diagrams that can be used to model different aspects of a system.</p> <p>In the context of Docs as Code, UML can be used to model and document the relationships and interactions between different components of the documentation system, such as the documentation files, the version control system, the build tools, and the hosting platform. UML can also be used to model the workflows and processes involved in creating, reviewing, and publishing documentation.</p>"},{"location":"articles/docs-as-code/#121-structural-diagrams","title":"1.2.1. Structural Diagrams","text":"<p>Structural diagrams represent the static structure of a system, including its components, classes, and relationships.</p> <ul> <li>Class diagrams <p>Class diagrams show the structure of a system by modeling the classes and their relationships. They include attributes and methods associated with each class.</p> </li> </ul> <p>Example:</p> <pre><code>+---------------------+\n|     Employee        |\n+---------------------+\n| - id: int           |\n| - name: String      |\n| - email: String     |\n+---------------------+\n| + getDetails()      |\n| + setDetails()      |\n+---------------------+\n</code></pre> <ul> <li>Object diagrams <p>Object diagrams show the instances of classes and their relationships. They are useful for showing how a system works at a particular point in time.</p> </li> </ul> <p>Example:</p> <pre><code>+---------------------+\n|     Employee        |\n+---------------------+\n| - id: 1001          |\n| - name: \"John Doe\"  |\n+---------------------+\n</code></pre>"},{"location":"articles/docs-as-code/#122-behavioral-diagrams","title":"1.2.2. Behavioral Diagrams","text":"<p>Behavioral diagrams represent the dynamic behavior of a system, including its interactions, collaborations, and workflows. Some examples of behavioral diagrams include:</p> <ul> <li>Use case diagrams <p>Use case diagrams show the interactions between a system and its users. They include actors (users or other systems) and use cases (actions or tasks that the system performs).</p> </li> </ul> <p>Example:</p> <pre><code>+---------+       +---------+\n|  Actor  |       |  Actor  |\n+---------+       +---------+\n      |              |\n      |              |\n+---------------------------+\n|         Use Case          |\n+---------------------------+\n</code></pre> <ul> <li>Sequence diagrams <p>Sequence diagrams show the interactions between different objects or components in a system over time. They include messages exchanged between the objects and the order in which they occur.</p> </li> </ul> <p>Example:</p> <pre><code>Actor -&gt; System: Request\nSystem -&gt; Database: Query\nDatabase -&gt; System: Results\nSystem -&gt; Actor: Response\n</code></pre>"},{"location":"articles/docs-as-code/#123-interaction-diagrams","title":"1.2.3. Interaction Diagrams","text":"<p>Interaction diagrams are a specific type of behavioral diagram in UML, along with activity diagrams, state machine diagrams, and timing diagrams. Interaction diagrams focus on the interactions between objects or components in a system, while other types of behavioral diagrams focus on different aspects of system behavior.</p> <ul> <li>Sequence diagrams <p>Sequence diagrams show the interactions between objects or components over time.</p> </li> </ul> <p>Example:</p> <pre><code>Customer -&gt; Online Store: Browse products\nOnline Store -&gt; Database: Retrieve product list\nDatabase -&gt; Online Store: Send product list\nOnline Store -&gt; Customer: Display product list\nCustomer -&gt; Online Store: Select product\nOnline Store -&gt; Database: Retrieve product details\nDatabase -&gt; Online Store: Send product details\nOnline Store -&gt; Customer: Display product details\nCustomer -&gt; Online Store: Add product to cart\nOnline Store -&gt; Customer: Update cart\n</code></pre> <ul> <li>Communication diagrams <p>Communication diagrams show the interactions between objects or components in a system by modeling the messages exchanged between them.</p> </li> </ul> <p>Example:</p> <pre><code>+-------------------+        +------------------+\n|    Customer       |        |   Order          |\n+-------------------+        +------------------+\n         |                         |\n         | order placed            |\n         |------------------------&gt;|\n         |                         |\n         | payment requested       |\n         |&lt;------------------------|\n         |                         |\n         | payment received        |\n         |------------------------&gt;|\n         |                         |\n         | order fulfilled         |\n         |&lt;------------------------|\n         |                         |\n</code></pre>"},{"location":"articles/docs-as-code/#124-tools","title":"1.2.4. Tools","text":"<ul> <li> <p>PlantUML</p> <p>PlantUML is an open-source tool for creating UML diagrams using a simple text-based syntax. It supports a wide range of UML diagram types and can generate diagrams in various formats, including PNG, SVG, and LaTeX.</p> </li> <li> <p>Mermaid</p> <p>Mermaid is an open-source tool for creating diagrams using a simple text-based syntax. It supports a wide range of diagram types, including UML diagrams, flowcharts, and sequence diagrams. It offers a variety of styling options and can generate diagrams in various formats, including SVG and PNG. Mermaid is particularly useful for creating diagrams in environments that don't support graphical diagramming tools, such as Markdown files or README files on GitHub. The text-based syntax makes it easy to include diagrams in documentation and other text-based files. Mermaid is also highly customizable, allowing users to define their own styles and diagram types using JavaScript.</p> </li> </ul>"},{"location":"articles/docs-as-code/#13-static-site-generators","title":"1.3. Static Site Generators","text":"<p>Static Site Generators (SSGs) are tools that use markup languages to generate static websites. SSG allow teams to write documentation using markup languages and generate a static website that can be hosted on web servers.</p> <p>SSG work process:</p> <ul> <li> <p>Content Creation</p> <p>Create website's content using a markup language like Markdown or HTML. Organize the content into pages, blog posts, and other types of content.</p> </li> <li> <p>Template Creation</p> <p>Create a set of templates that define the structure and design of the website. Templates typically use a templating language like Handlebars or Liquid to define placeholders for the content.</p> </li> <li> <p>Building</p> <p>Run the SSG tool to <code>build</code> the website. The tool reads the content and templates and generates a set of static HTML, CSS, and JavaScript files. The files are typically organized into a directory structure that mirrors the structure of your content.</p> </li> <li> <p>Deployment</p> <p>Deploy the static files to a web server or a content delivery network (CDN) that can serve them to users.</p> </li> </ul>"},{"location":"articles/docs-as-code/#2-principle","title":"2. Principle","text":"<p>By applying these principles, Docs as Code aims to improve the quality, consistency, and efficiency of documentation, and integrate documentation more closely with the software development process.</p> <ul> <li> <p>Version Control</p> <p>Docs as Code advocates for storing documentation in a version control system (VCS), such as Git, to manage changes over time, enable collaboration, and provide a history of changes.</p> </li> <li> <p>Automation</p> <p>Automation is a key principle of Docs as Code, with the goal of reducing manual effort and ensuring consistency in documentation. Automation can include tools for generating documentation from source code or other data sources, running tests on documentation, and deploying documentation to various platforms.</p> </li> <li> <p>Collaboration</p> <p>Docs as Code encourages collaboration between developers, technical writers, and other stakeholders in the documentation process, using tools like code reviews, pull requests, and issue tracking to facilitate communication and feedback.</p> </li> <li> <p>Consistency</p> <p>Docs as Code aims to establish and maintain consistency in documentation, using style guides, branding guidelines, and other standards to ensure a unified voice and style across documentation.</p> </li> <li> <p>Continuous Improvement</p> <p>Docs as Code advocates for continuous improvement in documentation, using metrics and feedback to identify areas for improvement, and incorporating changes into the documentation process.</p> </li> </ul>"},{"location":"articles/docs-as-code/#3-best-practice","title":"3. Best Practice","text":"<p>By following these best practices, organizations can improve the quality, consistency, and efficiency of their documentation, and integrate it more closely with the software development process. Additionally, they can benefit from the scalability and maintainability of code-based documentation.</p> <ul> <li> <p>Use a structured format</p> <p>Using a structured format, such as Markdown or AsciiDoc, can improve consistency, readability, and maintainability of documentation. Structured formats provide a standard way of organizing information, making it easier for readers to find what they need and for authors to update and modify content.</p> </li> <li> <p>Write documentation as code</p> <p>Writing documentation as code, with the same level of care and attention as software code, can improve the quality and consistency of documentation. This includes following coding conventions, documenting code changes, and using code review processes to ensure high quality.</p> </li> <li> <p>Automate documentation generation</p> <p>Automating the generation of documentation from source code can reduce the manual effort involved in documentation and ensure that documentation is up-to-date with the latest changes in the codebase. This can be achieved using tools like Sphinx or Javadoc.</p> </li> <li> <p>Create a single source of truth</p> <p>Creating a single source of truth for documentation, such as a documentation repository, can reduce duplication and ensure consistency across documentation. This can be achieved by centralizing documentation in a single location and using version control to manage changes.</p> </li> <li> <p>Use templates and style guides</p> <p>Using templates and style guides can ensure consistency in formatting and style across documentation, making it easier for readers to navigate and understand. This can also reduce the effort involved in creating and maintaining documentation by providing a standardized framework.</p> </li> <li> <p>Collaborate with stakeholders</p> <p>Collaborating with stakeholders, such as developers, product managers, and users, can improve the quality and relevance of documentation. This can be achieved through feedback mechanisms, like surveys or user testing, and by involving stakeholders in the documentation process.</p> </li> </ul>"},{"location":"articles/docs-as-code/#4-terminology","title":"4. Terminology","text":"<p>By understanding these key terminology related to Docs as Code, teams can better implement Docs as Code practices and improve the quality, consistency, and efficiency of their documentation.</p> <ul> <li> <p>Modular documentation</p> <p>Modular documentation involves breaking documentation into smaller, reusable components, such as templates, snippets, and examples, to improve efficiency and consistency in documentation.</p> </li> <li> <p>Automation</p> <p>Automation involves using tools and processes to reduce manual effort in creating and maintaining documentation. Automation can include tools for generating documentation from source code, running tests on documentation, and deploying documentation to various platforms.</p> </li> <li> <p>Pull request</p> <p>A proposed change to a code or documentation repository, typically submitted for review by other team members.</p> </li> <li> <p>Readme file</p> <p>A file in a code repository that contains information about the project, including installation instructions, usage guidelines, and other information.</p> </li> <li> <p>API documentation</p> <p>Documentation that describes how to use an application programming interface (API), typically including information on endpoints, parameters, and response formats.</p> </li> <li> <p>Documentation pipeline</p> <p>A series of steps that documentation goes through, such as writing, editing, reviewing, and publishing.</p> </li> <li> <p>Documentation generator</p> <p>A tool that automatically generates documentation from source code or other data sources, such as Swagger or Javadoc.</p> </li> <li> <p>Style guide</p> <p>A set of standards and guidelines for writing documentation, including language usage, formatting, and tone.</p> </li> <li> <p>Single source of truth</p> <p>A principle that promotes having a single, authoritative source for information, to avoid confusion and inconsistencies.</p> </li> <li> <p>DocOps</p> <p>A term used to describe the integration of documentation into the software development process, similar to DevOps.</p> </li> <li> <p>Doc site</p> <p>A website or portal that hosts documentation, often including search functionality, navigation, and other features.</p> </li> <li> <p>Knowledge base</p> <p>A centralized repository of information, often including documentation, FAQs, and other resources.</p> </li> <li> <p>Content management system (CMS)</p> <p>A software platform for managing digital content, such as documentation, articles, or multimedia.</p> </li> </ul>"},{"location":"articles/docs-as-code/#5-references","title":"5. References","text":"<ul> <li>Sentenz\u00a0static site generators\u00a0article.</li> <li>Sentenz comment article.</li> </ul>"},{"location":"articles/dora/","title":"DORA","text":"<p>DORA (DevOps Research and Assessment) is an organization that conducts research and provides assessments related to DevOps practices in software development. They have developed the DORA metrics, which measure software delivery performance and help organizations improve their DevOps capabilities.</p> <ul> <li>1. Category</li> <li>1.1. Deployment Frequency</li> <li>1.2. Lead Time for Changes</li> <li>1.3. Time to Restore Services</li> <li>1.4. Change Failure Rate</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/dora/#1-category","title":"1. Category","text":"<p>The DORA (DevOps Research and Assessment) metrics are a set of key performance indicators (KPIs) developed by the DevOps Research and Assessment organization. These metrics are designed to assess the performance of software delivery teams and provide insights into the effectiveness of DevOps practices.</p> <p>DORA has identified four key categories that are essential for high-performing DevOps organizations. The categories are often referred to as the Four Key Metrics or the <code>DORA Metrics</code>.</p> <p>By measuring and improving the four key metrics, organizations can achieve faster and more reliable software delivery, which is a key goal of DevOps. The DORA metrics provide a framework for organizations to assess their DevOps practices and identify areas for improvement.</p>"},{"location":"articles/dora/#11-deployment-frequency","title":"1.1. Deployment Frequency","text":"<p>This category measures how often code changes are deployed to production. High-performing DevOps organizations typically deploy changes more frequently, often multiple times per day, using techniques like continuous integration and continuous deployment.</p> <p>Performance Metric:</p> <ul> <li> <p>Elite</p> <p>On-Demand (multiple deploys per day)</p> </li> <li> <p>High</p> <p>Between once per day and once per week</p> </li> <li> <p>Medium</p> <p>Between once per week and once per mounth</p> </li> <li> <p>Low</p> <p>Between once per mounth and once every six mounths</p> </li> </ul>"},{"location":"articles/dora/#12-lead-time-for-changes","title":"1.2. Lead Time for Changes","text":"<p>This category measures the time it takes for code changes to go from development to production. It includes all the processes and steps involved in delivering a software change, such as coding, testing, building, and deploying. Shortening delivery lead time is a key goal of DevOps, as it enables organizations to respond faster to changing customer needs and market demands.</p> <p>Performance Metric:</p> <ul> <li> <p>Elite</p> <p>Less than one day</p> </li> <li> <p>High</p> <p>Between one day and one week</p> </li> <li> <p>Medium</p> <p>Between one week and one month</p> </li> <li> <p>Low</p> <p>Between one month and six months</p> </li> </ul>"},{"location":"articles/dora/#13-time-to-restore-services","title":"1.3. Time to Restore Services","text":"<p>This category measures how quickly an organization can recover from a production incident or outage. High-performing DevOps organizations have a shorter Mean Time to Recovery (MTTR), indicating that they are able to detect and resolve incidents quickly and minimize the impact on customers.</p> <p>Performance Metric:</p> <ul> <li> <p>Elite</p> <p>Less than one hour</p> </li> <li> <p>High</p> <p>Less than one day</p> </li> <li> <p>Medium</p> <p>Between one day and one week</p> </li> <li> <p>Low</p> <p>Between one week and one month</p> </li> </ul>"},{"location":"articles/dora/#14-change-failure-rate","title":"1.4. Change Failure Rate","text":"<p>This category measures the percentage of code changes that fail in production, e.g. lead to service impairment or service outage. High-performing DevOps organizations have a lower change failure rate, indicating that they are more reliable and resilient.</p> <p>Performance Metric:</p> <ul> <li> <p>Elite</p> <p>0%-15%</p> </li> <li> <p>High</p> <p>15%-30%</p> </li> <li> <p>Medium</p> <p>30%-45%</p> </li> <li> <p>Low</p> <p>45%-100%</p> </li> </ul>"},{"location":"articles/dora/#2-principle","title":"2. Principle","text":"<p>DORA (DevOps Research and Assessment) has identified several principles that are essential for high-performing DevOps organizations. These principles are based on research conducted by DORA and are intended to guide organizations in adopting and implementing DevOps practices effectively.</p> <p>By following these principles, organizations can adopt and implement DevOps practices effectively and achieve faster, more reliable software delivery. The principles provide a framework for organizations to create a culture of collaboration, automate their software delivery processes, measure key metrics, share knowledge, and continuously improve their practices.</p> <ul> <li> <p>Culture</p> <p>DevOps is a culture of collaboration and trust between development and operations teams. Organizations that foster a culture of collaboration, shared responsibility, and continuous learning are more likely to be successful in adopting DevOps practices.</p> </li> <li> <p>Automation</p> <p>Automation is essential for achieving faster and more reliable software delivery. Organizations that automate their software delivery processes, including testing, building, and deployment, are able to achieve faster lead times and lower error rates.</p> </li> <li> <p>Measurement</p> <p>Measurement is critical for understanding the effectiveness of DevOps practices and identifying areas for improvement. Organizations that measure key metrics like delivery lead time, deployment frequency, change failure rate, and mean time to recovery (MTTR) are better able to optimize their software delivery processes.</p> </li> <li> <p>Sharing</p> <p>Sharing knowledge and information across teams is essential for DevOps success. Organizations that share information, such as metrics, code, and best practices, are able to improve collaboration and drive innovation.</p> </li> <li> <p>Continuous Improvement</p> <p>DevOps is a continuous improvement process. Organizations that continually assess and optimize their software delivery processes are more likely to achieve high performance and deliver value to their customers.</p> </li> </ul>"},{"location":"articles/dora/#3-best-practice","title":"3. Best Practice","text":"<p>DORA (DevOps Research and Assessment) has identified several best practices for achieving high-performing software delivery and operational performance through the adoption of DevOps practices. These best practices are based on research conducted by DORA and are intended to guide organizations in improving their DevOps practices.</p> <p>By implementing these best practices, organizations can improve their DevOps practices and achieve faster, more reliable software delivery and operational performance. The best practices provide a framework for organizations to optimize their software delivery processes and foster a culture of collaboration, learning, and continuous improvement.</p> <ul> <li> <p>Test Automation</p> <p>Test automation is a key practice in DevOps that can help organizations achieve faster and more reliable software delivery. Automated testing can help catch errors and issues earlier in the development cycle, enabling teams to make changes more quickly and with greater confidence.</p> </li> <li> <p>Continuous Testing</p> <p>Continuous testing is a practice that involves testing throughout the software development cycle, from development to production. This includes unit testing, integration testing, and other types of testing that can help catch errors and issues earlier in the development cycle.</p> </li> <li> <p>Deployment Automation</p> <p>Deployment automation is a key practice in DevOps that involves automating the deployment process, from code check-in to production. This can help organizations achieve faster and more reliable software delivery.</p> </li> <li> <p>Loosely Coupled Architecture</p> <p>A loosely coupled architecture is a design approach that emphasizes modular, independent components that can be easily integrated and updated. This approach can help organizations achieve greater agility and flexibility in their software delivery process.</p> </li> <li> <p>Empowered Teams</p> <p>Empowering teams is a key principle in DevOps that emphasizes collaboration, shared responsibility, and continuous learning. Organizations that foster a culture of empowerment are more likely to be successful in adopting DevOps practices.</p> </li> <li> <p>Test Data Management</p> <p>Test data management is a practice that involves managing the data used in testing to ensure that it is accurate, relevant, and up-to-date. This can help organizations improve the accuracy and reliability of their testing processes.</p> </li> <li> <p>Monitoring and Observability</p> <p>Monitoring and observability are practices that involve tracking the performance and behavior of software systems in production. This can help organizations identify and address issues more quickly and effectively.</p> </li> <li> <p>Proactive Notifications</p> <p>Proactive notifications are a practice that involves alerting teams to potential issues or errors before they become critical. This can help organizations address issues more quickly and prevent downtime or other negative impacts.</p> </li> <li> <p>Database Change Management</p> <p>Database change management is a practice that involves managing changes to databases in a controlled and consistent manner. This can help organizations ensure the reliability and integrity of their databases.</p> </li> <li> <p>Code Maintainability</p> <p>Code maintainability is a key factor in achieving faster and more reliable software delivery. Well-maintained code is easier to update and less prone to errors, enabling organizations to make changes more quickly and with greater confidence.</p> </li> <li> <p>Learning Culture</p> <p>Fostering a culture of continuous learning, where teams are encouraged to learn new skills and share knowledge with each other, can help organizations drive innovation and improve performance.</p> </li> <li> <p>Version Control</p> <p>Using version control for all code changes can help organizations track changes and collaborate more effectively.</p> </li> <li> <p>Test Early and Often</p> <p>Testing early and often, including unit testing, integration testing, and automated testing, can help organizations catch errors and issues earlier in the development cycle.</p> </li> <li> <p>Shift Left on Security</p> <p>Introducing security testing earlier in the development cycle, or shifting left on security, can help organizations identify and address security issues earlier and more effectively.</p> </li> <li> <p>ChatOps</p> <p>Implementing ChatOps, which involves using chat tools to facilitate collaboration and communication between teams, can help organizations improve communication and collaboration.</p> </li> <li> <p>Trunk-Based Development</p> <p>Using trunk-based development, where all changes are committed to a single code repository, can help organizations improve collaboration and reduce code conflicts.</p> </li> <li> <p>Continuous Delivery</p> <p>Implementing continuous delivery practices, including continuous integration, continuous delivery, and continuous deployment, can help organizations achieve faster and more reliable software delivery.</p> </li> </ul>"},{"location":"articles/dora/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>DevOps</p> <p>A collaborative approach to software development and delivery that emphasizes communication, collaboration, and automation between development and operations teams.</p> </li> <li> <p>Four Key Metrics</p> <p>The four key metrics identified by DORA to measure software delivery and operational performance delivery lead time, deployment frequency, change failure rate, and mean time to recovery (MTTR).</p> </li> <li> <p>High-performing DevOps organizations</p> <p>Organizations that achieve faster and more reliable software delivery and operational performance through the adoption of DevOps practices.</p> </li> <li> <p>Continuous Integration (CI)</p> <p>A practice where developers frequently integrate their code changes into a shared repository, allowing for automated testing and early error detection.</p> </li> <li> <p>Continuous Delivery (CD)</p> <p>A practice where code changes are automatically built, tested, and deployed to production using a continuous integration process.</p> </li> <li> <p>Continuous Deployment</p> <p>A practice where code changes are automatically deployed to production, without the need for manual approval.</p> </li> <li> <p>Trunk-Based Development</p> <p>A development practice where all changes are committed to a single code repository or \"trunk\", which allows for easier collaboration and reduces code conflicts.</p> </li> <li> <p>ChatOps</p> <p>A practice where chat tools are used to facilitate communication and collaboration between teams, and to automate certain tasks and processes.</p> </li> <li> <p>Shift Left</p> <p>The practice of introducing activities like testing, security, and performance analysis earlier in the software development lifecycle, in order to catch issues earlier and improve software quality.</p> </li> <li> <p>Value Stream Mapping</p> <p>A technique for visualizing and analyzing the flow of work and value through an organization's software delivery process, in order to identify opportunities for improvement.</p> </li> </ul>"},{"location":"articles/dora/#5-references","title":"5. References","text":"<ul> <li>Google devops article.</li> <li>Google dora article.</li> <li>GitHub dora repository.</li> </ul>"},{"location":"articles/everything-as-code/","title":"Everything as Code","text":"<p>Everything as Code (XaC) is a software development philosophy that treats infrastructure as code.</p> <ul> <li>1. Category</li> <li>1.1. Infrastructure-as-Code (IaC)</li> <li>1.2. Configuration-as-Code (CaC)</li> <li>1.3. Documentation-as-Code (DaC)</li> <li>1.4. Security-as-Code (SaC)</li> <li>1.5. Compliance-as-Code (CoC)</li> <li>1.6. Database-as-Code (DaC)</li> <li>1.7. Test-as-Code (TaC)</li> <li>1.8. Policy-as-Code (PaC)</li> <li>1.9. Logging-as-Code (LaC)</li> <li>1.10. Monitoring-as-Code (MaC)</li> <li>1.11. Network-as-Code (NaC)</li> <li>1.12. Detection as code (DaC)</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/everything-as-code/#1-category","title":"1. Category","text":"<p><code>as-Code</code> refer to different domains or aspects of software development and infrastructure management where practices have been developed to represent configurations, settings, or definitions as code.</p> <p><code>as-code</code> aim to enhance the automation, standardization, and consistency of various aspects of software development, deployment, and infrastructure management, leading to more reliable and scalable systems. As technology and practices continue to evolve, new <code>as-code</code> categories may emerge in the future.</p>"},{"location":"articles/everything-as-code/#11-infrastructure-as-code-iac","title":"1.1. Infrastructure-as-Code (IaC)","text":"<p>Infrastructure-as-Code (IaC) involves managing and provisioning infrastructure resources (e.g. virtual machines, networks, storage) through code, rather than using manual processes to configure devices or systems.</p> <p>Tools:</p> <p>Popular tools for managing infrastructure as code include <code>Terraform</code>, <code>AWS CloudFormation</code>, <code>Azure Resource Manager</code>, and <code>Google Cloud Deployment Manager</code>.</p> <p>Example:</p> <p>Using Terraform to create an AWS EC2 instance:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0c94855ba95c71c99\"\n  instance_type = \"t2.micro\"\n}\n</code></pre>"},{"location":"articles/everything-as-code/#12-configuration-as-code-cac","title":"1.2. Configuration-as-Code (CaC)","text":"<p>In Configuration-as-Code (CaC) application and system configurations are represented as code, treating application config resources as versioned artifacts to manage and deploy consistent configurations across different environments.</p> <p>NOTE See configuration management for details.</p> <p>Tools:</p> <p>Popular tools for managing configuration as code include <code>Ansible</code>, <code>Puppet</code>, <code>Chef</code>, <code>SaltStack</code>, <code>Kubernetes ConfigMaps and Helm</code>.</p> <p>Example:</p> <p>Using <code>Ansible</code> to install and configure Nginx on a server:</p> <pre><code>---\n- name: Install and start nginx\n  hosts: webservers\n  tasks:\n    - name: Install nginx\n      apt:\n        name: nginx\n        state: present\n    - name: Start nginx\n      service:\n        name: nginx\n        state: started\n</code></pre>"},{"location":"articles/everything-as-code/#13-documentation-as-code-dac","title":"1.3. Documentation-as-Code (DaC)","text":"<p>Documentation-as-Code (DaC) involves writing documentation as code, allowing teams to manage documentation in version-controlled repositories and automate documentation generation.</p> <p>NOTE See docs as code for details.</p> <p>Tools:</p> <p>popular tools for managing documentation as code include <code>Markdown</code>, <code>Sphinx</code>, <code>MkDocs</code>, <code>Asciidoctor</code>, <code>Jekyll</code>, and <code>Docusaurus</code>.</p> <p>Example:</p> <p>Using <code>Markdown</code> to write documentation as code:</p> <pre><code># My Project\n\nThis is the documentation for my project.\n\n## Getting Started\n\nTo get started with my project, follow these steps:\n\n1. Install the dependencies by running `npm install`.\n2. Start the development server by running `npm start`.\n\n## Usage\n\nTo use my project, do the following:\n\n1. Click on the \"New\" button to create a new item.\n2. Enter the details for the item and click \"Save\".\n</code></pre>"},{"location":"articles/everything-as-code/#14-security-as-code-sac","title":"1.4. Security-as-Code (SaC)","text":"<p>Security-as-Code (SaC) involves expressing security policies, controls, and configurations as code, enabling security teams to manage and enforce security measures programmatically.</p> <p>Tools:</p> <p>Popular tools for managing security as code include <code>Open Policy Agent (OPA)</code>, <code>HashiCorp Sentinel</code>, and <code>AWS Config</code>.</p> <p>Example:</p> <p>Using <code>Open Policy Agent (OPA)</code> to enforce a security policy that only allows traffic from certain IP addresses:</p> <pre><code>package httpapi.authz\n\ndefault allow = false\n\nallow {\n    input.method == \"GET\"\n    input.path = [\"salary\", employee_id]\n    input.headers[\"X-Forwarded-For\"] == \"192.0.2.146\"\n}\n</code></pre>"},{"location":"articles/everything-as-code/#15-compliance-as-code-coc","title":"1.5. Compliance-as-Code (CoC)","text":"<p>Compliance-as-Code (CoC) refers to the presentation of compliance requirements to embed the core activities of compliance: prevent, detect, remediate.</p> <p>Tools:</p> <p>Popular tools for managing compliance as code include <code>InSpec</code>, <code>Chef Compliance</code>, and <code>AWS Config Rules</code>.</p> <p>Example:</p> <p>Using <code>InSpec</code> to check if a server is compliant with a certain security policy:</p> <pre><code>control 'ssh-1' do\n  impact 1.0\n  title 'Server: Configure sshd_config'\n  desc 'Set sshd_config options for secure access'\n  describe sshd_config do\n    its('PermitRootLogin') { should eq 'no' }\n    its('PasswordAuthentication') { should eq 'no' }\n    its('ChallengeResponseAuthentication') { should eq 'no' }\n    its('KbdInteractiveAuthentication') { should eq 'no' }\n  end\nend\n</code></pre>"},{"location":"articles/everything-as-code/#16-database-as-code-dac","title":"1.6. Database-as-Code (DaC)","text":"<p>Database-as-Code (DaC) involves representing database schema, configurations, and migrations as code.</p> <p>Tools:</p> <p>Popular tools for managing databases as code include <code>Liquibase</code>, <code>Flyway</code>, and <code>Sqitch</code>.</p> <p>Example:</p> <p>Using <code>Liquibase</code> to manage database schema changes:</p> <pre><code>&lt;changeSet id=\"create_table_person\" author=\"liquibase-docs\"&gt;\n    &lt;createTable tableName=\"person\"&gt;\n        &lt;column name=\"id\" type=\"int\"&gt;\n            &lt;constraints primaryKey=\"true\" nullable=\"false\"/&gt;\n        &lt;/column&gt;\n        &lt;column name=\"firstname\" type=\"varchar(50)\"/&gt;\n        &lt;column name=\"lastname\" type=\"varchar(50)\"&gt;\n            &lt;constraints nullable=\"false\"/&gt;\n        &lt;/column&gt;\n    &lt;/createTable&gt;\n&lt;/changeSet&gt;\n</code></pre>"},{"location":"articles/everything-as-code/#17-test-as-code-tac","title":"1.7. Test-as-Code (TaC)","text":"<p>Test-as-Code (TaC) involves writing automated tests and test scenarios as code to ensure software quality and enable continuous testing in CI/CD pipelines.</p> <p>NOTE See testing patterns for details.</p> <p>Tools:</p> <p>Popular tools for writing tests as code include testing frameworks such as <code>Selenium</code>, <code>GTest</code> (for C/C++), <code>JUnit</code> (for Java), <code>PyTest</code> (for Python), and <code>RSpec</code> (for Ruby).</p> <p>Example:</p> <p>Using <code>PyTest</code> to write a unit test for a Python function:</p> <pre><code>def add(x, y):\n    return x + y\n\ndef test_add():\n    assert add(1, 2) == 3\n</code></pre>"},{"location":"articles/everything-as-code/#18-policy-as-code-pac","title":"1.8. Policy-as-Code (PaC)","text":"<p>Policy-as-Code (PaC) refers to expressing business policies, guidelines, and rules as code, allowing for consistency and centrally manage policies, and automation in policy enforcement. It is often related to governance and compliance.</p> <p>Tools:</p> <p>Popular tools for managing policies as code include <code>Open Policy Agent (OPA)</code>,  <code>Rego</code> (from Open Policy Agent), <code>HashiCorp Sentinel</code>, and <code>AWS Organizations Service Control Policies</code>.</p> <p>Example:</p> <p>Using <code>Sentinel</code> to enforce a policy that restricts the creation of AWS EC2 instances to certain regions:</p> <pre><code>import \"tfplan\"\n\nmain = rule {\n    all tfplan.resources.aws_instance as _, instances {\n        all instances as _, r {\n            r.applied.change.after.region in [\"us-west-2\", \"us-east-1\"]\n        }\n    }\n}\n</code></pre>"},{"location":"articles/everything-as-code/#19-logging-as-code-lac","title":"1.9. Logging-as-Code (LaC)","text":"<p>Logging-as-Code (LaC) involves defining logging configurations, log formats, and log storage settings as code, making it easier to manage and maintain log systems across various components.</p> <p>Tools:</p> <p>Popular tools for managing logging as code include logging frameworks such as <code>Logback</code> (for Java), Python\u2019s built-in logging module, <code>log4net</code> (for .NET), and <code>Elasticsearch</code>, <code>Logstash</code> and <code>Kibana</code> (ELK Stack).</p> <p>Example:</p> <p>Using <code>Logback</code> to configure logging for a Java application:</p> <pre><code>&lt;configuration&gt;\n  &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n    &lt;encoder&gt;\n      &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;\n    &lt;/encoder&gt;\n  &lt;/appender&gt;\n\n  &lt;root level=\"debug\"&gt;\n    &lt;appender-ref ref=\"STDOUT\" /&gt;\n  &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"articles/everything-as-code/#110-monitoring-as-code-mac","title":"1.10. Monitoring-as-Code (MaC)","text":"<p>Monitoring-as-Code (MaC) focuses on representing monitoring and observability settings, alerts, and metric configurations as code, allowing teams to centrally manage and automate their monitoring practices.</p> <p>Tools:</p> <p>Popular tools for managing monitoring as code include monitoring solutions such as <code>Prometheus</code> and <code>Grafana</code>, <code>Datadog</code>, and <code>New Relic</code>.</p> <p>Example:</p> <p>Using <code>Prometheus</code> to configure monitoring for a server:</p> <pre><code>global:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'node'\n    static_configs:\n      - targets: ['localhost:9100']\n</code></pre>"},{"location":"articles/everything-as-code/#111-network-as-code-nac","title":"1.11. Network-as-Code (NaC)","text":"<p>Network-as-Code (NaC) involves defining and managing network infrastructure as code, allowing teams to deploy and manage network resources appling software engineering practices.</p> <p>Tools:</p> <p>Popular tools for managing network infrastructure as code include network automation frameworks such as <code>NAPALM</code>, <code>Netmiko</code>, and <code>Ansible</code>.</p> <p>Example:</p> <p>Using <code>NAPALM</code> to configure a network device:</p> <pre><code>from napalm import get_network_driver\n\ndriver = get_network_driver('ios')\ndevice = driver('192.0.2.1', 'admin', 'password')\ndevice.open()\n\nconfig = [\n    'hostname myrouter',\n    'interface Ethernet1',\n    'description Uplink to ISP',\n    'ip address 203.0.113.1 255.255.255.252'\n]\n\ndevice.load_merge_candidate(config=config)\ndevice.commit_config()\ndevice.close()\n</code></pre>"},{"location":"articles/everything-as-code/#112-detection-as-code-dac","title":"1.12. Detection as code (DaC)","text":"<p>Detection as code (DaC) is a security paradigm that treats threat detection as code. Detection rules are written in a structured, machine-readable format, allowing teams to apply automated threat detection and response management.</p> <p>Tools:</p> <p>Popular tools for managing detection rules as code include threat detection solutions such as <code>Sigma</code>, <code>Snort</code>, and <code>Splunk</code>.</p> <p>Examples:</p> <p>Using <code>Sigma</code> to define a detection rule for a specific type of attack:</p> <pre><code>title: Suspicious Process Creation\nstatus: experimental\ndescription: Detects suspicious process creation\nauthor: Florian Roth\nlogsource:\n    product: windows\n    service: sysmon\ndetection:\n    selection:\n        EventID: 1\n        CommandLine|contains|all:\n            - '-n'\n            - '-e'\n            - 'cmd'\n    condition: selection\nfields:\n    - CommandLine\n    - ParentCommandLine\nfalsepositives:\n    - Unknown\nlevel: high\n</code></pre>"},{"location":"articles/everything-as-code/#2-principles","title":"2. Principles","text":"<p>Everything as Code (XaC) is a concept that extends the idea of Infrastructure as Code (IaC) to include various aspects of software development, deployment, and operations represented as code. While there is no standardized set of principles specifically labeled as XaC principles, the concept aligns with the principles of IaC and the general principles of software development and DevOps practices.</p> <p>NOTE XaC is a flexible concept, and its principles may be adapted and expanded based on the specific needs and goals of an organization. By adopting XaC principles, teams can foster a culture of automation, collaboration, and efficiency in software development, operations, and infrastructure management.</p> <ul> <li> <p>Automation</p> <p>Emphasize automation of processes wherever possible. Representing various aspects of software development, deployment, and operations as code allows for automation, reducing manual tasks and human error.</p> </li> <li> <p>Version Control</p> <p>Apply version control to code representations of different components. This enables tracking changes, collaboration among team members, and the ability to roll back to previous states if needed.</p> </li> <li> <p>Reproducibility</p> <p>Ensure that code-based representations are reproducible across different environments and stages of the development lifecycle. The same code should produce consistent results in different settings.</p> </li> <li> <p>Collaboration</p> <p>Facilitate collaboration among teams, including developers, operations, and security, by using code repositories as a single source of truth for configurations and settings.</p> </li> <li> <p>Scalability</p> <p>Design code representations to be scalable and easily adaptable to varying workloads and infrastructure requirements.</p> </li> <li> <p>Standardization</p> <p>Promote standardization and consistency in configurations and settings across different components, environments, and teams.</p> </li> <li> <p>Continuous Integration and Continuous Deployment (CI/CD)</p> <p>Integrate Everything as Code practices into CI/CD pipelines to automate the testing, deployment, and management of various software and infrastructure components.</p> </li> <li> <p>Security by Design</p> <p>Implement security measures and best practices as part of the code-based configurations to ensure security is integrated from the beginning.</p> </li> <li> <p>Immutable Infrastructure</p> <p>Treat infrastructure as immutable by updating and redeploying code-based configurations rather than making changes directly on live systems.</p> </li> <li> <p>Documentation</p> <p>Maintain comprehensive and up-to-date documentation alongside code representations to facilitate understanding and maintainability.</p> </li> <li> <p>Testing and Validation</p> <p>Implement automated testing and validation of code-based configurations to ensure correctness and compliance with desired outcomes.</p> </li> <li> <p>Continuous Improvement</p> <p>Embrace continuous improvement by regularly reviewing and refining code-based representations based on feedback and lessons learned.</p> </li> <li> <p>Auditability and Compliance</p> <p>Use code-based configurations to enhance auditability and compliance tracking, ensuring that systems meet industry regulations and internal policies.</p> </li> </ul>"},{"location":"articles/everything-as-code/#3-best-practice","title":"3. Best Practice","text":"<p>Implementing Everything as Code (XaC) involves applying code-based practices to various aspects of software development, deployment, and operations.</p> <p>By following best practices, organizations can benefit from the advantages of XaC, including automation, reproducibility, scalability, security, and collaboration, leading to more efficient, reliable, and maintainable software and infrastructure systems.</p> <ul> <li> <p>Infrastructure as Code (IaC)</p> <p>Adopt Infrastructure as Code for provisioning and managing infrastructure resources, such as virtual machines, networks, storage, and security groups. Use tools like Terraform or AWS CloudFormation to define and manage infrastructure in code.</p> </li> <li> <p>Configuration as Code (CaC)</p> <p>Express application configurations and settings as code. Utilize tools like Kubernetes ConfigMaps, Helm Charts, or configuration files in version-controlled repositories to manage application settings.</p> </li> <li> <p>Version Control</p> <p>Apply version control to all code representations, including infrastructure code, application configurations, and scripts. Use Git or other version control systems to track changes, collaborate, and roll back when necessary.</p> </li> <li> <p>Automated Testing</p> <p>Implement automated testing for code representations to validate their correctness and ensure that code changes do not introduce issues or regressions.</p> </li> <li> <p>Continuous Integration and Continuous Deployment (CI/CD)</p> <p>Integrate XaC practices into CI/CD pipelines to automate testing, deployment, and management processes. Automate the promotion of code representations through various environments.</p> </li> <li> <p>Immutable Infrastructure</p> <p>Treat infrastructure as immutable by avoiding manual changes on live systems. Instead, recreate and redeploy infrastructure components using code-based configurations.</p> </li> <li> <p>Security as Code (SaC)</p> <p>Incorporate security best practices into code-based representations. Use tools like Open Policy Agent (OPA) or security frameworks to enforce security policies as code.</p> </li> <li> <p>Documentation</p> <p>Maintain comprehensive and up-to-date documentation alongside code representations to facilitate understanding and collaboration among team members.</p> </li> <li> <p>Standardization and Consistency</p> <p>Establish coding standards and guidelines for XaC practices to ensure consistency across projects and teams.</p> </li> <li> <p>Compliance as Code (CoC)</p> <p>Integrate compliance checks and validation into XaC processes to ensure that systems meet industry regulations and internal policies.</p> </li> <li> <p>Collaboration</p> <p>Foster collaboration among teams, including developers, operations, security, and other stakeholders, to jointly manage and improve code representations.</p> </li> <li> <p>Continuous Improvement</p> <p>Continuously review and refine code-based representations based on feedback and lessons learned. Embrace a culture of continuous improvement in XaC practices.</p> </li> <li> <p>Monitor and Audit</p> <p>Implement monitoring and auditing capabilities to track changes, monitor system behavior, and ensure compliance with XaC practices.</p> </li> <li> <p>Infrastructure and Application as Code (IaaC)</p> <p>Combine Infrastructure as Code and Application as Code to holistically manage both infrastructure and application components using code representations.</p> </li> <li> <p>Adapt to Changes</p> <p>Stay flexible and adaptive to changes in the software development lifecycle and infrastructure requirements. XaC practices should support agility and rapid iteration.</p> </li> </ul>"},{"location":"articles/everything-as-code/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Infrastructure as Code (IaC)</p> <p>Refers to representing infrastructure resources and configurations as code, allowing for automated provisioning and management of infrastructure.</p> </li> <li> <p>Configuration as Code (CaC)</p> <p>Involves expressing application and system configurations as code, enabling version-controlled and automated configuration management.</p> </li> <li> <p>Security as Code (SaC)</p> <p>Focuses on representing security policies, controls, and configurations as code, enabling programmable security measures.</p> </li> <li> <p>Compliance as Code (CoC)</p> <p>Involves expressing compliance requirements and checks as code, allowing for automated compliance validation.</p> </li> <li> <p>Test as Code (TaC)</p> <p>Refers to writing automated tests and test scenarios as code to ensure software quality and continuous testing.</p> </li> <li> <p>Policy as Code (PaC)</p> <p>Involves expressing business policies, guidelines, and rules as code, enabling automated policy enforcement.</p> </li> <li> <p>Logging as Code (LaC)</p> <p>Represents logging configurations, log formats, and log storage settings as code, facilitating programmable log management.</p> </li> <li> <p>Monitoring as Code (MaC)</p> <p>Focuses on representing monitoring and observability settings, alerts, and metric configurations as code.</p> </li> <li> <p>Database as Code (DaC)</p> <p>Involves representing database schema, configurations, and migrations as code.</p> </li> <li> <p>Network as Code (NaC)</p> <p>Refers to defining network configurations and settings using code, enabling automated network management.</p> </li> <li> <p>Everything as Code (XaC)</p> <p>The overarching concept that encompasses all the <code>as code</code> practices, promoting the idea of representing various aspects of software and infrastructure as code.</p> </li> </ul>"},{"location":"articles/feature-flags/","title":"Feature Flags","text":"<p>Feature Flags, also known as Feature Flags, are a software development technique that enables developers to turn on and off certain features or functionality of an application or service without deploying new code.</p> <p>In essence, a feature toggle is a conditional statement that checks whether a particular feature is enabled or disabled, and determines the behavior of the application accordingly.</p> <ul> <li>1. Category</li> <li>1.1. Release Toggles</li> <li>1.2. Experiment Toggles</li> <li>1.3. Operations Toggles</li> <li>1.4. Permission Toggles</li> <li>1.5. Configuration Toggles</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/feature-flags/#1-category","title":"1. Category","text":"<p>Feature flags can be implemented in a number of different ways, including using conditional statements in code, using configuration files, or using external services such as LaunchDarkly or Split. However, it's important to use feature flags judiciously, as they can add complexity to code and make it harder to maintain over time. It's also important to have a clear strategy for managing feature flags, including documenting them, testing them thoroughly, and removing them once they are no longer needed.</p>"},{"location":"articles/feature-flags/#11-release-toggles","title":"1.1. Release Toggles","text":"<p>Release toggles control whether a feature is released to users or not. They are typically used to control the rollout of new features and to perform canary releases.</p> <ul> <li> <p>Early access toggle</p> <p>This toggle allows a small group of users to access a new feature before it's released to everyone else. It's often used to gather feedback and ensure that the feature works as intended before releasing it to a wider audience.</p> </li> <li> <p>Gradual release toggle</p> <p>This toggle enables a new feature to be gradually rolled out to a subset of users. This is useful for testing and monitoring the performance of the feature before releasing it to everyone.</p> </li> <li> <p>Rollback toggle</p> <p>This toggle is used to quickly disable a feature that has been released if it's causing issues or isn't working as intended. It can be a useful tool in ensuring that the application remains stable and reliable.</p> </li> </ul> <p>Example:</p> <pre><code>if (featureToggle.isEnabled(\"newFeature\")) {\n    // New feature code here\n} else {\n    // Old feature code here\n}\n</code></pre>"},{"location":"articles/feature-flags/#12-experiment-toggles","title":"1.2. Experiment Toggles","text":"<p>Experiment toggles control whether a feature is visible to users or not. They are typically used for A/B testing and other experimentation scenarios.</p> <ul> <li> <p>A/B testing toggle</p> <p>This toggle allows developers to test different versions of a feature to see which one performs better. Users are randomly assigned to either the control group (which doesn't see the new feature) or the experimental group (which does see the new feature).</p> </li> <li> <p>Personalization toggle</p> <p>This toggle enables different versions of a feature to be shown to different users based on their demographics or behavior. This is useful for tailoring the user experience to different segments of the audience.</p> </li> <li> <p>Feature subset toggle</p> <p>This toggle enables a feature to be released in different stages or variations to different groups of users. For example, some users might see a simplified version of the feature while others see a more advanced version.</p> </li> </ul> <p>Example:</p> <pre><code>if (featureToggle.isEnabled(\"newFeature\", userId)) {\n    // New feature code here\n} else {\n    // Old feature code here\n}\n</code></pre>"},{"location":"articles/feature-flags/#13-operations-toggles","title":"1.3. Operations Toggles","text":"<p>Operations toggles control the behavior of an application or service in different environments, such as development, testing, and production. They are typically used to enable or disable specific functionality in response to changes in the environment.</p> <ul> <li> <p>Maintenance toggle</p> <p>This toggle enables the application to be put into maintenance mode, which can be useful for performing upgrades or fixing critical bugs.</p> </li> <li> <p>Feature flag toggle</p> <p>This toggle enables the feature flags themselves to be turned on or off. This can be useful for troubleshooting or disabling features that are causing issues.</p> </li> <li> <p>Operational toggle</p> <p>This toggle enables developers to turn on or off specific functionality in response to changes in the environment, such as an increase in traffic or a change in user behavior. For example, a toggle might be used to disable certain features if the application is experiencing high load.</p> </li> </ul> <p>Example:</p> <pre><code>if (environment.equals(\"production\") &amp;&amp; featureToggle.isEnabled(\"newFeature\")) {\n    // Production code here\n} else {\n    // Development/testing code here\n}\n</code></pre>"},{"location":"articles/feature-flags/#14-permission-toggles","title":"1.4. Permission Toggles","text":"<p>Permission toggles control access to specific features or functionality based on user roles, permissions or to a particular user or group of users. They are typically used to enable or disable certain features based on the user's role or permissions.</p> <ul> <li> <p>Admin toggle</p> <p>This toggle enables specific features or functionality to be available only to users with administrative privileges. For example, an admin toggle might be used to give admins access to a dashboard that regular users don't see.</p> </li> <li> <p>User role toggle</p> <p>This toggle enables specific features or functionality to be available only to users with a particular role or permission level. For example, a user role toggle might be used to give premium users access to additional features that free users don't have.</p> </li> </ul> <p>Example:</p> <pre><code>if (currentUser.isAdmin() &amp;&amp; featureToggle.isEnabled(\"newFeature\")) {\n    // Admin-only feature code here\n} else {\n    // Regular user feature code here\n}\n</code></pre>"},{"location":"articles/feature-flags/#15-configuration-toggles","title":"1.5. Configuration Toggles","text":"<p>Configuration toggles control the behavior of an application or service based on configuration settings. They are typically used to enable or disable specific functionality based on the environment or configuration of the application.</p> <ul> <li> <p>Environment toggle</p> <p>This toggle enables specific features or functionality to be enabled or disabled based on the environment in which the application is running. For example, a development environment toggle might be used to enable debug mode or disable certain features that are still in development.</p> </li> <li> <p>Configuration toggle</p> <p>This toggle enables specific features or functionality to be enabled or disabled based on certain configurations or settings. For example, a configuration toggle might be used to enable or disable a feature based on the user's language or location.</p> </li> </ul> <p>Example:</p> <pre><code>if (configuration.getBoolean(\"newFeatureEnabled\")) {\n    // New feature code here\n} else {\n    // Old feature code here\n}\n</code></pre>"},{"location":"articles/feature-flags/#2-principle","title":"2. Principle","text":"<p>By following these principles, developers can ensure that feature flags are used effectively and do not negatively impact software quality or user experience. Feature flags can be a valuable technique for managing software development and deployment, but their use should be carefully considered and managed to ensure their proper use.</p> <ul> <li> <p>Plan ahead</p> <p>Feature flags should be planned and designed before any coding begins. This ensures that they are integrated into the codebase in a consistent and organized way. It's important to think about the purpose of each toggle, and how it will be used throughout the development cycle.</p> </li> <li> <p>Use meaningful names</p> <p>Toggle names should be clear and descriptive, so that other developers can easily understand what they do. It's also important to avoid using generic names, such as \"flag\" or \"toggle\", which can cause confusion.</p> </li> <li> <p>Test thoroughly</p> <p>Before deploying a feature toggle, it should be thoroughly tested in a staging or testing environment to ensure that it works as intended. It's important to test all possible scenarios and edge cases, and to have a plan in place for rolling back if something goes wrong.</p> </li> <li> <p>Document everything</p> <p>All feature flags should be documented in a centralized location, such as a wiki or documentation site. This should include information on the purpose of the toggle, when it was implemented, and who implemented it. This documentation should be kept up to date as the project evolves.</p> </li> <li> <p>Remove toggles as soon as possible</p> <p>Feature flags should be removed from the codebase as soon as they are no longer needed. This helps to keep the codebase clean and maintainable, and reduces the risk of bugs or conflicts. It's important to have a clear process in place for removing toggles, including testing and deployment procedures.</p> </li> </ul>"},{"location":"articles/feature-flags/#3-best-practice","title":"3. Best Practice","text":"<p>By following these best practices, developers can use feature flags to manage software development in a flexible and efficient way, while minimizing the risk of bugs and conflicts.</p> <ul> <li> <p>Limit the number of toggles</p> <p>Feature flags should be used sparingly and only when necessary. The more toggles there are, the more complex the codebase becomes, and the harder it is to maintain.</p> </li> <li> <p>Use feature flags in combination with other techniques</p> <p>Feature flags should be used in combination with other techniques, such as continuous integration and continuous deployment, to ensure that new features are delivered in a timely and efficient manner.</p> </li> <li> <p>Use version control</p> <p>Feature flags should be managed using version control tools, such as Git or SVN, to keep track of changes and make it easy to roll back if something goes wrong.</p> </li> <li> <p>Use descriptive names</p> <p>Feature flags should be named in a descriptive and meaningful way, so that other developers can easily understand their purpose and function.</p> </li> <li> <p>Test thoroughly</p> <p>All feature flags should be thoroughly tested before deployment to ensure that they work as intended. This includes testing all possible scenarios and edge cases, and having a plan in place for rolling back if something goes wrong.</p> </li> <li> <p>Monitor toggles in production</p> <p>Feature flags should be monitored in production to ensure that they are performing as expected and not causing any performance or security issues.</p> </li> <li> <p>Remove toggles as soon as possible</p> <p>Feature flags should be removed from the codebase as soon as they are no longer needed. This helps to keep the codebase clean and maintainable, and reduces the risk of bugs or conflicts.</p> </li> </ul>"},{"location":"articles/feature-flags/#4-terminology","title":"4. Terminology","text":"<p>By understanding these terminologies, developers can effectively use feature flags and related deployment strategies to manage software development and releases.</p> <ul> <li> <p>Toggle</p> <p>A toggle is a piece of code that controls the behavior of a feature or functionality. It can be turned on or off to enable or disable the feature.</p> </li> <li> <p>Feature flag</p> <p>A feature flag is another name for a feature toggle. It is a mechanism for controlling the release and visibility of features in software development.</p> </li> <li> <p>Flagsmith</p> <p>Flagsmith is an open-source feature flagging and remote configuration platform. It provides a centralized way to manage feature flags, and includes features like multivariate testing and targeting.</p> </li> </ul>"},{"location":"articles/feature-flags/#5-references","title":"5. References","text":"<ul> <li>Martin Fowler feature toggles article.</li> </ul>"},{"location":"articles/file-systems/","title":"File Systems // TODO","text":"<p>A file system is a structure of directories that is used to organize and store files. A file system is the method an operating system uses to name files and assign them locations for efficient storage and retrieval.</p> <ul> <li>1. Flash Driver</li> <li>1.1. JFFS2</li> <li>2. Linux</li> <li>2.1. ext3</li> <li>2.2. extfs</li> <li>2.3. overlay2</li> <li>2.4. btrfs</li> <li>2.5. zfs</li> <li>3. Microsoft</li> <li>3.1. FAT32</li> <li>3.2. NTFS</li> <li>4. macOS</li> <li>4.1. APFS</li> <li>5. Swap Space</li> <li>6. References</li> </ul>"},{"location":"articles/file-systems/#1-flash-driver","title":"1. Flash Driver","text":"<p>The special properties of data carriers based on flash memory mean that there are some file systems that take special account of these properties.</p>"},{"location":"articles/file-systems/#11-jffs2","title":"1.1. JFFS2","text":"<p>JFFS2 (Journaling Flash File System, version 2): further developed variant of JFFS, support for NAND flash, compression, etc.; contrary to the name, it does not use journaling.</p>"},{"location":"articles/file-systems/#2-linux","title":"2. Linux","text":""},{"location":"articles/file-systems/#21-ext3","title":"2.1. ext3","text":"<p>ext3 (Third Extended File System) file system developed with the Linux kernel. ext3 is an advanced variant of ext2 with journaling.</p>"},{"location":"articles/file-systems/#22-extfs","title":"2.2. extfs","text":""},{"location":"articles/file-systems/#23-overlay2","title":"2.3. overlay2","text":""},{"location":"articles/file-systems/#24-btrfs","title":"2.4. btrfs","text":""},{"location":"articles/file-systems/#25-zfs","title":"2.5. zfs","text":""},{"location":"articles/file-systems/#3-microsoft","title":"3. Microsoft","text":""},{"location":"articles/file-systems/#31-fat32","title":"3.1. FAT32","text":"<p>FAT32 (File Allocation Table) newer variant of the FAT file system family with extended limits compared to FAT16, from Windows 95b or Windows 2000.</p>"},{"location":"articles/file-systems/#32-ntfs","title":"3.2. NTFS","text":"<p>NTFS (New Technology File System) with journaling file system of the Windows NT product line.</p>"},{"location":"articles/file-systems/#4-macos","title":"4. macOS","text":""},{"location":"articles/file-systems/#41-apfs","title":"4.1. APFS","text":""},{"location":"articles/file-systems/#5-swap-space","title":"5. Swap Space","text":"<p>The primary function of swap space is to substitute disk space for RAM memory when real RAM fills up and more space is needed. Swap space is used for virtual memory storage areas when the system does not have enough physical memory to handle current processes.</p>"},{"location":"articles/file-systems/#6-references","title":"6. References","text":"<ul> <li>Oracle file systems article.</li> </ul>"},{"location":"articles/git/","title":"Git // TODO","text":"<ul> <li>Git Commands</li> <li>References</li> </ul>"},{"location":"articles/git/#git-commands","title":"Git Commands","text":"<ul> <li>Commit Hash for DevOps</li> </ul> <p>Receive the currentcommit with the following built-in commands:</p> <pre><code>git rev-parse HEAD\n</code></pre> <p>Add the commit hash to an application:</p> <pre><code>gcc main.c -o main -DGIT_COMMIT_HASH=\"$(git rev-parse HEAD)\"\n</code></pre> <ul> <li>List Contributors</li> </ul> <p>Rank all contributors locally based on their commits count with the following built-in commands:</p> <pre><code>git shortlog -sn\n</code></pre> <ul> <li>Normalizing Line Endings</li> </ul> <p>This command normalizes the line endings to the configuration in <code>.gitattributes</code>.</p> <pre><code>git add --renormalize .\ngit rm --cached -r .\ngit reset --hard\n</code></pre> <p>Verifying line endings.</p> <pre><code>git ls-files --eol\n</code></pre> <ul> <li>Diff</li> </ul> <p>This command shows the file differences which are not yet staged.</p> <pre><code>git diff\n</code></pre> <p>This command shows the differences between the two branches mentioned.</p> <pre><code>git diff [first branch] [second branch]\n</code></pre> <ul> <li>Amend</li> </ul> <p>Create a new message for last commit.</p> <pre><code>git commit --amend\n</code></pre> <p>Add a file to the last commit.</p> <p>Append <code>--no-edit</code> to the commit command if you do not want to edit the <code>commit</code> message.</p> <pre><code>git add &lt;filename&gt;\ngit commit --amend\n</code></pre> <ul> <li>Stashing</li> </ul> <p>Stash changes.</p> <pre><code>git stash save\n</code></pre> <p>Discard your stashed changes.</p> <pre><code>git stash drop\n</code></pre> <p>Apply and drop your stashed changes.</p> <pre><code>git stash pop\n</code></pre> <ul> <li>Refs and Log</li> </ul> <p>Use reflog to show the log of reference changes to HEAD.</p> <pre><code>git reflog\n</code></pre> <p>Check the Git history of a file.</p> <pre><code>git log &lt;file&gt;\n</code></pre> <p>Check the content of each change to a file.</p> <p>Append <code>--follow</code> to the commit command, to follows it past file renames.</p> <pre><code>gitk &lt;file&gt;\n</code></pre> <ul> <li>Prune</li> </ul> <p>Remove deleted and merged remote branches.</p> <pre><code>git fetch --prune\n</code></pre> <ul> <li>Ignore</li> </ul> <p>Add it to .gitignore:</p> <pre><code>echo &lt;file&gt; &gt;&gt; .gitignore\n</code></pre> <p>Remove it from the index:</p> <pre><code>git rm --cached &lt;file&gt;\n</code></pre> <p>Add .gitignore to index:</p> <pre><code>git add &lt;file&gt;\n</code></pre> <p>Confirm:</p> <pre><code>git status --ignored\n</code></pre>"},{"location":"articles/git/#references","title":"References","text":"<ul> <li>Git book article.</li> <li>Git support article.</li> </ul>"},{"location":"articles/githooks/","title":"Githooks","text":"<p>Hooks are built-in features used by Git to trigger actions at certain points in git command. Hooks has a way to fire off custom scripts when certain important actions occur. There are two groups of these hooks: client-side and server-side.</p> <ul> <li>1. Client-Side Hooks</li> <li>1.1. pre-commit</li> <li>1.2. prepare-commit-msg</li> <li>1.3. commit-msg</li> <li>1.4. post-commit</li> <li>1.5. pre-rebase</li> <li>1.6. post-checkout</li> <li>1.7. post-rewrite</li> <li>1.8. post-merge</li> <li>1.9. pre-push</li> <li>1.10. applypatch-msg</li> <li>1.11. pre-applypatch</li> <li>1.12. post-applypatch</li> <li>2. Server-Side Hooks</li> <li>2.1. pre-receive</li> <li>2.2. update</li> <li>2.3. post-receive</li> <li>3. Configuration</li> <li>4. References</li> </ul>"},{"location":"articles/githooks/#1-client-side-hooks","title":"1. Client-Side Hooks","text":"<p>Client-side hooks are triggered by operations such as committing and merging, while server-side hooks run on network operations such as receiving pushed commits. Use these hooks for all sorts of reasons.</p> <p>NOTE Client-side hooks are not copied when you clone a repository.</p>"},{"location":"articles/githooks/#11-pre-commit","title":"1.1. pre-commit","text":"<p>The pre-commit hook is run first, before you even type in a commit message. It\u2019s used to inspect the snapshot that\u2019s about to be committed, to see if you\u2019ve forgotten something, to make sure tests run, or to examine whatever you need to inspect in the code. Exiting non-zero from this hook aborts the commit, although you can bypass it with <code>git commit --no-verify</code>. You can do things like check for code style (run lint or something equivalent), check for trailing whitespace (the default hook does exactly this), or check for appropriate documentation on new methods.</p> <p>Hook is invoked by <code>git-commit</code>.</p> <p>Usage:</p> <ul> <li>Enforcing coding standards.</li> </ul>"},{"location":"articles/githooks/#12-prepare-commit-msg","title":"1.2. prepare-commit-msg","text":"<p>The prepare-commit-msg hook is run before the commit message editor is fired up but after the default message is created. It lets you edit the default message before the commit author sees it. This hook takes a few parameters: the path to the file that holds the commit message so far, the type of commit, and the commit SHA-1 if this is an amended commit. This hook generally isn\u2019t useful for normal commits; rather, it\u2019s good for commits where the default message is auto-generated, such as templated commit messages, merge commits, squashed commits, and amended commits. You may use it in conjunction with a commit template to programmatically insert information.</p> <p>Hook is invoked by <code>git-commit</code>.</p> <p>Usage:</p> <ul> <li>Editing the commit message.</li> </ul> <p>Parameters:</p> <ol> <li>Name of the file that containing the commit log message.</li> <li>Source of the commit message.</li> </ol>"},{"location":"articles/githooks/#13-commit-msg","title":"1.3. commit-msg","text":"<p>The commit-msg hook takes one parameter, which again is the path to a temporary file that contains the commit message written by the developer. If this script exits non-zero, Git aborts the commit process, so use it to validate your project state or commit message before allowing a commit to go through according to the Conventional Commits.</p> <p>Hook is invoked by <code>git-commit</code> and <code>git-merge</code>.</p> <p>Usage:</p> <ul> <li>Enforce Conventional Commits to commit messages.</li> </ul> <p>Parameters:</p> <ol> <li>Name of the file that containing the proposed commit log message.</li> </ol>"},{"location":"articles/githooks/#14-post-commit","title":"1.4. post-commit","text":"<p>The post-commit hook runs after the entire commit process is completed. It doesn\u2019t take any parameters, but you can easily get the last commit by running <code>git log -1 HEAD</code>. Generally, this script is used for notification or something similar.</p> <p>Hook is invoked by <code>git-commit</code>.</p> <p>Usage:</p> <ul> <li>Notification by email/app about a new commit.</li> </ul>"},{"location":"articles/githooks/#15-pre-rebase","title":"1.5. pre-rebase","text":"<p>The pre-rebase hook runs before you rebase anything and can halt the process by exiting non-zero. Use this hook to disallow rebasing any commits that have already been pushed according to the Merging Strategies.</p> <p>Hook is invoked by <code>git-rebase</code>.</p> <p>Usage:</p> <ul> <li>Prevent a branch from getting rebased.</li> </ul> <p>Parameters:</p> <ol> <li>Upstream from fork.</li> <li>Rebased branch (is not set when rebasing the current branch).</li> </ol>"},{"location":"articles/githooks/#16-post-checkout","title":"1.6. post-checkout","text":"<p>The post-checkout hook runs after a successful <code>git checkout</code>. Use it to set up your working directory properly for your project environment. This may mean moving in large binary files that you don\u2019t want source controlled, auto-generating documentation, or something along those lines.</p> <p>Hook is invoked by <code>git-checkout</code>, <code>git-switch</code> or <code>git-clone</code>.</p> <p>Usage:</p> <ul> <li>Perform repository validity checks.</li> <li>Enforce the creation of support branches from the base branches.</li> </ul> <p>Parameters:</p> <ol> <li>Ref of the previous HEAD.</li> <li>Ref of the new HEAD.</li> <li>Flag indicator, (changing branches, flag=1) or (retrieving a file from the index, flag=0).</li> </ol>"},{"location":"articles/githooks/#17-post-rewrite","title":"1.7. post-rewrite","text":"<p>The post-rewrite hook is run by commands that replace commits, such as git <code>commit --amend</code> and git rebase (though not by <code>git filter-branch</code>). Its single argument is which command triggered the rewrite, and it receives a list of rewrites on stdin. This hook has many of the same uses as the <code>post-checkout</code> and <code>post-merge</code> hooks.</p> <p>Hook is invoked by <code>git-commit</code> and <code>git-rebase</code>.</p> <p>Usage:</p> <ul> <li>Receives a list of the rewritten commits on stdin.</li> </ul> <p>Parameters:</p> <ol> <li>Denotes the command it was invoked by.</li> </ol>"},{"location":"articles/githooks/#18-post-merge","title":"1.8. post-merge","text":"<p>The post-merge hook runs after a successful <code>merge</code> command, which happens when a <code>git pull</code> is done on a local repository. Use it to restore data in the working tree that Git can\u2019t track, such as permissions data. This hook can likewise validate the presence of files external to Git control that you may want copied in when the working tree changes.</p> <p>Hook is invoked by <code>git-merge</code>.</p> <p>Usage:</p> <ul> <li>Use in conjunction with a corresponding pre-commit hook to save and restore any form of metadata associated with the working tree (e.g. permissions/ownership, ACL).</li> </ul> <p>Parameters:</p> <ol> <li>Status flag specifying whether or not the merge being done was a squash merge.</li> </ol>"},{"location":"articles/githooks/#19-pre-push","title":"1.9. pre-push","text":"<p>The pre-push hook runs during <code>git push</code>, after the remote refs have been updated but before any objects have been transferred. It receives the name and location of the remote as parameters, and a list of to-be-updated refs through <code>stdin</code>. Use it to validate a set of ref updates before a push occurs (a non-zero exit code will abort the push). Further, prevent pushing to [main/develop] branch according to the Branching Strategies.</p> <p>Hook is invoked by <code>git-push</code>.</p> <p>Usage:</p> <ul> <li>Prevent a push.</li> </ul> <p>Parameters:</p> <ol> <li>Name of the destination remote.</li> <li>Location of the destination remote.</li> </ol>"},{"location":"articles/githooks/#110-applypatch-msg","title":"1.10. applypatch-msg","text":"<p>The applypatch-msg email workflow invoked by the <code>git am</code> command takes a single argument: the name of the temporary file that contains the proposed commit message. Git aborts the patch if this script exits non-zero. Use this to make sure a commit message is properly formatted, or to normalize the message by having the script edit it in place.</p>"},{"location":"articles/githooks/#111-pre-applypatch","title":"1.11. pre-applypatch","text":"<p>The pre-applypatch hook runs during <code>git am</code>, after the patch is applied but before a commit is made, so use it to inspect the snapshot before making the commit. You can run tests or otherwise inspect the working tree with this script. If something is missing or the tests don\u2019t pass, exiting non-zero aborts the <code>git am</code> script without committing the patch.</p>"},{"location":"articles/githooks/#112-post-applypatch","title":"1.12. post-applypatch","text":"<p>The post-applypatch hook runs during <code>git am</code>, which runs after the commit is made. Use it to notify a group or the author of the patch you pulled in that you\u2019ve done so. You can\u2019t stop the patching process with this script.</p>"},{"location":"articles/githooks/#2-server-side-hooks","title":"2. Server-Side Hooks","text":"<p>Server-side hooks can be used as a system administrator to enforce nearly any kind of policy for your project. These scripts run before and after pushes to the server. The pre hooks can exit non-zero at any time to reject the push as well as print an error message back to the client; you can set up a push policy that\u2019s as complex as you wish.</p> <p>NOTE Server-side hooks can be used to enforce a policy - Git-Enforced Policy.</p>"},{"location":"articles/githooks/#21-pre-receive","title":"2.1. pre-receive","text":"<p>The first script to run when handling a push from a client is pre-receive. It takes a list of references that are being pushed from <code>stdin</code>; if it exits non-zero, none of them are accepted. Use this hook to do things like make sure none of the updated references are non-fast-forwards, or to do access control for all the refs and files they\u2019re modifying with the push.</p> <p>Hook is invoked by <code>git-receive-pack</code>.</p> <p>Usage:</p> <ul> <li>Push the code to production.</li> </ul>"},{"location":"articles/githooks/#22-update","title":"2.2. update","text":"<p>The update script is very similar to the <code>pre-receive</code> script, except that it\u2019s run once for each branch the pusher is trying to update. If the pusher is trying to push to multiple branches, <code>pre-receive</code> runs only once, whereas update runs once per branch they\u2019re pushing to. Instead of reading from stdin, this script takes three arguments: the name of the reference (branch), the SHA-1 that reference pointed to before the push, and the SHA-1 the user is trying to push. If the update script exits non-zero, only that reference is rejected; other references can still be updated.</p> <p>Hook is invoked by <code>git-receive-pack</code>.</p> <p>Usage:</p> <ul> <li>Enforce a <code>fast-forward only</code> policy.</li> </ul> <p>Parameters:</p> <ol> <li>Name of the ref being updated.</li> <li>Old object name stored in the ref.</li> <li>New object name to be stored in the ref.</li> </ol>"},{"location":"articles/githooks/#23-post-receive","title":"2.3. post-receive","text":"<p>The post-receive hook runs after the entire process is completed and can be used to update other services or notify users. It takes the same <code>stdin</code> data as the <code>pre-receive</code> hook. Examples include emailing a list, notifying a continuous integration server, or updating a ticket-tracking system \u2013 you can even parse the commit messages to see if any tickets need to be opened, modified, or closed. This script can\u2019t stop the push process, but the client doesn\u2019t disconnect until it has completed, so be careful if you try to do anything that may take a long time.</p> <p>Hook is invoked by <code>git-receive-pack</code>.</p> <p>Usage:</p> <ul> <li>Push the code to production.</li> </ul>"},{"location":"articles/githooks/#3-configuration","title":"3. Configuration","text":"<p>The hooks are all stored in the hooks subdirectory of the Git directory. In most projects, that\u2019s <code>.git/hooks</code>, by default the hooks directory is <code>$GIT_DIR/hooks</code>. When you initialize a new repository with <code>git init</code>, Git populates the hooks directory with a bunch of example scripts, many of which are useful by themselves; but they also document the input values of each script. All the examples are written as shell scripts, with some Perl thrown in, but any properly named executable scripts will work fine \u2013 you can write them in Ruby or Python or whatever language you are familiar with. If you want to use the bundled hook scripts, you\u2019ll have to rename them; their file names all end with <code>.sample</code>.</p> <p>To enable a hook script, put a file in the <code>hooks</code> subdirectory of your .git directory that is named appropriately (without any extension) and is executable. From that point forward, it should be called. We\u2019ll cover most of the major hook filenames here.</p> <p>The <code>.git</code> folder is not under version control, since most of its contents are device and user-specific. If you\u2019re using Git version 2.9 or greater, change the hooks directory using <code>core.hooksPath</code> configuration variable. Create a folder inside the repository and place it under version control and change the hooks directory with the following command:</p> <pre><code>git config core.hooksPath githooks\n</code></pre>"},{"location":"articles/githooks/#4-references","title":"4. References","text":"<ul> <li>Git book article.</li> </ul>"},{"location":"articles/identity-and-access-management/","title":"Identity and Access Management","text":"<p>Identity and Access Management (IAM) should be based on established standards such as OAuth 2.0 and Open ID Connect.</p> <ul> <li>Authentication</li> <li>Authorization</li> <li>References</li> </ul>"},{"location":"articles/identity-and-access-management/#authentication","title":"Authentication","text":""},{"location":"articles/identity-and-access-management/#authorization","title":"Authorization","text":""},{"location":"articles/identity-and-access-management/#references","title":"References","text":"<ul> <li>Okta identity platform.</li> <li>Ory identity platform.</li> <li>AppAuth a Native App SDK for OAuth 2.0 and OpenID Connect.</li> <li>JWT (JSON Web Tokens) are an open, industry standard RFC 7519 method for representing claims securely between two parties.</li> </ul>"},{"location":"articles/incident-management/","title":"Incident Management","text":"<p>Incident management is a systematic approach to handling and resolving incidents or disruptions that occur within an organization's systems, services, or operations. It involves a set of processes, policies, and procedures designed to detect, respond to, mitigate, and recover from incidents effectively. Incident management aims to minimize the impact of incidents on business operations and ensure the swift restoration of normal service levels.</p> <ul> <li>1. Category</li> <li>1.1. Severity/Priority Levels</li> <li>1.2. Incident Types</li> <li>1.3. Technical Categories</li> <li>1.4. Service Categories</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/incident-management/#1-category","title":"1. Category","text":"<p>Incident management, incidents are often categorized based on their impact, urgency, or the nature of the issue. Categorizing incidents helps prioritize their resolution and allocate appropriate resources.</p>"},{"location":"articles/incident-management/#11-severitypriority-levels","title":"1.1. Severity/Priority Levels","text":"<p>Severity and priority are commonly used terms in incident management to classify and prioritize incidents based on their impact and urgency.</p> <p>The severity and priority levels assigned to incidents help incident management teams prioritize their efforts, allocate appropriate resources, and ensure that critical incidents receive immediate attention. These classifications are often defined in an organization's incident management processes and may be adjusted based on specific business requirements and service level agreements (SLAs).</p> <p>Types of Levels:</p> <ol> <li> <p>Severity Levels</p> <p>Severity refers to the impact and seriousness of an incident on business operations or services. It helps assess the potential harm or disruption caused by the incident.</p> <p>Levels of Severity:</p> <ul> <li> <p>High/Critical</p> <p>Incidents with severe impact that result in a complete loss of service, significant financial loss, or pose a significant risk to safety, security, or compliance. These incidents require immediate attention and the highest priority for resolution.</p> </li> <li> <p>Medium/Moderate</p> <p>Incidents with a moderate impact that affect a specific functionality or service, but do not completely disrupt business operations. These incidents require prompt attention and resolution to prevent further escalation or significant impact.</p> </li> <li> <p>Low/Minor</p> <p>Incidents with a minor impact that cause minimal disruption or affect non-critical functionality. These incidents may be more tolerable or have workarounds available, allowing them to be addressed with lower priority compared to higher-severity incidents.</p> </li> </ul> </li> <li> <p>Priority Levels</p> <p>Priority refers to the urgency or order in which incidents should be resolved. It helps determine the sequence of incident handling based on business needs, service level agreements (SLAs), and available resources.</p> <p>Levels of Priority:</p> <ul> <li> <p>P1 (Priority 1)</p> <p>Incidents classified as P1 have the highest priority and require immediate attention and resolution. They typically involve critical business services, systems, or operations that are completely unavailable or severely impacted.</p> </li> <li> <p>P2 (Priority 2)</p> <p>Incidents classified as P2 have a high priority and need to be addressed promptly. While they may not have the same level of immediate impact as P1 incidents, they still affect critical services or functions and can cause significant disruption if left unresolved.</p> </li> <li> <p>P3 (Priority 3)</p> <p>Incidents classified as P3 have a medium priority and should be addressed within a reasonable timeframe. They generally impact non-critical services or functions and have a moderate impact on business operations.</p> </li> <li> <p>P4 (Priority 4)</p> <p>Incidents classified as P4 have the lowest priority and can be resolved within a longer timeframe. These incidents typically involve minor issues, non-essential services, or non-critical functionalities.</p> </li> </ul> </li> </ol>"},{"location":"articles/incident-management/#12-incident-types","title":"1.2. Incident Types","text":"<p>Incident types refer to different categories or classifications of incidents based on their nature or specific problem types.</p> <p>Types of Incident:</p> <ol> <li> <p>Performance Degradation</p> <p>Performance degradation incidents involve situations where systems, applications, or services experience a decrease in performance or responsiveness. This could be due to high resource utilization, network congestion, software bugs, or other factors affecting performance.</p> </li> <li> <p>Service Outage</p> <p>Service outage incidents occur when a system, application, or service becomes completely unavailable. It could be due to hardware failures, software crashes, network outages, or denial-of-service (DoS) attacks.</p> </li> <li> <p>Security Breach</p> <p>Security incidents involve unauthorized access, data breaches, malware infections, or other cybersecurity-related events. These incidents require immediate attention to prevent further damage, protect sensitive information, and mitigate potential risks.</p> </li> <li> <p>Data Loss or Corruption</p> <p>Incidents in the loss, deletion, or corruption of important data. It could be accidental, such as human error or hardware failures, or intentional, such as cyberattacks or malicious actions.</p> </li> <li> <p>User Access Issues</p> <p>User access incidents encompass problems related to user authentication, authorization, or permissions. Examples include forgotten passwords, account lockouts, access requests, or configuration errors that prevent users from accessing the required resources.</p> </li> <li> <p>Application Errors</p> <p>Application errors or failures within specific applications lol could be a software bug, an unhandled exception, or a crash that causes the application to behave unexpectedly or become unusable.</p> </li> <li> <p>Configuration Issues</p> <p>Configuration issues pertain to misconfigurations that impact system functionality, security, or performance. It could involve incorrect network configurations, misconfigured access controls, or improper system settings.</p> </li> <li> <p>Communication Disruptions</p> <p>These incidents involve issues with communication channels or connectivity. It could be disruptions in network connectivity, telecommunication failures, email delivery problems, or other issues that affect communication and collaboration.</p> </li> <li> <p>Hardware Failures</p> <p>Incidents related to hardware failures encompass problems with servers, storage devices, networking equipment, or other physical infrastructure components. Hardware failures can lead to service disruptions or complete outages.</p> </li> <li> <p>Software Updates and Patches</p> <p>Incidents arising from software updates or patch deployments include issues like compatibility problems, installation failures, or unexpected behavior after applying updates.</p> </li> </ol>"},{"location":"articles/incident-management/#13-technical-categories","title":"1.3. Technical Categories","text":"<p>Technical categories are used to classify incidents based on the technical aspect or area affected. Technical categories help incident management teams and support personnel quickly identify the domain or expertise required to resolve the incidents efficiently. Proper categorization ensures that incidents are routed to the appropriate teams or individuals for prompt and effective resolution.</p> <p>Types of Technical Categories:</p> <ol> <li> <p>Network</p> <p>Incidents related to network connectivity, network devices (routers, switches), or network protocols. Examples include network outages, slow network performance, or configuration issues impacting network connectivity.</p> </li> <li> <p>Hardware</p> <p>Incidents involving hardware failures or issues with physical infrastructure components. This can include servers, storage devices, network equipment, or other hardware failures.</p> </li> <li> <p>Software</p> <p>Incidents related to software applications, operating systems, or software integrations. Examples include application crashes, software errors, or compatibility issues.</p> </li> <li> <p>Database</p> <p>Incidents involving databases, such as data corruption, performance issues, or database connectivity problems. Examples include slow database queries, database server crashes, or data integrity issues.</p> </li> <li> <p>Security</p> <p>Incidents related to security breaches, unauthorized access, malware infections, or other security-related events. It includes incidents such as hacking attempts, data breaches, denial-of-service (DoS) attacks, or security vulnerabilities.</p> </li> <li> <p>Applications</p> <p>Incidents specific to a particular application or software system. This could include custom-developed applications, third-party software, or software modules. Examples include application-specific errors, functionality issues, or configuration problems.</p> </li> <li> <p>Infrastructure</p> <p>Incidents related to the broader IT infrastructure beyond hardware and network components. This can include power supply, cooling systems, physical facilities, or environmental factors. Examples include power outages, HVAC failures, or physical damage to infrastructure.</p> </li> <li> <p>Telecommunications</p> <p>Incidents involving telecommunication services or devices, such as phone systems, mobile networks, or voice over IP (VoIP) services. Examples include call quality issues, dropped calls, or telecommunication service disruptions.</p> </li> <li> <p>Cloud</p> <p>Incidents related to cloud-based services or infrastructure, such as cloud service providers, cloud storage, virtual machines, or cloud-based applications. Examples include issues with cloud connectivity, performance degradation in cloud services, or misconfigured cloud resources.</p> </li> <li> <p>Desktop/End-User</p> <p>Incidents related to end-user desktops or workstations, such as software installation issues, hardware malfunctions, user profile problems, or issues with peripherals.</p> </li> </ol>"},{"location":"articles/incident-management/#14-service-categories","title":"1.4. Service Categories","text":"<p>Service categories are used to classify incidents based on the affected service or business function. Categorizing incidents by service helps in understanding the impact on specific areas of the organization and enables efficient incident management.</p> <p>Types of Service Categories:</p> <ol> <li> <p>Email Service</p> <p>Incidents related to email services, such as issues with sending or receiving emails, email delivery delays, mailbox access problems, or spam-related issues.</p> </li> <li> <p>Website or Web Application</p> <p>Incidents that affect websites or web applications. This can include website outages, broken links, slow page load times, functionality errors, or issues with form submissions.</p> </li> <li> <p>Customer Support</p> <p>Incidents related to customer support systems or processes. It could involve problems with customer support ticketing systems, live chat tools, call center applications, or issues with accessing customer information.</p> </li> <li> <p>Payment Processing</p> <p>Incidents that impact payment processing systems or payment gateways. This can include transaction failures, issues with payment methods, payment gateway errors, or discrepancies in payment records.</p> </li> <li> <p>Inventory Management</p> <p>Incidents related to inventory management systems or processes. It could involve issues with stock levels, inventory synchronization problems, or errors in tracking and managing inventory.</p> </li> <li> <p>Collaboration Platform</p> <p>Incidents that affect collaboration platforms or tools used for teamwork and communication within an organization. This can include problems with document sharing, project management systems, video conferencing tools, or issues with accessing shared resources.</p> </li> <li> <p>Human Resources</p> <p>Incidents related to human resources systems or processes. It could involve problems with employee onboarding, leave management, payroll systems, or issues with accessing HR-related information.</p> </li> <li> <p>Customer Relationship Management (CRM)</p> <p>Incidents related to customer relationship management systems or processes. This can include issues with managing customer data, customer profiles, sales pipelines, or problems with CRM integrations.</p> </li> <li> <p>Enterprise Resource Planning (ERP)</p> <p>Incidents that impact enterprise resource planning systems used for managing business processes, such as finance, procurement, or supply chain management. This can include issues with order processing, invoicing, reporting, or problems with ERP system modules.</p> </li> <li> <p>Telecommunication Services</p> <p>Incidents related to telecommunication services within the organization, such as phone systems, mobile services, or video conferencing solutions. This can include issues with call quality, dropped calls, telecommunication service disruptions, or problems with telecommunication devices.</p> </li> <li> <p>IT Infrastructure</p> <p>Incidents related to general IT infrastructure components and services that support the organization's operations. This can include issues with servers, networks, data centers, backups, or problems with IT service delivery.</p> </li> </ol>"},{"location":"articles/incident-management/#2-principles","title":"2. Principles","text":"<p>The principles of incident management guide the effective handling and resolution of incidents. These principles provide a framework for incident response and help organizations minimize the impact of incidents.</p> <ul> <li> <p>Preparedness</p> <p>Being prepared is crucial for effective incident management. This involves having well-defined incident management processes, procedures, and documentation in place before incidents occur. It includes creating incident response plans, establishing communication channels, and ensuring that incident management teams are trained and equipped to handle incidents promptly.</p> </li> <li> <p>Proactive Monitoring</p> <p>Implementing proactive monitoring systems and tools allows organizations to detect and identify incidents early. By continuously monitoring systems, networks, and applications, organizations can identify potential issues, performance degradations, or anomalies that may lead to incidents. Proactive monitoring helps in swift incident detection and faster response times.</p> </li> <li> <p>Rapid Response</p> <p>Timely response is essential in incident management. When an incident occurs, it's crucial to initiate the incident response process promptly. This includes acknowledging the incident, notifying the appropriate stakeholders, and activating the incident management team. Rapid response helps prevent the incident from escalating and minimizes the impact on business operations.</p> </li> <li> <p>Clear Communication</p> <p>Effective communication is vital during incident management. Establishing clear and open communication channels among incident management teams, stakeholders, and affected parties is crucial. Regular and timely updates should be provided to keep stakeholders informed about the incident's status, progress, and expected resolution timeline.</p> </li> <li> <p>Escalation and Collaboration</p> <p>Knowing when to escalate an incident and involve additional resources or higher-level support is important. If an incident exceeds the capabilities of the initial responders, it should be escalated to the appropriate teams or individuals with the necessary expertise. Collaboration between different teams and departments ensures a coordinated effort to resolve incidents efficiently.</p> </li> <li> <p>Prioritization</p> <p>Incidents should be prioritized based on their impact and urgency. Assigning severity or priority levels helps in determining the order in which incidents should be handled. High-impact or critical incidents that have a severe impact on business operations or services should receive immediate attention and higher priority for resolution.</p> </li> <li> <p>Root Cause Analysis</p> <p>Conducting a thorough root cause analysis is essential to identify the underlying cause of an incident. Once the incident is resolved, a detailed investigation should be carried out to determine why the incident occurred and to implement preventive measures. Understanding the root cause helps in preventing similar incidents from recurring in the future.</p> </li> <li> <p>Continuous Improvement</p> <p>Incident management is an iterative process, and organizations should strive for continuous improvement. Regularly reviewing and analyzing incident management practices, identifying areas for improvement, and implementing corrective actions leads to more efficient incident handling and better overall incident management capabilities.</p> </li> <li> <p>Documentation and Knowledge Management</p> <p>Proper documentation of incidents, including their resolution steps, findings, and lessons learned, is crucial. This documentation serves as a valuable knowledge base for future reference, helps in training new team members, and supports continuous improvement efforts. Incident management systems or knowledge management platforms can be utilized to store and share incident-related information.</p> </li> </ul>"},{"location":"articles/incident-management/#3-best-practice","title":"3. Best Practice","text":"<p>Best practices of incident management encompass a set of proven approaches and strategies that organizations can follow to optimize their incident response and resolution processes.</p> <ul> <li> <p>Establish Clearly Defined Processes</p> <p>Develop well-defined incident management processes and procedures tailored to the organization's needs. These processes should outline the steps to be followed from incident identification and logging to resolution and post-incident analysis. Having standardized processes ensures consistency and efficiency in handling incidents.</p> </li> <li> <p>Implement an Incident Management System</p> <p>Utilize a dedicated incident management system or IT service management (ITSM) tool to facilitate incident tracking, documentation, and collaboration among incident management teams. Such systems provide centralized incident repositories, communication channels, and reporting capabilities to streamline the incident management process.</p> </li> <li> <p>Adopt an Incident Classification and Prioritization Framework</p> <p>Define a clear incident classification and prioritization framework that aligns with the organization's objectives and service level agreements (SLAs). This helps prioritize incidents based on their impact, urgency, and business criticality, ensuring that the most critical incidents receive prompt attention and resources.</p> </li> <li> <p>Enable Effective Communication</p> <p>Establish clear communication channels for incident reporting, updates, and coordination. This includes maintaining contact lists, establishing incident communication groups or channels, and defining escalation paths. Timely and transparent communication ensures that stakeholders are informed throughout the incident lifecycle.</p> </li> <li> <p>Form Cross-Functional Incident Response Teams</p> <p>Create incident response teams comprising individuals from different functional areas to ensure diverse expertise and perspectives during incident resolution. These teams should consist of representatives from IT, operations, security, communications, and other relevant departments, enabling effective collaboration and quicker incident resolution.</p> </li> <li> <p>Conduct Regular Training and Drills</p> <p>Provide ongoing training and conduct incident response drills and simulations to familiarize incident management teams with the processes, tools, and procedures. This helps improve response times, decision-making, and coordination during real incidents.</p> </li> <li> <p>Establish Service Level Agreements (SLAs)</p> <p>Define SLAs with stakeholders, outlining the response and resolution timeframes for different incident priority levels. SLAs set clear expectations and accountability for incident management teams and help prioritize incidents based on their impact and urgency.</p> </li> <li> <p>Implement Continuous Monitoring and Alerting</p> <p>Deploy proactive monitoring systems to detect incidents early and set up automated alerts for critical system thresholds or abnormal behaviors. This enables proactive incident management by identifying potential issues before they escalate into significant disruptions.</p> </li> <li> <p>Conduct Post-Incident Reviews and Analysis</p> <p>Perform thorough post-incident reviews, also known as post-mortems, to analyze incidents and identify areas for improvement. These reviews help uncover root causes, identify process gaps, and implement preventive measures to avoid similar incidents in the future.</p> </li> <li> <p>Foster a Culture of Continuous Improvement</p> <p>Encourage a culture of continuous improvement by regularly reviewing incident management practices, incorporating feedback from incident stakeholders, and implementing lessons learned. This ensures that incident management processes evolve and mature over time.</p> </li> </ul>"},{"location":"articles/incident-management/#4-terminology","title":"4. Terminology","text":"<p>Terminologies form the foundation of incident management processes and are used to communicate, prioritize, and address incidents effectively within an organization.</p> <ul> <li> <p>Incident</p> <p>An incident refers to any unplanned interruption or disruption in normal operations, services, or processes. It could be an event, error, failure, or deviation from expected results that requires attention and resolution.</p> </li> <li> <p>Service Level Agreement (SLA)</p> <p>An SLA is a formal agreement or contract between an organization and its customers or stakeholders. It outlines the agreed-upon levels of service, including response times, resolution times, and performance metrics, for incidents and service requests.</p> </li> <li> <p>Severity</p> <p>Severity represents the impact or seriousness of an incident on business operations. It is used to prioritize incidents based on their impact, usually classified into levels such as high, medium, or low.</p> </li> <li> <p>Priority</p> <p>Priority denotes the urgency or order in which incidents should be resolved. It helps determine the sequence of incident handling based on their impact, urgency, and business criticality. Priorities are typically categorized as P1, P2, P3, etc., with P1 being the highest priority.</p> </li> <li> <p>Incident Management</p> <p>Incident management refers to the overall process of identifying, categorizing, prioritizing, resolving, and documenting incidents in an organized and efficient manner. It involves a set of procedures, tools, and resources to manage and mitigate the impact of incidents on business operations.</p> </li> <li> <p>Incident Response</p> <p>Incident response involves the immediate actions taken upon the identification of an incident. It includes acknowledging the incident, notifying the appropriate stakeholders, initiating the incident management process, and assigning resources for incident resolution.</p> </li> <li> <p>Incident Owner</p> <p>The incident owner is the person or team responsible for managing and coordinating the resolution of a specific incident. The incident owner ensures that the incident is properly handled, assigns appropriate resources, and communicates with stakeholders throughout the incident lifecycle.</p> </li> <li> <p>Escalation</p> <p>Escalation refers to the process of increasing the level of support or involving higher-level resources when an incident cannot be resolved within the agreed-upon timeframe or by the initial responders. Escalation ensures that incidents receive the necessary attention and expertise for timely resolution.</p> </li> <li> <p>Root Cause Analysis (RCA)</p> <p>Root cause analysis is a systematic process of investigating and identifying the underlying cause or causes of an incident. It aims to determine the fundamental reason behind the incident occurrence to implement corrective actions and prevent similar incidents in the future.</p> </li> <li> <p>Post-Incident Analysis</p> <p>Post-incident analysis, also known as post-mortem or retrospective, is the examination and evaluation of an incident after its resolution. It involves reviewing the incident response process, analyzing the root cause, identifying lessons learned, and making improvements to prevent recurrence.</p> </li> </ul>"},{"location":"articles/licenses/","title":"Licenses","text":"<p>Open source licenses grant permission to use, modify, and redistribute licensed software for any purpose, subject to conditions preserving the provenance and openness of the software.</p> <p>When open source software is copied and redistributed, which is usually permitted by any type of open source license, a number of obligations and prohibitions are imposed on the distributor It is common for recipients of such software to redistribute it in such a way as to create a chain of distributors and recipients who must all comply with the same license obligations.</p> <ul> <li>1. License List</li> <li>1.1. SPDX</li> <li>1.2. OSADLE</li> <li>2. License</li> <li>2.1. GNU AGPLv3</li> <li>2.2. GNU GPLv3</li> <li>2.3. GNU LGPLv3</li> <li>2.4. MPL 2.0</li> <li>2.5. Apache License 2.0</li> <li>2.6. MIT License</li> <li>2.7. BSL-1.0</li> <li>2.8. CC0 1.0</li> <li>3. Glossary</li> <li>3.1. Permissions</li> <li>3.2. Conditions</li> <li>3.3. Limitations</li> <li>4. References</li> </ul>"},{"location":"articles/licenses/#1-license-list","title":"1. License List","text":""},{"location":"articles/licenses/#11-spdx","title":"1.1. SPDX","text":"<p>The SPDX License List is a list of commonly found licenses and exceptions used in free and open or collaborative software, data, hardware, or documentation.</p> <p>A generated dataset of the SPDX license list in text format. See references for more information on SPDX.</p>"},{"location":"articles/licenses/#12-osadle","title":"1.2. OSADLE","text":"<p>The OSADLE license checklists provides a common list of license in raw data of obligations of commonly used open source software that are accepted by distributors and copyright holders and trusted by all members of the distribution chain.</p>"},{"location":"articles/licenses/#2-license","title":"2. License","text":"<p>The licenses represent the entire spectrum of open source licenses, from highly protective to unconditional. The following licenses are sorted by the number of conditions, from most (GNU AGPLv3) to none (Unlicense).</p>"},{"location":"articles/licenses/#21-gnu-agplv3","title":"2.1. GNU AGPLv3","text":"<p>GNU AGPLv3 (Affero General Public License v3.0)</p> <p>Permissions of this strongest copyleft license are conditioned on making available complete source code of licensed works and modifications, which include larger works using a licensed work, under the same license. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights. When a modified version is used to provide a service over a network, the complete source code of the modified version must be made available.</p> Permissions Conditions Limitations Commercial use Disclose source Liability Distribution License and copyright notice Warranty Modification Network use is distribution Patent use Same license Private use State changes"},{"location":"articles/licenses/#22-gnu-gplv3","title":"2.2. GNU GPLv3","text":"<p>GNU GPLv3 (General Public License v3.0)</p> <p>Permissions of this strong copyleft license are conditioned on making available complete source code of licensed works and modifications, which include larger works using a licensed work, under the same license. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights.</p> Permissions Conditions Limitations Commercial use Disclose source Liability Distribution License and copyright notice Warranty Modification Same license Patent use State changes Private use"},{"location":"articles/licenses/#23-gnu-lgplv3","title":"2.3. GNU LGPLv3","text":"<p>GNU LGPLv3 (Lesser General Public License v3.0)</p> <p>Permissions of this copyleft license are conditioned on making available complete source code of licensed works and modifications under the same license or the GNU GPLv3. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights. However, a larger work using the licensed work through interfaces provided by the licensed work may be distributed under different terms and without source code for the larger work.</p> Permissions Conditions Limitations Commercial use Disclose source Liability Distribution License and copyright notice Warranty Modification Same license (library) Patent use State changes Private use"},{"location":"articles/licenses/#24-mpl-20","title":"2.4. MPL 2.0","text":"<p>MPL 2.0 (Mozilla Public License 2.0)</p> <p>Permissions of this weak copyleft license are conditioned on making available source code of licensed files and modifications of those files under the same license (or in certain cases, one of the GNU licenses). Copyright and license notices must be preserved. Contributors provide an express grant of patent rights. However, a larger work using the licensed work may be distributed under different terms and without source code for files added in the larger work.</p> Permissions Conditions Limitations Commercial use Disclose source Liability Distribution License and copyright notice Trademark use Modification Same license (file) Warranty Patent use Private use"},{"location":"articles/licenses/#25-apache-license-20","title":"2.5. Apache License 2.0","text":"<p>Apache License 2.0</p> <p>A permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</p> Permissions Conditions Limitations Commercial use License and copyright notice Liability Distribution State changes Trademark use Modification Warranty Patent use Private use"},{"location":"articles/licenses/#26-mit-license","title":"2.6. MIT License","text":"<p>MIT License (Massachusetts Institute of Technology)</p> <p>A short and permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</p> Permissions Conditions Limitations Commercial use License and copyright notice Liability Distribution Warranty Modification Private use"},{"location":"articles/licenses/#27-bsl-10","title":"2.7. BSL-1.0","text":"<p>BSL-1.0 (Boost Software License 1.0)</p> <p>A permissive license only requiring preservation of copyright and license notices for source (and not binary) distribution. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</p> Permissions Conditions Limitations Commercial use License and copyright notice for source Liability Distribution Warranty Modification Private use"},{"location":"articles/licenses/#28-cc0-10","title":"2.8. CC0 1.0","text":"<p>The Unlicense</p> <p>A license with no conditions whatsoever which dedicates works to the public domain. Unlicensed works, modifications, and larger works may be distributed under different terms and without source code.</p> Permissions Conditions Limitations Commercial use Liability Distribution Warranty Modification Private use"},{"location":"articles/licenses/#3-glossary","title":"3. Glossary","text":"<p>Definitions of the license terms.</p>"},{"location":"articles/licenses/#31-permissions","title":"3.1. Permissions","text":"<p>Open source licenses grant to the public permissions to do things with licensed works which copyright or other <code>intellectual property</code> laws might otherwise disallow.</p> <ul> <li> <p>Commercial use</p> <p>The licensed material and derivatives may be used for commercial purposes.</p> </li> <li> <p>Distribution</p> <p>The licensed material may be distributed.</p> </li> <li> <p>Modification</p> <p>The licensed material may be modified.</p> </li> <li> <p>Patent use</p> <p>This license provides an express grant of patent rights from contributors.</p> </li> <li> <p>Private use</p> <p>The licensed material may be used and modified in private.</p> </li> </ul>"},{"location":"articles/licenses/#32-conditions","title":"3.2. Conditions","text":"<p>Most open source licenses grants of permissions are subject to compliance with conditions.</p> <ul> <li> <p>Disclose source</p> <p>Source code must be made available when the licensed material is distributed.</p> </li> <li> <p>License and copyright notice</p> <p>A copy of the license and copyright notice must be included with the licensed material.</p> </li> <li> <p>License and copyright notice for source</p> <p>A copy of the license and copyright notice must be included with the licensed material in source form, but is not required for binaries.</p> </li> <li> <p>Network use is distribution</p> <p>Users who interact with the licensed material via network are given the right to receive a copy of the source code.</p> </li> <li> <p>Same license</p> <p>Modifications must be released under the same license when distributing the licensed material. In some cases a similar or related license may be used.</p> </li> <li> <p>Same license (file)</p> <p>Modifications of existing files must be released under the same license when distributing the licensed material. In some cases a similar or related license may be used.</p> </li> <li> <p>Same license (library)</p> <p>Modifications must be released under the same license when distributing the licensed material. In some cases a similar or related license may be used, or this condition may not apply to works that use the licensed material as a library.</p> </li> <li> <p>State changes</p> <p>Changes made to the licensed material must be documented.</p> </li> </ul>"},{"location":"articles/licenses/#33-limitations","title":"3.3. Limitations","text":"<p>Most open source licenses also have limitations that usually disclaim warranty and liability, and sometimes expressly exclude patents or trademarks from licenses grants.</p> <ul> <li> <p>Patent use</p> <p>This license explicitly states that it does NOT grant any rights in the patents of contributors.</p> </li> <li> <p>Liability</p> <p>This license includes a limitation of liability.</p> </li> <li> <p>Trademark use</p> <p>This license explicitly states that it does NOT grant trademark rights, even though licenses without such a statement probably do not grant any implicit trademark rights.</p> </li> <li> <p>Warranty</p> <p>This license explicitly states that it does NOT provide any warranty.</p> </li> </ul>"},{"location":"articles/licenses/#4-references","title":"4. References","text":"<ul> <li>Sentenz SPDX convention article.</li> <li>Sentenz license guide article.</li> <li>Open Source Initiative (OSI) licenses article.</li> <li>Choose a licenses article.</li> </ul>"},{"location":"articles/logging-and-monitoring/","title":"Logging and Monitoring","text":"<p>Logging and monitoring are essential components of modern software systems and infrastructure. They play a crucial role in ensuring the reliability, performance, and security of applications and services.</p> <ul> <li>1. Category</li> <li>1.1. Logging<ul> <li>1.1.1. Log Entries</li> <li>1.1.2. Log Sources</li> <li>1.1.3. Log Levels</li> <li>1.1.4. Log Management</li> </ul> </li> <li>1.2. Monitoring<ul> <li>1.2.1. Metrics</li> <li>1.2.2. Alerting</li> <li>1.2.3. Dashboards</li> <li>1.2.4. Proactive Analysis</li> </ul> </li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/logging-and-monitoring/#1-category","title":"1. Category","text":"<p>Effective logging and monitoring practices are crucial for system observability, troubleshooting, and maintaining service levels. They enable organizations to identify and resolve issues quickly, optimize system performance, and ensure the health and reliability of their software systems.</p>"},{"location":"articles/logging-and-monitoring/#11-logging","title":"1.1. Logging","text":"<p>Logging involves the process of recording events, activities, or messages generated by a system or application. These logs serve as a historical record that can be used for troubleshooting, auditing, and analysis.</p>"},{"location":"articles/logging-and-monitoring/#111-log-entries","title":"1.1.1. Log Entries","text":"<p>Log entries (log events or log records) are individual, structured messages in a log file that capture specific events or activities that occur within a system, application, or infrastructure component. Log entries provide a detailed record of these events, often including relevant information such as timestamps, log levels, source details, and message content.</p> <p>Log entries are crucial for monitoring system behavior, troubleshooting issues, and gaining insights into the health and performance of a system. Analyzing log entries collectively helps identify patterns, detect anomalies, track user actions, understand system flow, and identify areas for improvement or optimization.</p> <p>Features of Log Entries:</p> <ol> <li> <p>Timestamp</p> <p>Each log entry includes a timestamp that indicates the date and time when the event occurred. The timestamp helps in understanding the sequence of events and is valuable for troubleshooting and analysis.</p> </li> <li> <p>Log Level</p> <p>Log entries typically have a log level associated with them, indicating the severity or importance of the logged event. Common log levels include <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, and <code>CRITICAL</code>. Log levels help filter and prioritize log entries based on their significance.</p> </li> <li> <p>Source or Origin</p> <p>Log entries include information about the source or origin of the event. This can be the name or identifier of the application, system, or component that generated the log entry. Source information is helpful in identifying the specific part of the system where an event occurred.</p> </li> <li> <p>Message Content</p> <p>The message content of a log entry contains the details or description of the event or activity. It may include relevant information such as error messages, status updates, diagnostic information, user actions, or other contextual data. The message content provides important insights into the event and aids in troubleshooting and analysis.</p> </li> <li> <p>Contextual Information</p> <p>Log entries often include additional contextual information that helps understand the circumstances surrounding the event. This can include data like request parameters, user IDs, session IDs, transaction IDs, IP addresses, or any other relevant information that provides context and aids in root cause analysis.</p> </li> <li> <p>Log Format</p> <p>Log entries follow a specific format that defines the structure and content of each entry. The format may vary depending on the logging framework or library used and can include placeholders or variables that get replaced with actual values at runtime.</p> </li> <li> <p>Log Storage</p> <p>Log entries are stored in log files or log databases for future reference and analysis. The log storage system collects and organizes log entries based on criteria such as time, source, log level, or other relevant attributes. The storage system should be designed to efficiently handle the volume of log data and support easy retrieval and searching.</p> </li> </ol> <p>Example of Log Entries:</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: Application Server\nMessage: Successfully processed request for user ID 12345.\nContext: Request URL: /api/v1/users/12345\n         IP Address: 192.168.0.100\n         User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\n</code></pre> <p>In the example, the log entry provides the following details:</p> <ul> <li> <p>Timestamp</p> <p>The event occurred on June 6, 2023, at 14:30:45.</p> </li> <li> <p>Log Level</p> <p>The log entry has an INFO level, indicating that it represents an informational event.</p> </li> <li> <p>Source</p> <p>The log entry originated from an Application Server.</p> </li> <li> <p>Message</p> <p>The log entry states that a request for user ID 12345 was successfully processed.</p> </li> <li> <p>Context</p> <p>Additional contextual information is provided, including the Request URL (<code>/api/v1/users/12345</code>), the IP address of the client (<code>192.168.0.100</code>), and the User-Agent string of the client's browser.</p> </li> </ul>"},{"location":"articles/logging-and-monitoring/#112-log-sources","title":"1.1.2. Log Sources","text":"<p>Log sources refer to the systems, applications, or infrastructure components that generate log entries. These sources produce logs as a means of capturing and recording events, activities, errors, and other relevant information. Each source may have its own logging mechanism and format.</p> <p>Managing and analyzing logs from diverse sources is important for maintaining system health, detecting issues, troubleshooting problems, and gaining insights into the behavior of complex systems.</p> <p>Common Log Sources:</p> <ol> <li> <p>Application Logs</p> <p>Applications generate logs to record events and activities within their codebase. Application logs can capture information about user actions, errors, warnings, performance metrics, API requests and responses, database interactions, and more.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: Online Marketplace Application\nMessage: User with ID 12345 added item \"iPhone X\" to the shopping cart.\nContext: Session ID: ABCD1234\n         IP Address: 192.168.0.100\n</code></pre> </li> <li> <p>Server Logs</p> <p>Servers, such as web servers or application servers, produce logs that document various aspects of their operation. Server logs can include details about incoming requests, response codes, server errors, resource consumption, security events, and access control information.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: Apache Web Server\nMessage: GET /products/12345 HTTP/1.1 200 OK\nContext: Client IP: 192.168.0.100\n         User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\n</code></pre> </li> <li> <p>Network Device Logs</p> <p>Network devices, including routers, switches, firewalls, and load balancers, generate logs that provide insights into network traffic, connectivity issues, security events, and device performance. Network device logs are crucial for monitoring and troubleshooting network infrastructure.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: CRITICAL\nSource: Firewall\nMessage: Blocked incoming connection attempt from IP address 123.456.789.123 to port 22 (SSH).\nContext: Protocol: TCP\n         Destination Port: 22\n         Source IP: 123.456.789.123\n</code></pre> </li> <li> <p>Operating System Logs</p> <p>Operating systems generate logs that track system-level events and activities. These logs can include information about system startup and shutdown, hardware failures, driver issues, security events, resource utilization, and process-level activities.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: WARNING\nSource: Linux Server\nMessage: Disk space utilization exceeded 90% on /var partition.\nContext: Filesystem: /var\n         Utilization: 92%\n</code></pre> </li> <li> <p>Database Logs</p> <p>Databases produce logs that record transactions, queries, errors, and other database-related events. Database logs are critical for tracking database performance, identifying slow queries, diagnosing issues, and ensuring data integrity.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: MySQL Database\nMessage: Executed query: SELECT * FROM users WHERE user_id = 12345\nContext: Execution Time: 3.215 seconds\n         Rows Returned: 1\n</code></pre> </li> <li> <p>Security Logs</p> <p>Security logs, including logs from intrusion detection systems (IDS), intrusion prevention systems (IPS), firewalls, and security information and event management (SIEM) systems, capture security-related events and alerts. These logs help monitor potential security breaches, identify malicious activities, and investigate incidents.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: CRITICAL\nSource: Intrusion Detection System (IDS)\nMessage: Detected a possible SQL injection attempt on web application.\nContext: Source IP: 123.456.789.123\n         Target URL: /products/12345?name=' OR '1'='1\n</code></pre> </li> <li> <p>Application Performance Monitoring (APM) Logs</p> <p>APM tools generate logs that focus on application performance metrics and monitoring. APM logs provide detailed insights into application behavior, transaction traces, response times, code-level performance, and resource utilization.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: APM Tool\nMessage: Transaction completed successfully. Transaction ID: ABC123DEF\nContext: Response Time: 123 ms\n         CPU Utilization: 30%\n         Memory Utilization: 512 MB\n</code></pre> </li> <li> <p>Cloud Service Logs</p> <p>Cloud service providers offer logs specific to their platforms and services. These logs can include information about virtual machines, container instances, cloud storage, serverless functions, and other cloud-based resources. Cloud service logs are essential for monitoring, auditing, and troubleshooting applications hosted in the cloud.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: AWS Lambda Function\nMessage: Function execution completed. Request ID: 1234567890\nContext: Duration: 250 ms\n         Memory Allocated: 128 MB\n</code></pre> </li> <li> <p>Infrastructure Logs</p> <p>Infrastructure components like load balancers, caching servers, messaging systems, and container orchestrators generate logs that help monitor and manage the underlying infrastructure. These logs provide insights into resource allocation, scaling events, service discovery, container lifecycle, and more.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: Kubernetes Cluster\nMessage: Scaled up application deployment \"web-app\" replicas.\nContext: Namespace: default\n         Replica Count: 4\n</code></pre> </li> <li> <p>Middleware and Framework Logs</p> <p>Middleware software, such as message queues, caching systems, and application frameworks, generate logs specific to their functionality. These logs capture information about middleware operations, event processing, message queues, cache hits/misses, and framework-level events.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: ERROR\nSource: RabbitMQ Message Queue\nMessage: Message lost from queue \"orders\" for processing.\nContext: ID: 3454353\n</code></pre> </li> </ol>"},{"location":"articles/logging-and-monitoring/#113-log-levels","title":"1.1.3. Log Levels","text":"<p>Log levels are used to classify the severity or importance of log entries. They provide a standardized way to categorize log messages based on their significance, allowing developers and system administrators to filter and prioritize logs during analysis and troubleshooting.</p> <p>The specific log levels and their interpretation may vary across logging frameworks, libraries, or programming languages. It's important to define and adhere to a consistent log level strategy within an application or system to ensure logs are appropriately classified, enabling effective log analysis and troubleshooting.</p> <p>Common Log Levels:</p> <p>NOTE Listed in increasing order of severity.</p> <ol> <li> <p>DEBUG</p> <p>The DEBUG log level is used for detailed debugging information. It provides granular information about the application's internal operations, variable values, and execution paths. DEBUG-level logs are typically used during development and are not intended for production environments.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: DEBUG\nSource: Online Marketplace Application\nMessage: Calculate the shopping cart of the user with ID 12345.\nContext: Function: calculatePrice()\n         Product ID: 12345\n</code></pre> </li> <li> <p>INFO</p> <p>The INFO log level represents informational messages that indicate the normal operation of an application. INFO-level logs provide important status updates, such as successful operations, major milestones, or significant events. They help track the overall flow and progress of the application.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: INFO\nSource: Online Marketplace Application\nMessage: User logged in successfully.\nContext: User ID: 98765\n         IP Address: 192.168.0.100\n</code></pre> </li> <li> <p>WARNING</p> <p>The WARNING log level indicates potentially harmful or unexpected events that may cause issues but do not necessarily result in errors. These logs highlight abnormal or non-critical conditions that require attention. Examples include deprecated features, configuration issues, or potential performance bottlenecks.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: WARNING\nSource: Online Marketplace Application\nMessage: Deprecated feature is being used.\nContext: Deprecated Method: calculatePrice()\n         Upgraded Method: getPrice()\n</code></pre> </li> <li> <p>ERROR</p> <p>The ERROR log level signifies errors or exceptions that occur during the execution of an application. These logs represent problems that prevent the application from functioning correctly or as expected. Error-level logs are important for troubleshooting and identifying issues that need immediate attention.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: ERROR\nSource: MySQL Database\nMessage: Database connection failed. Unable to establish a connection to the database server.\nContext: Database URL: jdbc:mysql://localhost:3306/mydb\n         Retry Attempts: 4\n</code></pre> </li> <li> <p>CRITICAL</p> <p>The CRITICAL log level indicates severe errors or failures that result in application instability or significant loss of functionality. These logs represent critical issues that require immediate intervention to prevent system crashes, data corruption, or major disruptions. CRITICAL-level logs often trigger alerts or notifications for urgent action.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: CRITICAL\nSource: Linux Server\nMessage: Insufficient disk space.\nContext: Disk Space: 10%\n         Partition: /var/logs\n</code></pre> </li> <li> <p>FATAL</p> <p>The FATAL log level represents the most severe and unrecoverable errors. It indicates situations where the application cannot continue its normal execution and must terminate abruptly. FATAL-level logs are typically used in exceptional cases where the application's core functionality is compromised, leading to system failures.</p> <pre><code>Timestamp: 2023-06-06 14:30:45\nLog Level: FATAL\nSource: Online Marketplace Application\nMessage: Application encountered an unrecoverable error. Terminating execution.\nContext: Error Code: 500\n         Error Message: Internal Server Error\n</code></pre> </li> </ol>"},{"location":"articles/logging-and-monitoring/#114-log-management","title":"1.1.4. Log Management","text":"<p>Log management refers to the process of collecting, storing, analyzing, and acting upon log data generated by various systems and applications within an organization. It involves implementing strategies, tools, and best practices to effectively manage and utilize log information for operational insights, troubleshooting, compliance, and security purposes.</p> <p>Effective log management provides organizations with valuable insights into their systems, helps in identifying and resolving issues, enables proactive monitoring, enhances security incident response, and supports compliance requirements. It is an essential component of a comprehensive IT infrastructure management strategy.</p> <p>Centralized log management solutions, such as Elasticsearch, Splunk, or ELK stack (Elasticsearch, Logstash, and Kibana), are commonly used to aggregate and process logs from multiple sources.</p> <p>Features of Log Management:</p> <ol> <li> <p>Log Collection</p> <p>Log collection involves gathering log data from different sources, such as applications, servers, network devices, databases, and security systems. This can be achieved through various methods, including log file ingestion, log streaming, log agent-based collection, or leveraging centralized log management solutions.</p> <p>An organization utilizes log collection agents installed on servers and applications to collect log data. These agents send logs to a centralized log management system.</p> </li> <li> <p>Log Storage</p> <p>Log storage is the process of storing log data in a centralized repository or distributed storage systems. Log storage should be designed to handle large volumes of logs efficiently and securely. It may involve using technologies like log databases, log file systems, or cloud-based storage solutions.</p> </li> <li> <p>Log Retention</p> <p>Log retention defines the duration for which log data is stored. Retention periods may vary based on legal requirements, compliance regulations, business needs, or security considerations. Organizations should establish log retention policies to ensure they retain logs for an appropriate duration.</p> </li> <li> <p>Log Analysis</p> <p>Log analysis involves parsing, searching, and analyzing log data to gain insights into system behavior, performance, security incidents, and operational issues. It may include real-time log monitoring, log aggregation, log correlation, and applying various analysis techniques to identify patterns, anomalies, or trends.</p> <p>The log management system performs real-time log analysis, using tools like Splunk or ELK (Elasticsearch, Logstash, Kibana). It parses and indexes log data, allowing for advanced searching, filtering, and correlation of logs across multiple sources.</p> </li> <li> <p>Log Visualization</p> <p>Log visualization helps in presenting log data in a more understandable and interactive format. Visualization tools can create charts, graphs, dashboards, and reports that provide a visual representation of log data, aiding in easier interpretation, monitoring, and analysis.</p> <p>The log management system generates interactive dashboards and visualizations using Kibana or Grafana. These visualizations provide insights into system performance, error rates, and security events, enabling easy monitoring and analysis.</p> </li> <li> <p>Log Alerting</p> <p>Log alerting allows organizations to set up notifications or alerts based on specific log events or conditions. It enables timely notifications of critical events, errors, security breaches, or abnormal behavior. Log alerting helps in proactive monitoring and immediate response to critical incidents.</p> </li> <li> <p>Log Compliance and Audit</p> <p>Log management plays a crucial role in meeting compliance requirements and facilitating audits. Logs can be used to demonstrate adherence to regulatory standards, track user activities, maintain data integrity, and provide evidence in case of security incidents or legal investigations.</p> <p>The log management system ensures compliance with industry regulations such as <code>PCI-DSS</code> or <code>HIPAA</code> by collecting and retaining logs in a tamper-evident manner. Logs are available for audits and can be easily searched and retrieved when required.</p> </li> <li> <p>Log Security</p> <p>Log data itself is sensitive and needs to be protected. Implementing log security measures, such as log encryption, access controls, and secure transmission protocols, helps ensure the confidentiality, integrity, and availability of log data.</p> </li> <li> <p>Log Archiving</p> <p>Log archiving involves moving older or less frequently accessed log data to long-term storage or offline backups. Archiving helps optimize storage resources and enables retrieval of historical log data when required for compliance, forensic analysis, or trend analysis.</p> </li> <li> <p>Log Governance</p> <p>Log governance encompasses defining policies, standards, and processes for log management within an organization. It includes roles and responsibilities, log management guidelines, and procedures for log collection, retention, analysis, and access control.</p> </li> </ol>"},{"location":"articles/logging-and-monitoring/#12-monitoring","title":"1.2. Monitoring","text":"<p>Monitoring involves the continuous observation and measurement of various aspects of a system's performance, availability, and health. It provides real-time insights into the system's behavior and helps detect issues and anomalies.</p>"},{"location":"articles/logging-and-monitoring/#121-metrics","title":"1.2.1. Metrics","text":"<p>Metrics are quantitative measurements used to track and analyze the performance, behavior, and health of systems, applications, or processes. They provide valuable insights into various aspects of a system's operation and help monitor key indicators to understand its overall state. Metrics help organizations to optimize their operations, troubleshoot issues, and drive continuous improvement.</p> <p>Features of Metrics:</p> <ol> <li> <p>Types of Metrics</p> <p>Metrics can be broadly classified into several categories:</p> <ul> <li> <p>System Metrics</p> <p>These metrics provide information about the underlying hardware and infrastructure, such as CPU usage, memory consumption, disk I/O, and network traffic.</p> </li> <li> <p>Application Metrics</p> <p>These metrics focus on the behavior and performance of the application itself. Examples include response time, throughput, error rates, and resource utilization.</p> </li> <li> <p>Business Metrics</p> <p>Business metrics measure the impact of the system or application on business goals and objectives. They can include conversion rates, revenue, customer satisfaction scores, or any other metric directly tied to business performance.</p> </li> </ul> </li> <li> <p>Metric Collection</p> <p>Metrics are collected through instrumentation, which involves adding code or using specialized tools to capture and record relevant data points. This data is typically aggregated over time intervals and stored in a time-series database or monitoring system.</p> </li> <li> <p>Metric Visualization</p> <p>Metrics are often visualized using charts, graphs, dashboards, or other visual representations. Visualization tools like Grafana or Kibana help present the collected metrics in an easily understandable format, enabling users to monitor trends, identify anomalies, and make informed decisions.</p> </li> <li> <p>Metric Analysis</p> <p>Metrics can be analyzed to gain insights into system behavior and performance. Analysis techniques may involve identifying patterns, comparing historical data, setting thresholds for alerting, and detecting correlations between different metrics.</p> </li> <li> <p>Alerting</p> <p>Metrics can trigger alerts based on predefined thresholds or conditions. Alerting mechanisms notify system administrators or other stakeholders when a metric value exceeds or falls below a certain threshold, indicating a potential issue or abnormal behavior.</p> </li> <li> <p>Metric Retention</p> <p>Metrics are typically stored and retained for a defined period, allowing historical analysis and trend identification. Retention policies determine how long metrics are retained, considering factors such as storage capacity, compliance requirements, and analysis needs.</p> </li> <li> <p>Metric Exporting</p> <p>Metrics can be exported to external systems or tools for further analysis, long-term storage, or integration with other monitoring platforms. Common export formats include Prometheus exposition format, JSON, or CSV.</p> </li> <li> <p>Metric Standardization</p> <p>Establishing consistent metric naming conventions, units, and formats across systems and applications improves interoperability and simplifies metric analysis and visualization.</p> </li> <li> <p>Metric Aggregation</p> <p>Metrics are often aggregated to provide higher-level summaries or composite metrics. Aggregation techniques include averaging, summing, min/max values, percentiles, or other statistical calculations.</p> </li> <li> <p>Metric-driven Decision Making</p> <p>Metrics play a crucial role in data-driven decision making. By analyzing metrics, organizations can identify performance bottlenecks, optimize resource allocation, detect anomalies, and make informed decisions to improve system efficiency, user experience, and overall business outcomes.</p> </li> </ol>"},{"location":"articles/logging-and-monitoring/#122-alerting","title":"1.2.2. Alerting","text":"<p>Alerting is a crucial component of monitoring systems and plays a vital role in notifying system administrators or other stakeholders about potential issues or abnormal conditions that require attention. It helps ensure timely response and remediation actions, minimizing downtime and maintaining the health and availability of systems.</p> <p>Alerting enables proactive monitoring and quick response to abnormal conditions, helping organizations identify and address issues promptly. By configuring effective alert rules, defining appropriate notification channels, and establishing escalation policies, organizations can maintain system availability, minimize downtime, and provide efficient incident response.</p> <p>Features of Alerting:</p> <ol> <li> <p>Alert Rules</p> <p>Alert rules define the conditions or thresholds that trigger an alert. These rules are based on specific metrics, events, or log entries. For example, an alert rule could be defined to trigger when CPU usage exceeds 90% for a sustained period.</p> </li> <li> <p>Alert Conditions</p> <p>Alert conditions specify the criteria for triggering an alert. They can involve comparisons, aggregations, or complex conditions based on multiple metrics or events. For example, an alert condition may be triggered when the average response time exceeds a certain threshold and the number of errors exceeds a specified limit.</p> </li> <li> <p>Notification Channels</p> <p>Notification channels determine how alerts are delivered to the intended recipients. Common notification channels include email, SMS, instant messaging platforms, or integrations with incident management tools like PagerDuty or Slack. Multiple channels can be configured to ensure alerts reach the right people through their preferred communication channels.</p> </li> <li> <p>Severity Levels</p> <p>Alerts often have different severity levels assigned to them, indicating the urgency or impact of the issue. Severity levels help prioritize alerts and determine the appropriate response time. For example, a critical severity level may require an immediate response, while a low severity level may allow for a delayed response.</p> <p>Common Severity Levels:</p> <ul> <li> <p>Critical</p> <p>Represents the highest level of severity. Critical issues indicate a complete system failure, major service outage, or a critical security breach that requires immediate attention. The impact is severe, resulting in significant disruption or loss of functionality.</p> </li> <li> <p>High</p> <p>Indicates a major issue that has a significant impact on the system or service. High severity issues may cause service degradation, affect multiple users, or result in a loss of critical functionality. Urgent action is required to mitigate the impact.</p> </li> <li> <p>Medium</p> <p>Denotes an issue that has a moderate impact on the system or service. Medium severity issues may cause some disruption or affect a limited number of users. While they are not as critical as high severity issues, they still require attention and timely resolution.</p> </li> <li> <p>Low</p> <p>Represents a minor issue with a minimal impact on the system or service. Low severity issues may be cosmetic, have limited functionality impact, or affect only a few users. They can be addressed during regular maintenance cycles or addressed at a lower priority.</p> </li> <li> <p>Informational</p> <p>This level is used for informational messages, notifications, or events that do not require immediate action. They provide additional context or non-urgent updates and are not considered as issues or incidents.</p> </li> </ul> </li> <li> <p>Escalation Policies</p> <p>Escalation policies define a sequence of actions or steps to be followed when an alert is triggered. This can involve escalating the alert to different individuals or teams based on predefined rules or time-based escalations. Escalation policies ensure that alerts are not overlooked and that the appropriate parties are notified in a timely manner.</p> </li> <li> <p>Alert Suppression</p> <p>Alert suppression allows for the temporary suppression of alerts during planned maintenance activities or known periods of high load or instability. This helps prevent unnecessary noise and alert fatigue when certain conditions are expected and can be safely ignored.</p> </li> <li> <p>Alert Deduplication</p> <p>Alert deduplication ensures that only unique and actionable alerts are delivered, even if multiple instances of the same issue occur within a short period. This prevents overwhelming recipients with duplicate alerts and focuses on the underlying root cause.</p> </li> <li> <p>Alert History and Tracking</p> <p>A system should maintain a history of triggered alerts, including relevant details such as timestamps, alert rules, and the actions taken. This history allows for post-incident analysis, troubleshooting, and tracking of the alert lifecycle.</p> </li> <li> <p>Integration with Monitoring Systems</p> <p>Alerting is typically integrated with monitoring systems or tools that collect and analyze metrics, logs, or events. These systems continuously evaluate data against defined alert rules and trigger notifications when conditions are met.</p> </li> <li> <p>Testing and Maintenance</p> <p>Regular testing and maintenance of alerting systems are essential to ensure they are functioning correctly. This involves verifying alerting configurations, reviewing escalation policies, and performing periodic tests to validate that alerts are being triggered and delivered as expected.</p> </li> </ol>"},{"location":"articles/logging-and-monitoring/#123-dashboards","title":"1.2.3. Dashboards","text":"<p>Dashboards are powerful tools for data visualization and analysis, enabling users to monitor and understand system performance, track metrics, and make informed decisions. By presenting data in a visually appealing and accessible format, dashboards facilitate effective communication and enable stakeholders to stay informed about the health and status of systems or applications, and track key performance indicators (KPIs).</p> <p>Features of Dashboards:</p> <ol> <li> <p>Data Visualization</p> <p>Dashboards use charts, graphs, tables, and other visual elements to present data in a visually appealing and informative manner. Common visualizations include line charts, bar charts, pie charts, gauges, heatmaps, and tables.</p> </li> <li> <p>Customization</p> <p>Dashboards can be customized based on the specific needs of users or organizations. Users can choose which metrics or data points to display, how they are organized, and the visualizations used. Customization options often include resizing, rearranging, and adding or removing elements.</p> </li> <li> <p>Real-time or Historical Data</p> <p>Dashboards can display real-time data, continuously updating metrics and indicators as new data is received. They can also show historical data, allowing users to analyze trends and patterns over time.</p> </li> <li> <p>Aggregated and Drill-down Views</p> <p>Dashboards can provide both high-level aggregated views and drill-down capabilities for more detailed analysis. Users can start with an overview of the system or application and then drill down into specific metrics or components to gain deeper insights.</p> </li> <li> <p>Multiple Data Sources</p> <p>Dashboards can pull data from various sources, including monitoring systems, databases, APIs, log files, and external services. This allows for comprehensive monitoring and analysis by consolidating data from different sources into a single view.</p> </li> <li> <p>Alerts and Notifications</p> <p>Dashboards often include alerting capabilities, displaying alerts or notifications when predefined thresholds or conditions are met. This helps users quickly identify and respond to critical events or anomalies.</p> </li> <li> <p>Dashboard Sharing and Collaboration</p> <p>Dashboards can be shared with team members or stakeholders, enabling collaboration and facilitating a common understanding of system performance and metrics. Sharing options may include exporting or embedding dashboards in other platforms or granting access to specific users or user groups.</p> </li> <li> <p>Mobile and Responsive Design</p> <p>Dashboards are often designed to be mobile-friendly and responsive, allowing users to access and view them on different devices such as smartphones or tablets. Responsive design ensures optimal viewing and usability across various screen sizes.</p> </li> <li> <p>Data Driven Decision Making</p> <p>Dashboards empower users to make data-driven decisions by providing real-time or historical insights into the performance, trends, and anomalies of systems or applications. They help identify issues, track key performance indicators, and optimize operations.</p> </li> <li> <p>Continuous Improvement</p> <p>Dashboards can be iteratively improved based on user feedback, changing requirements, or evolving business needs. Regular evaluation and refinement ensure that dashboards remain relevant, effective, and aligned with organizational goals.</p> </li> </ol>"},{"location":"articles/logging-and-monitoring/#124-proactive-analysis","title":"1.2.4. Proactive Analysis","text":"<p>Proactive analysis is an approach to data analysis that focuses on identifying potential issues or opportunities before they manifest or become apparent. It involves leveraging data, metrics, and analytics to gain insights, detect patterns, and make predictions about future events or outcomes. Proactive analysis aims to prevent problems, optimize processes, and drive proactive decision-making.</p> <p>Proactive analysis empowers organizations to be ahead of the curve by leveraging data and analytics to identify and address potential issues or opportunities. By moving from reactive to proactive decision-making, organizations can gain a competitive edge, optimize operations, and drive continuous improvement.</p> <p>Features of Proactive Analysis:</p> <ol> <li> <p>Data Collection</p> <p>Proactive analysis starts with collecting relevant data from various sources, including monitoring systems, logs, databases, user interactions, or external data feeds. The data may include metrics, events, logs, user behavior, or other relevant information.</p> </li> <li> <p>Data Exploration</p> <p>Exploring and understanding the data is an important step in proactive analysis. This involves examining the data for patterns, trends, correlations, or anomalies that may provide insights or indicate potential issues or opportunities.</p> </li> <li> <p>Data Modeling</p> <p>Proactive analysis often involves creating models or algorithms to analyze the data. This can include statistical models, machine learning algorithms, predictive models, or anomaly detection algorithms. The models are trained or calibrated using historical data and then applied to new or real-time data for analysis.</p> </li> <li> <p>Pattern Detection</p> <p>Proactive analysis aims to identify patterns or trends in the data that can provide insights into potential issues or opportunities. By detecting patterns early, organizations can take proactive measures to address emerging issues or capitalize on emerging trends.</p> </li> <li> <p>Predictive Analytics</p> <p>Predictive analytics is a key component of proactive analysis. It involves using historical data and statistical techniques to make predictions or forecasts about future events or outcomes. Predictive analytics can help anticipate system failures, customer behavior, market trends, or other relevant factors.</p> </li> <li> <p>Alerts and Notifications</p> <p>Proactive analysis can generate alerts or notifications when specific conditions or thresholds are met. These alerts can be based on predictive models, anomaly detection, or predefined rules. By receiving timely alerts, stakeholders can take proactive actions to mitigate risks or leverage opportunities.</p> </li> <li> <p>Root Cause Analysis</p> <p>When an issue occurs, proactive analysis can help identify the root causes by analyzing historical data, patterns, or correlations. By understanding the underlying causes, organizations can address the root issues and prevent similar problems from recurring in the future.</p> </li> <li> <p>Continuous Monitoring and Improvement</p> <p>Proactive analysis is an ongoing process that requires continuous monitoring, evaluation, and refinement. As new data becomes available and systems evolve, proactive analysis should be adapted and updated to stay effective and relevant.</p> </li> <li> <p>Collaboration and Action</p> <p>Proactive analysis involves collaboration between different stakeholders, including data analysts, domain experts, and decision-makers. Insights and findings from proactive analysis should be communicated and translated into actionable steps to drive proactive decision-making and optimization.</p> </li> <li> <p>Business Impact</p> <p>The ultimate goal of proactive analysis is to drive positive business impact. By anticipating issues, optimizing processes, and capitalizing on opportunities, organizations can improve efficiency, reduce costs, enhance customer satisfaction, and achieve better business outcomes.</p> </li> </ol>"},{"location":"articles/logging-and-monitoring/#2-principles","title":"2. Principles","text":"<p>NOTE Logging and monitoring are not one-time tasks but ongoing processes. Regularly review and update your logging and monitoring strategies to align with evolving business needs, technology changes, and security requirements.</p> <ul> <li> <p>Collect Relevant Data</p> <p>Focus on collecting and logging relevant and meaningful data. Determine the key events, activities, metrics, and logs that provide valuable insights into the system's behavior, performance, and security. Avoid logging excessive or irrelevant data that can clutter the logs and make analysis more challenging.</p> </li> <li> <p>Consistent Logging Standards</p> <p>Establish consistent logging standards and guidelines across your applications and systems. Define a common log format, including the structure, message content, and log levels. Consistency in logging makes it easier to aggregate and analyze logs from different sources.</p> </li> <li> <p>Centralized Logging</p> <p>Implement a centralized log management system to aggregate logs from various sources into a centralized repository. Centralized logging simplifies log storage, searchability, and analysis. It also enables cross-system correlation and provides a holistic view of the entire infrastructure.</p> </li> <li> <p>Real-time Monitoring</p> <p>Implement real-time monitoring to detect issues and anomalies promptly. Monitor key metrics and set up alerts or notifications when predefined thresholds are breached. Real-time monitoring enables proactive identification and resolution of issues, minimizing their impact on system performance and availability.</p> </li> <li> <p>Logging for Auditing and Compliance</p> <p>Ensure that your logging practices align with auditing and compliance requirements specific to your industry or organization. Log critical security events, access attempts, and other activities that need to be audited. Consider data privacy regulations and the appropriate levels of log anonymization or encryption.</p> </li> <li> <p>Automation and Integration</p> <p>Leverage automation and integrations to streamline logging and monitoring processes. Use tools and frameworks that allow for seamless integration with your applications, systems, and infrastructure components. Automate log collection, aggregation, and analysis to reduce manual effort and increase efficiency.</p> </li> <li> <p>Regular Log Analysis</p> <p>Perform regular log analysis to gain insights into system behavior, identify trends, and detect anomalies or issues. Analyze logs for patterns, errors, warning signs, or security breaches. Log analysis helps in troubleshooting, capacity planning, performance optimization, and identifying areas for improvement.</p> </li> <li> <p>Continuous Improvement</p> <p>Continuously evaluate and improve your logging and monitoring practices. Review the effectiveness of your logs, metrics, and monitoring strategies. Incorporate feedback from incident responses and learn from past experiences to refine your logging and monitoring approach.</p> </li> </ul>"},{"location":"articles/logging-and-monitoring/#3-best-practice","title":"3. Best Practice","text":"<p>Best practices to establish robust logging and monitoring practices that enable efficient troubleshooting, system optimization, and proactive issue detection and resolution.</p> <ul> <li> <p>Define Clear Objectives</p> <p>Clearly define the objectives and goals of your logging and monitoring strategy. Identify the key metrics, events, and logs that are crucial for your system's performance, availability, and security. Align your logging and monitoring practices with your overall business objectives.</p> </li> <li> <p>Start with a Logging Framework</p> <p>Utilize a logging framework or library specific to your programming language or platform. These frameworks provide standardized logging capabilities, such as log levels, structured log entries, and integration with logging systems. Examples include Log4j for Java, Serilog for .NET, or Winston for Node.js.</p> </li> <li> <p>Use Descriptive Log Messages</p> <p>Write clear and descriptive log messages that provide valuable context and information. Include relevant details such as timestamps, error codes, request identifiers, and user context. This helps in troubleshooting and understanding the flow of events.</p> </li> <li> <p>Implement Log Levels Appropriately</p> <p>Utilize different log levels (e.g., <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>) effectively based on the severity and importance of the logged events. Avoid excessive logging at lower levels that may impact system performance and storage.</p> </li> <li> <p>Log Errors and Exceptions</p> <p>Log errors, exceptions, and stack traces when they occur. Include relevant error details, error messages, and stack trace information. These logs are invaluable for troubleshooting and identifying the root causes of issues.</p> </li> <li> <p>Include Request and Response Information</p> <p>In web-based applications, log relevant information about incoming requests and outgoing responses. Include details like request headers, URL, request parameters, response codes, and response times. This helps in tracking and analyzing user interactions.</p> </li> <li> <p>Consider Log Rotation and Retention</p> <p>Define log rotation and retention policies to manage log storage effectively. Implement mechanisms to limit log file sizes and archive or delete old logs based on predefined criteria. This ensures that logs are available when needed while optimizing storage usage.</p> </li> <li> <p>Monitor Key Performance Metrics</p> <p>Identify and monitor critical performance metrics specific to your application or system. This may include CPU usage, memory consumption, response times, error rates, and throughput. Set appropriate thresholds and alerts to proactively identify performance issues.</p> </li> <li> <p>Establish Effective Alerting</p> <p>Configure alerting mechanisms to notify the relevant teams or individuals when predefined conditions or thresholds are breached. Ensure that alerts are actionable, concise, and provide sufficient information to initiate timely investigation and remediation.</p> </li> <li> <p>Regularly Review and Analyze Logs</p> <p>Allocate time to regularly review and analyze logs for patterns, trends, and anomalies. This can help identify system issues, security breaches, performance bottlenecks, or areas for improvement. Use log analysis tools and techniques to streamline the process.</p> </li> <li> <p>Secure Log Transmission and Storage</p> <p>Ensure that log data is transmitted and stored securely. Encrypt log data during transmission and consider encryption at rest. Follow security best practices to protect log data from unauthorized access or tampering.</p> </li> <li> <p>Collaborate and Share Insights</p> <p>Foster collaboration between development, operations, and security teams. Share insights and findings from log analysis to facilitate a better understanding of the system's behavior, identify areas for improvement, and drive continuous improvement efforts.</p> </li> </ul>"},{"location":"articles/logging-and-monitoring/#4-terminology","title":"4. Terminology","text":"<p>Understanding terms related to logging and monitoring help to navigate and effectively utilize logging and monitoring practices.</p> <ul> <li> <p>Log</p> <p>A log is a record of events or activities generated by a system, application, or infrastructure component. Logs capture information such as timestamps, log levels, source details, and message content.</p> </li> <li> <p>Log Entry</p> <p>A log entry represents a single event or activity recorded in a log. It typically includes relevant information such as a timestamp, severity level, log message, and additional context.</p> </li> <li> <p>Log Level</p> <p>Log levels indicate the severity or importance of a log entry. Common log levels include <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, and <code>CRITICAL</code>. Developers can adjust the log level to control the verbosity of logs.</p> </li> <li> <p>Log Source</p> <p>A log source refers to the system, application, or component that generates logs. Examples include application logs, server logs, network device logs, database logs, and security logs.</p> </li> <li> <p>Log Management</p> <p>Log management involves the collection, storage, analysis, and retrieval of logs. It encompasses practices and tools used to aggregate logs from various sources, store them in a central repository, and facilitate efficient searching, filtering, and analysis of log data.</p> </li> <li> <p>Centralized Logging</p> <p>Centralized logging is the practice of aggregating logs from multiple sources into a central repository or log management system. It allows for easy log storage, analysis, and correlation, providing a unified view of log data across an infrastructure.</p> </li> <li> <p>Monitoring</p> <p>Monitoring involves the continuous observation and measurement of system behavior, performance, and health. It typically involves collecting and analyzing metrics and generating alerts or notifications based on predefined thresholds or conditions.</p> </li> <li> <p>Metrics</p> <p>Metrics are numerical measurements that represent various aspects of system performance, behavior, or resource utilization. Examples of metrics include CPU usage, memory consumption, response times, error rates, throughput, and network latency.</p> </li> <li> <p>Alerting</p> <p>Alerting is the process of generating notifications or alerts when certain predefined conditions or thresholds are breached. Alerts notify system administrators or DevOps teams, enabling them to respond promptly to issues or anomalies.</p> </li> <li> <p>Dashboard</p> <p>A dashboard is a visual representation of real-time and historical data, often presented in the form of graphs, charts, or other visual elements. Dashboards provide a concise overview of key metrics and help stakeholders gain insights into system behavior and performance.</p> </li> <li> <p>Anomaly Detection</p> <p>Anomaly detection is the process of identifying abnormal behavior or patterns in system metrics or logs. It involves using statistical analysis or machine learning algorithms to detect deviations from normal patterns, which may indicate potential issues or security breaches.</p> </li> <li> <p>Performance Monitoring</p> <p>Performance monitoring focuses on measuring and analyzing metrics related to system performance, resource utilization, and response times. It helps identify performance bottlenecks, optimize system performance, and ensure efficient resource allocation.</p> </li> </ul>"},{"location":"articles/logging-and-monitoring/#5-references","title":"5. References","text":"<ul> <li>Github Prometheus repository.</li> <li>Github Grafana repository.</li> </ul>"},{"location":"articles/merging-strategies/","title":"Merging Strategies // TODO","text":""},{"location":"articles/merging-strategies/#references","title":"References","text":"<ul> <li>W3docs merge strategies article.</li> <li>GitHub merge methods article.</li> <li>Azure DevOps squash merge article.</li> <li>GitLab fast-forward merge requests article.</li> <li>Microsoft pull requests with rebase article.</li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/","title":"Monorepo vs Polyrepo // TODO","text":"<p>Monorepo means using one repository that contains many projects, and polyrepo means using a repository per project. This page discusses the similarities and differences, and has advice and opinions on both.</p> <ul> <li>Introduction</li> <li>What is monorepo?</li> <li>What is polyrepo?</li> <li>Comparisons</li> <li>Key similarities</li> <li>Key differences</li> <li>Tooling</li> <li>Bazel</li> <li>Lerna</li> <li>OctoLinker</li> <li>Monorepo scaling</li> <li>Monorepo scaling problem</li> <li>Monorepo scaling mitigations</li> <li>Monorepo scaling metrics</li> <li>Proponents of monorepo</li> <li>If components need to release together, then use a monorepo</li> <li>If components need to share common code, then use a monorepo</li> <li>I\u2019ve found monorepos to be extremely valuable in an less-mature, high-churn codebase</li> <li>A common mission</li> <li>Proponents of polyrepo</li> <li>If tech's biggest names use a monorepo, should we do the same?</li> <li>Coupling between unrelated projects</li> <li>Visible organization</li> <li>Opinions about splitting</li> <li>Splitting one repo is easier than combining multiple repos</li> <li>Splitting may be too fine</li> <li>Opinions about balances</li> <li>It's a social problem in how you manage boundaries</li> <li>Challenges of monorepo and polyrepo</li> <li>On-premise applications or desktop applications</li> <li>Opinions about alternatives</li> <li>Could you get the best of both worlds by having a monorepo of submodules?</li> <li>Hybrid of \"many repos\"</li> <li>Prediction of a new type of VCS</li> </ul> <p>See:</p> <ul> <li>SCM at Facebook</li> <li>SCM at Google</li> <li>Why Google stores billions of lines of code in a single repository (2016) (acm.org)</li> <li>Scaling Mercurial at Facebook</li> <li>Cthulhu: Organizing Go Code in a Scalable Repo</li> </ul> <p>Opinion posts:</p> <ul> <li>Monorepos: Please don\u2019t! - By Matt Klein</li> <li>Monorepos and the Fallacy of Scale - By Paulus Esterhazy</li> </ul> <p>Hacker News discussions:</p> <ul> <li>Monorepos: Please don\u2019t!</li> <li>Monorepos and the Fallacy of Scale</li> </ul> <p>Credits:</p> <ul> <li>Opinions and comments on this page are thanks to many people on various discussion websites, such as Hacker News, and lightly edited for clarity.</li> <li>If you're the author of an opinion here, and would like to attribute it, or explain more, please let us know and we'll give you commit access.</li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/#introduction","title":"Introduction","text":""},{"location":"articles/monorepo-vs-polyrepo/#what-is-monorepo","title":"What is monorepo?","text":"<p>Monorepo is a nickname that means \"using one repository for the source code management version control system\".</p> <ul> <li>A monorepo architecture means using one repository, rather than multiple repositories.</li> <li>For example, a monorepo can use one repo that contains a directory for a web app project, a directory for a mobile app project, and a directory for a server app project.</li> <li>Monorepo is also known as one-repo or uni-repo.</li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/#what-is-polyrepo","title":"What is polyrepo?","text":"<p>Polyrepo is a nickname that means \"using multiple repostories for the source code management version control system\".</p> <ul> <li>A polyrepo architecture means using multiple repositories, rather than one repository.</li> <li>For example, a polyrepo can use a repo for a web app project, a repo for a mobile app project, and a repo for a server app project.</li> <li>Polyrepo is also known as many-repo or multi-repo.</li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/#comparisons","title":"Comparisons","text":""},{"location":"articles/monorepo-vs-polyrepo/#key-similarities","title":"Key similarities","text":"<p>Key similarities between monorepo and polyrepo:</p> <ul> <li>Both architectures ultimately track the same source code files, and do it by using source code management (SCM) version control systems (VCS) such as git or mercurial.</li> <li>Both architectures are proven successful for projects of all sizes.</li> <li>Both architectures are straightforward to implement using any typical SCM VCS, up to a scaling limit.</li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/#key-differences","title":"Key differences","text":"<p>Key differences between monorepo and polyrepo, summarized from many proponents, and intending to highlight typical differences between a typical monorepo and a typical polyrepo.</p> Monorepo Polyrepo Contents Typically a repo contains multiple projects, programming languages, packaging processes, etc. Typically a repo contains one project, programming language, packaging process, etc. Projects Manages projects in one repository, together, holistically. Manages projects in multiple repositories, separately, independently. Workflows Enables workflows in all projects simultaneously, all within the monorepo.  Enables workflows in each project one at a time, each in its own repo. Changes Ensures changes affect all the projects, can be tracked together, tested together, and released together. Ensures changes affect only one project, can be tracked separately, tested separately, and released separately. Collaboration Encourages collaboration and code sharing within an organization.  Encourages collaboration and code sharing across organizations. Testing Good white box testing because all projects are testable together, and verifiable holistically. Good black box testing because each project is testable separately, and verifiable independently. Releases Coordinated releases are inherent, yet must use a polygot of tooling. Coordinated releases must be programmed, yet can use vanilla tooling. State The current state of everything is one commit in one repo. The current state of everything is a commit per repo. Coupling Tight coupling of projects. No coupling of projects. Thinking Encourages thinking about conjoins among projects. Encourages thinking about contracts between projects. Access <p>Access control defaults to all projects.<p>Some teams use tools for finer-grained access control.<p>Gitlab offers ownership control where you can say who owns what directories for things like approving merge requests that affect those directories. Google Piper has finer-grained access control. Phabricator offers herald rules that stop a merge from happening if a file has changed in a specific subdirrectory. Some teams use service owners, so when a change spans multiple services, they are all added automatically as blocking reviewers. <p>Access control defaults to per project.<p>Some teams use tools for broader-graned access control.<p>GitHub offers teams where you can say one team owns many projects and for things like approving requests that affect multiple repos. Scaling Scaling needs specialized tooling. It is currently not practical to use vanilla git with very large repos, or very large files, without any extra tooling. For monorepo scaling, teams invest in writing custom tooling and providing custom training. Scaling needs specialized coordination. It is currently not practical to use vanilla git with many projects across many repos, where a team wants to coordinate code changes, testing, packaging, and releasing. For polyrepo scaling, teams invest in writing coordination scripts and careful cross-version compatibility. Tooling Google wrote the tool \u201cbazel\u201d, which tracks internal dependencies by using directed acyclic graphs. Lyft wrote the tool \u201crefactorator\u201d, which automates making changes in multiple repos, including opening PRs, tracking status, etc."},{"location":"articles/monorepo-vs-polyrepo/#tooling","title":"Tooling","text":""},{"location":"articles/monorepo-vs-polyrepo/#bazel","title":"Bazel","text":"<p>Bazel is a fast, scalable, multi-language and extensible build system. Bazel only rebuilds what is necessary. With advanced local and distributed caching, optimized dependency analysis and parallel execution, you get fast and incremental builds.</p> <p>Bazel requires you to explicitly declare your dependencies for each 'target' you want to build. These dependencies can be within the same Bazel workspace, or imported at build time via say git - there's no need to have all the files directly in your repo.</p> <p>The nice thing is you can declare the commit id or file hash for the dependency you're importing to make sure you're getting what you expect, and keep Bazel's reproducibility properties.</p>"},{"location":"articles/monorepo-vs-polyrepo/#lerna","title":"Lerna","text":"<p>Lerna is a tool that optimizes the workflow around managing multi-package repositories with git and npm.</p>"},{"location":"articles/monorepo-vs-polyrepo/#octolinker","title":"OctoLinker","text":"<p>Octolinker really helps when browsing a polyrepo on Github. You can just click the import [project] name and it will switch to the repo.</p>"},{"location":"articles/monorepo-vs-polyrepo/#monorepo-scaling","title":"Monorepo scaling","text":""},{"location":"articles/monorepo-vs-polyrepo/#monorepo-scaling-problem","title":"Monorepo scaling problem","text":"<p>Monorepo scaling becomes a problem when a typical developer can't work well with the code by using typical tools such as vanilla git.</p> <ul> <li> <p>Monorepo scaling eventually becomes impractical in terms of space: when a monorepo grows to have more data than fits on a developer's laptop, then the developer cannot fetch the monorepo, and it may be impractical to obtain to more storage space.</p> </li> <li> <p>Monorepo scaling eventually becomes impractical in terms of time: when a monorepo grows, then a complete file transfer takes more time, and in practice, there are other operations that also take more time, such as git pruning, git repacking.</p> </li> <li> <p>A monorepo may grow so large contain so many projects that it takes too much mental effort to work across projects, such as for searching, editing, and isolating changes.</p> </li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/#monorepo-scaling-mitigations","title":"Monorepo scaling mitigations","text":"<p>Monorepo scaling can be improved by:</p> <ul> <li> <p>Some type of virtual file system (VFS) that allows a portion of the code to be present locally. This might be accomplished via a proprietary VCS like Perforce which natively operates this way, or via Google\u2019s \u201cG3\u201d internal tooling, or via Microsoft\u2019s GVFS.</p> </li> <li> <p>Sophisticated source code indexing/searching/discovery capabilities as a service. This is because a typical developer is not going to have all the source code locallly, in a searchable state, using vanilla tooling.</p> </li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/#monorepo-scaling-metrics","title":"Monorepo scaling metrics","text":"<p>Monorepo scaling seems to become an issue, in practice, at approximately these kinds of metrics:</p> <ul> <li> <p>10-100 developers writing code full time.</p> </li> <li> <p>10-100 projects in progress at the same time.</p> </li> <li> <p>10-100 packaging processes during the same time period, such as a daily release.</p> </li> <li> <p>1K-10K versioned dependencies, such as Node modules, Python packages, Ruby gems, etc.</p> </li> <li> <p>1M-10M lines of code</p> </li> </ul>"},{"location":"articles/monorepo-vs-polyrepo/#proponents-of-monorepo","title":"Proponents of monorepo","text":""},{"location":"articles/monorepo-vs-polyrepo/#if-components-need-to-release-together-then-use-a-monorepo","title":"If components need to release together, then use a monorepo","text":"<p>If you think components might need to release together then they should go in the same repo, because you can in fact pretty easily manage projects with different release schedules from the same repo if you really need to.</p> <p>On the other hand if you've got a whole bunch of components in different repos which need to release together it suddenly becomes a real pain.</p>"},{"location":"articles/monorepo-vs-polyrepo/#if-components-need-to-share-common-code-then-use-a-monorepo","title":"If components need to share common code, then use a monorepo","text":"<p>If you have components that will never need to release together, then of course you can stick them in different repositories-- but if you do this and you want to share common code among the repositories, then you will need to manage that code with some sort of robust versioning system, and robust versioning systems are hard. Only do something like that when the value is high enough to justify the overhead. If you're in a startup, chances are very good that the value is not high enough.</p>"},{"location":"articles/monorepo-vs-polyrepo/#ive-found-monorepos-to-be-extremely-valuable-in-an-less-mature-high-churn-codebase","title":"I\u2019ve found monorepos to be extremely valuable in an less-mature, high-churn codebase","text":"<p>Need to change a function signature or interface? Cool, global find &amp; replace.</p> <p>At some point a monorepo outgrows its usefulness. The sheer amount of files in something that\u2019s 10K+ LOC (not that large, I know) warrants breaking apart the codebase into packages.</p> <p>Still, I almost err on the side of monorepos because of the convenience that editors like vscode offer: autocomplete, auto-updating imports, etc.</p>"},{"location":"articles/monorepo-vs-polyrepo/#a-common-mission","title":"A common mission","text":"<p>I find it helpful to think of a company as a group of people engaged in a common mission. The company pursues its mission through multiple subprojects, and every decision taken and every code change introduced is a step towards its primary goal. The code base is a chunk of the company's institutional knowledge about its overarching goal and means to that end.</p> <p>Looking at it from this perspective, a monorepo can be seen as the most natural expression of the fact that all team members are engaged in a single, if multi-faceted, enterprise.</p>"},{"location":"articles/monorepo-vs-polyrepo/#proponents-of-polyrepo","title":"Proponents of polyrepo","text":""},{"location":"articles/monorepo-vs-polyrepo/#if-techs-biggest-names-use-a-monorepo-should-we-do-the-same","title":"If tech's biggest names use a monorepo, should we do the same?","text":"<p>Some of tech\u2019s biggest names use a monorepo, including Google, Facebook, Twitter, and others. Surely if these companies all use a monorepo, the benefits must be tremendous, and we should all do the same, right? Wrong!</p> <p>Why? Because, at scale, a monorepo must solve every problem that a polyrepo must solve, with the downside of encouraging tight coupling, and the additional herculean effort of tackling VCS scalability.</p> <p>Thus, in the medium to long term, a monorepo provides zero organizational benefits, while inevitably leaving some of an organization\u2019s best engineers with a wicked case of PTSD (manifested via drooling and incoherent mumbling about git performance internals).</p>"},{"location":"articles/monorepo-vs-polyrepo/#coupling-between-unrelated-projects","title":"Coupling between unrelated projects","text":"<p>I worry about the monorepo coupling between unrelated products. While I admit part of this probably comes from my more libertarian world view but I have seen something as basic as a server upgrade schedule that is tailored for one product severely hurt the development of another product, to the point of almost halting development for months. I can't imagine needing a new feature or a big fix from a dependency but to be stuck because the whole company isn't ready to upgrade.</p> <p>I've read of at least one less serious case of this from google with JUnit: \"In 2007, Google tried to upgrade their JUnit from 3.8.x to 4.x and struggled as there was a subtle backward incompatibility in a small percentage of their usages of it. The change-set became very large, and struggled to keep up with the rate developers were adding tests.\"</p>"},{"location":"articles/monorepo-vs-polyrepo/#visible-organization","title":"Visible organization","text":"<p>I argue that a visible organization of a codebase into repositories makes it easier to reuse code in the same way that interface/implementation splits do: it makes it clearer which parts felt domain-specific and which felt like reusable libraries.</p> <p>Being able to represent \"not directly involved, but versioned together\" and \"separate enough to be versioned separately\" is a very valuable distinction to have in your toolbox.</p> <p>Once your team is large enough that your developers are not all attending the same standup, then you should be working in multiple repositories. You need to have a release cycle with semVer etc. so that developers who aren't in close communication with you can understand the impact of changes to your code area. Since tags are repository-global, the repository should be the unit of versioning/releasing.</p>"},{"location":"articles/monorepo-vs-polyrepo/#opinions-about-splitting","title":"Opinions about splitting","text":""},{"location":"articles/monorepo-vs-polyrepo/#splitting-one-repo-is-easier-than-combining-multiple-repos","title":"Splitting one repo is easier than combining multiple repos","text":"<p>You can split big repositories into smaller ones quite easily (in Git anyway). If you only need to do this once, then subtree will do the job, even retaining all your history if you want. As another way to split, you can duplicate the repo and pull trees out of each dupe in normal commits.</p> <p>But combining small repositories together into a bigger repo is a lot harder.</p> <p>So start out with a monorepo.</p> <p>Only split a monorepo into multiple smaller repositories when you're clear that it really makes sense.</p>"},{"location":"articles/monorepo-vs-polyrepo/#splitting-may-be-too-fine","title":"Splitting may be too fine","text":"<p>My problem with polyrepo is that often organizations end up splitting things too finely, and now I'm unable to make a single commit to introduce a feature because my changes have to live across several repositories.</p> <p>This makes code review more annoying because you have to tab back and forth to see all the context.</p> <p>This makes it worse to make changes to fundamental (internal) libraries used by every project. It's too much hassle to track down all the uses of a particular function, so I end up putting that change elsewhere, which means someone else will do it a little different in their corner of the world, which utterly confuses the first person who's unlucky enough to work in both code bases (at the same time, or after moving teams).</p>"},{"location":"articles/monorepo-vs-polyrepo/#opinions-about-balances","title":"Opinions about balances","text":""},{"location":"articles/monorepo-vs-polyrepo/#its-a-social-problem-in-how-you-manage-boundaries","title":"It's a social problem in how you manage boundaries","text":"<p>Among many of the major monorepos, boundaries still exist, they just become far more opaque because no one has to track them. You find the weird gatekeepers in the dark that spring out only when you get late in your code review process because you touched \"their\" file and they got an automated notice from a hidden rules engine in your CI process you didn't even realize existed.</p> <p>In the polyrepo case those boundaries have to be made explicit (otherwise no one gets anything done) and those owners should be easily visible. You may not like the friction they sometimes bring to the table, but at least it won't be a surprise.</p>"},{"location":"articles/monorepo-vs-polyrepo/#challenges-of-monorepo-and-polyrepo","title":"Challenges of monorepo and polyrepo","text":"<p>My last 2 jobs have been working on developer productivity for 100+ developer organizations. One organization uses monorepo, one organization uses polyrepo.</p> <p>Neither really seems to result in less work, or a better experience. But I've found that your choice just dictates what type of problems you have to solve.</p> <p>Monorepo involves mostly challenges around scaling the organization in a single repo.</p> <p>Polyrepo involves mostly challenges with coordination.</p>"},{"location":"articles/monorepo-vs-polyrepo/#on-premise-applications-or-desktop-applications","title":"On-premise applications or desktop applications","text":"<p>If you're creating on-premise applications or desktop applications (things requiring long lived release branches), the discussion is totally different.</p> <p>I find that shipped software actually operates better in a normal, branched monorepo. You just branch the whole thing. The alternative is several repos and using a package manager, which minimizes merging in many branches, as you can just point the package manager at the updated module version, but brings its own hassles.</p> <p>I've worked on projects where there were 6-7 major branches active at the same time and several smaller ones, besides the master branch. Then you'd have to merge everywhere applicable, etc. This is a totally different approach from the Google monorepo approach of \"master only\", basically. And probably one of the main reasons why Golang is having a ton of difficulties in the outside world by not having a proper versioning story.</p> <p>Comment: Once you're shipping software off prem you need to patch it between major and minor releases. Typically one way to do that is to branch when you do a release to a branch namded for the release. Say 1.2. Then when issues pop up you fix it in the branch then see if it applies to the trunk or other branches after that.</p>"},{"location":"articles/monorepo-vs-polyrepo/#opinions-about-alternatives","title":"Opinions about alternatives","text":""},{"location":"articles/monorepo-vs-polyrepo/#could-you-get-the-best-of-both-worlds-by-having-a-monorepo-of-submodules","title":"Could you get the best of both worlds by having a monorepo of submodules?","text":"<p>Code would live in separate repos, but references would be declared in the monorepo. Checkins and rollbacks to the monorepo would trigger CI.</p> <p>Answer: There's not much good to either world. You need fairly extensive tooling to make working with a repo of submodules comfortable at any scale. At large scale, that tooling can be simpler than the equivalent monorepo tooling, assuming that your individual repos remain \"small\" but also appropriately granular (not a given--organizing is hard, especially if you leave it to individual project teams). However, in the process of getting there, a monorepo requires no particular bespoke tooling at small or even medium scale (it's just \"a repo\"), and the performance intolerability pretty much scales smoothly from there. And those can be treated as technical problems if you don't want to approach social problems.</p> <p>Answer: We actually did this. When I started at Uber ATG one of our devs made a submodule called <code>uber_monorepo</code> that was linked from the root of our git repo. In our repo's <code>.buckconfig</code> file we had access to everything that the mobile developers at Uber had access to by prefixing our targets with <code>//uber_monorepo/</code>. We did however run into the standard dependency resolution issue when you have any loosely coupled dependency. Updating our submodule usually required a 1-2 day effort because we were out of sync for a month or two.</p>"},{"location":"articles/monorepo-vs-polyrepo/#hybrid-of-many-repos","title":"Hybrid of \"many repos\"","text":"<p>We consider our org to be \"many repos\". We have several thousands. However, hundreds of them contain 5, 10, or 20+ packages/projects/services. It's funny because we'll talk about creating \"monorepos\" (plural) for certain part of our product, and it confuses people.</p> <p>There's a few thousand libraries, those obviously refer to each other.. Some have readme files and that's enough, some have full documentation \"books\", some have comments in the code and that's enough.</p> <p>We don't mandate a company wide development process, so each team and groups can choose their own process and how they track their stuff.</p> <p>We do have automation and tooling to keep track of things though.</p>"},{"location":"articles/monorepo-vs-polyrepo/#prediction-of-a-new-type-of-vcs","title":"Prediction of a new type of VCS","text":"<p>What we all really want is a VCS where repos can be combined and separated easily, or where one repo can gain the benefits of a monorepo without the drawbacks of one.</p> <p>Prediction: just as DVCS killed off pre-DVCS practically overnight, the thing that will quickly kill off DVCS is a new type of VCS where you can trivially combine/separate repos and sections of repos as needed. You can assign, at the repo level, sub-repos to include in this one, get an atomic commit hash for the state of the whole thing, and where my VCS client doesn't need to actually download every linked repo, but where tools are available to act like I have.</p> <p>In a sense, we already have all of these features, in folders. You can combine and separate them, you can make a local folder mimic a folder on a remote system, and access its content without needing to download it all ahead of time. They just don't have any VCS features baked in. We've got {filesystems, network filesystems, and VCS}, and each of the three has some features the others would like.</p>"},{"location":"articles/package-managers/","title":"Package Managers","text":"<p>A package manager is a programming language's tool to create project environments and import external dependencies.</p> <ul> <li>1. Vcpkg</li> <li>2. Conan</li> <li>3. build2</li> <li>4. Hunter</li> <li>5. cget</li> </ul> <p>A reasonable set of features for selecting a package manager is build system support (e.g. CMake), IDE integration, command line interface and distributed repositories.</p>"},{"location":"articles/package-managers/#1-vcpkg","title":"1. Vcpkg","text":"<p>Vcpkg is a C/C++ dependency manager from Microsoft for all platforms, buildsystems, and workflows.</p> <p>vcpkg is a free C/C++ package manager for acquiring and managing libraries. Choose from over 1500 open source libraries to download and build in a single step or add your own private libraries to simplify your build process. Maintained by the Microsoft C++ team and open source contributors.</p>"},{"location":"articles/package-managers/#2-conan","title":"2. Conan","text":"<p>Conan  is a C/C++ package manager written in Python.</p> <p>Conan is a MIT-licensed, Open Source package manager for C and C++ development, allowing development teams to easily and efficiently manage their packages and dependencies across platforms and build systems.</p>"},{"location":"articles/package-managers/#3-build2","title":"3. build2","text":"<p>build2 is a C/C++ cross-platform toolchain for building and packaging code.</p> <p>build2 is a modern build system and dependency manager for the C++ language that provide a consistent, out of the box interface across multiple platforms and compilers</p>"},{"location":"articles/package-managers/#4-hunter","title":"4. Hunter","text":"<p>Hunter  is a C/C++, CMake driven cross-platform package manager for Linux, Windows, macOS, iOS, Android, Raspberry Pi, etc.</p> <p>Hunter does not need any external configuration file or run any application before editing the project or running cmake from command line. All the libraries to be installed are set in the file CMakeFiles.txt. If they are not available in the system, they are automatically downloaded and installed.</p>"},{"location":"articles/package-managers/#5-cget","title":"5. cget","text":"<p>cget  is a C/C++ package retrieval, not a package manager.</p> <p>cget creates a cmake toolchain file with the settings necessary to build and find the libraries.</p>"},{"location":"articles/programming-paradigms/","title":"Programming Paradigms","text":"<p>Programming paradigms are broad approaches to solving computational problems through programming. These paradigms are used to classify programming languages based on the fundamental principles they use to express computational processes.</p> <p>Programming paradigms are essential to understanding the underlying principles of programming languages and can help guide software design and development decisions.</p> <ul> <li>1. Imperative Paradigm</li> <li>2. Object-Oriented Paradigm</li> <li>3. Functional Paradigm</li> <li>4. Declarative Paradigm</li> <li>5. Logic Paradigm</li> <li>6. Event-Driven Paradigm</li> <li>7. Procedural Paradigm</li> <li>8. Scripting Paradigm</li> </ul>"},{"location":"articles/programming-paradigms/#1-imperative-paradigm","title":"1. Imperative Paradigm","text":"<p>Imperative paradigm represents algorithms as sequences of commands or statements that describe how to perform a computation. Languages in this paradigm include C, Pascal, and BASIC.</p>"},{"location":"articles/programming-paradigms/#2-object-oriented-paradigm","title":"2. Object-Oriented Paradigm","text":"<p>Object-Oriented paradigm represents algorithms as objects and classes that encapsulate both data and behavior. Object-oriented programming languages include Java, Python, and Ruby.</p>"},{"location":"articles/programming-paradigms/#3-functional-paradigm","title":"3. Functional Paradigm","text":"<p>Functional paradigm represents algorithms as mathematical functions that can be combined and manipulated in a way that eliminates the need for state and mutable data. Languages in this paradigm include Haskell, Lisp, and Scheme.</p>"},{"location":"articles/programming-paradigms/#4-declarative-paradigm","title":"4. Declarative Paradigm","text":"<p>Declarative paradigm represents algorithms as descriptions of the desired outcome, rather than instructions on how to compute it. Examples of declarative languages include SQL, Prolog, and Regular Expressions.</p>"},{"location":"articles/programming-paradigms/#5-logic-paradigm","title":"5. Logic Paradigm","text":"<p>Logic paradigm represents algorithms as statements in formal logic that can be used for theorem proving and other tasks. Examples of logic programming languages include Prolog and Mercury.</p>"},{"location":"articles/programming-paradigms/#6-event-driven-paradigm","title":"6. Event-Driven Paradigm","text":"<p>Event-Driven paradigm focuses on the asynchronous response to external events, and triggers functions based on specific events. Examples of event-driven languages include JavaScript and ActionScript.</p>"},{"location":"articles/programming-paradigms/#7-procedural-paradigm","title":"7. Procedural Paradigm","text":"<p>Procedural paradigm represents algorithms as sequences of procedures or subroutines that can be executed step by step. Examples of procedural programming languages include FORTRAN, COBOL, and Algol.</p>"},{"location":"articles/programming-paradigms/#8-scripting-paradigm","title":"8. Scripting Paradigm","text":"<p>Scripting paradigm represents algorithms as scripts that automate repetitive tasks. Examples of scripting languages include Perl, Python, and Ruby.</p>"},{"location":"articles/project-layout/","title":"Project Layout","text":"<p>Project layout refers to the arrangement of files, folders, and other resources within a project. A well-designed project layout can improve the organization, readability, and maintainability of the project's codebase.</p> <ul> <li>1. Category</li> <li>1.1. File Types<ul> <li>1.1.1. Source Code</li> <li>1.1.2. Header</li> <li>1.1.3. Build</li> <li>1.1.4. Documentation</li> <li>1.1.5. Test</li> <li>1.1.6. Data</li> <li>1.1.7. Resource</li> <li>1.1.8. Log</li> <li>1.1.9. Configuration</li> <li>1.1.10. Script</li> <li>1.1.11. Library</li> </ul> </li> <li>1.2. Directory Types<ul> <li>1.2.1. Source Code</li> <li>1.2.2. Documentation</li> <li>1.2.3. Tests</li> <li>1.2.4. Configuration</li> <li>1.2.5. Resources</li> <li>1.2.6. Data</li> <li>1.2.7. Libraries</li> <li>1.2.8. Artifacts</li> </ul> </li> <li>1.3. Structure Types<ul> <li>1.3.1. Flat Structure</li> <li>1.3.2. Hierarchical Structure</li> <li>1.3.3. Modular Structure</li> <li>1.3.4. Layered Structure</li> <li>1.3.5. Component-based Structure</li> <li>1.3.6. Functional-based Structure</li> <li>1.3.7. Task-based Structure</li> </ul> </li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/project-layout/#1-category","title":"1. Category","text":"<p>The project layout can be divided into several categories, depending on the type of project, its size, and its complexity.</p>"},{"location":"articles/project-layout/#11-file-types","title":"1.1. File Types","text":""},{"location":"articles/project-layout/#111-source-code","title":"1.1.1. Source Code","text":"<p>Source files contain the actual source code of the project. They are typically written in a programming language such as C, Python, Java, etc.</p> <p>Source code files usually have file extensions that match the programming language, such as:</p> <ul> <li><code>.c</code></li> <li><code>.py</code></li> <li><code>.java</code></li> <li><code>.go</code></li> </ul>"},{"location":"articles/project-layout/#112-header","title":"1.1.2. Header","text":"<p>Header files typically contain function prototypes and definitions of constants and variables used in the source code files.</p> <p>They are usually written in the same programming language as the source code files and have file extensions such as:</p> <ul> <li><code>.h</code></li> <li><code>.hpp</code></li> </ul>"},{"location":"articles/project-layout/#113-build","title":"1.1.3. Build","text":"<p>Files used to build the project, including makefiles, build scripts, and build configuration files. Build files are typically used to automate the build process and ensure that the project is built consistently across different environments.</p> <p>NOTE See build systems for details.</p> <ul> <li><code>Makefile</code></li> <li><code>CMakeLists.txt</code></li> <li><code>.gradle</code></li> </ul>"},{"location":"articles/project-layout/#114-documentation","title":"1.1.4. Documentation","text":"<p>These files contain documentation for the project, including user manuals, API documentation, and other information about the project.</p> <p>NOTE See docs as code and static site generators for details.</p> <p>Documentation files can be written in various formats:</p> <ul> <li><code>.md</code></li> <li><code>.rst</code></li> <li><code>.html</code></li> </ul>"},{"location":"articles/project-layout/#115-test","title":"1.1.5. Test","text":"<p>These files contain tests for the project, including unit tests, integration tests, and other types of tests.</p> <p>Test files are typically written in the same programming language as the source code files and have appendix bevor file extensions such as:</p> <ul> <li><code>_test</code></li> </ul>"},{"location":"articles/project-layout/#116-data","title":"1.1.6. Data","text":"<p>These files contain data used by the project, such as configuration files, input files, and output files.</p> <p>Data files can be written in various formats:</p> <ul> <li><code>.csv</code></li> <li><code>.json</code></li> <li><code>.xml</code></li> </ul>"},{"location":"articles/project-layout/#117-resource","title":"1.1.7. Resource","text":"<p>These files contain resources used by the project, such as images, icons, and other assets.</p> <p>Resource files can be written in various formats:</p> <ul> <li><code>.png</code></li> <li><code>.jpg</code></li> <li><code>.svg</code></li> <li><code>.csv</code></li> </ul>"},{"location":"articles/project-layout/#118-log","title":"1.1.8. Log","text":"<p>These files contain logs generated by the project, including error logs, access logs, and other types of logs.</p> <p>Log files can be written in various formats:</p> <ul> <li><code>.log</code></li> <li><code>.txt</code></li> </ul>"},{"location":"articles/project-layout/#119-configuration","title":"1.1.9. Configuration","text":"<p>These files contain configuration settings for the project, such as environment variables, database connection strings, and other settings that are necessary for the project to run correctly.</p> <p>NOTE See data serialization formats for details.</p> <p>Configuration files can be written in various formats:</p> <ul> <li><code>.env</code> <p>This is a file format used to store environment variables that are used by a software application or system. Environment variables are variables that are set in the operating system's environment and are accessible by programs running on the system. <code>.env</code> files are often used in web applications to store sensitive information such as database credentials or API keys. The file typically contains a list of key-value pairs in the format <code>KEY=VALUE</code>, with each pair on a separate line.</p> </li> </ul> <p>Example:</p> <pre><code># Example .env file\n\nDB_HOST=localhost\nDB_PORT=5432\nDB_NAME=mydatabase\nDB_USER=myusername\nDB_PASSWORD=mypassword\n</code></pre> <ul> <li><code>.json</code> <p>JSON (JavaScript Object Notation) is a lightweight data serialization format that is commonly used for transmitting data between a web server and a client-side application. JSON is easy for humans to read and write, and easy for machines to parse and generate. JSON is often used to store configuration data, such as database connection strings, or to transmit data between different components of a software system. JSON files consist of key-value pairs in a hierarchical structure.</p> </li> </ul> <p>Example:</p> <pre><code>{\n  \"database\": {\n    \"host\": \"localhost\",\n    \"port\": 5432,\n    \"name\": \"mydatabase\",\n    \"user\": \"myusername\",\n    \"password\": \"mypassword\"\n  }\n}\n</code></pre> <ul> <li><code>.yaml</code> <p>YAML (YAML Ain't Markup Language) is a data serialization format that is designed to be human-readable and easy to parse. YAML is often used to store configuration data, such as settings for a web application or a deployment pipeline. YAML is similar to JSON in many ways, but has some additional features such as support for comments and the ability to reference other parts of the document. YAML files consist of key-value pairs in a hierarchical structure.</p> </li> </ul> <p>Example:</p> <pre><code># Example YAML file\n\ndatabase:\n  host: localhost\n  port: 5432\n  name: mydatabase\n  user: myusername\n  password: mypassword\n</code></pre> <ul> <li><code>.ini</code> <p>INI files are a simple text-based format for storing configuration data. INI files typically contain a series of sections, each of which contains a set of key-value pairs. INI files are widely used on Windows systems, where they are used to store configuration data for applications and system settings. INI files are also used in some Linux-based systems for storing configuration data for applications. While INI files are simple and easy to use, they have limited support for more complex data structures and are not well-suited for storing large amounts of data.</p> </li> </ul> <p>Example:</p> <pre><code>; Example INI file\n\n[database]\nhost=localhost\nport=5432\nname=mydatabase\nuser=myusername\npassword=mypassword\n</code></pre>"},{"location":"articles/project-layout/#1110-script","title":"1.1.10. Script","text":"<p>These are files that contain scripts used by the project.</p> <ul> <li><code>.sh</code></li> </ul> <p>Script files written in the Bash scripting language, which is commonly used in Unix-based operating systems (such as Linux and macOS) to automate tasks and run shell commands. Bash scripts can be used to perform a wide variety of tasks, such as setting environment variables, running system commands, and automating file operations.</p> <ul> <li><code>.py</code></li> </ul> <p>Python is a high-level, interpreted language. Python scripts can be used to automate tasks, manipulate data, and create complex software applications.</p> <p>NOTE Bash and Python scripts can be executed from the command line, typically by running a command that specifies the path to the script file. For example, to run a Bash script named <code>myscript.sh</code>, type <code>./myscript.sh</code> in the terminal, assuming that the script file is located in the current working directory. Similarly, to run a Python script named <code>myscript.py</code>, type <code>python myscript.py</code> in the terminal.</p>"},{"location":"articles/project-layout/#1111-library","title":"1.1.11. Library","text":"<p>These file extensions are related to libraries, which are collections of pre-compiled code that can be linked into a program during the build process.</p> <ul> <li> <p><code>.a</code></p> <p>These are static libraries in Unix-based systems (such as Linux or macOS). They contain pre-compiled object code that can be linked into a program at compile time. When a program is built, the linker copies the relevant object code from the <code>.a</code> file into the final executable.</p> </li> <li> <p><code>.so</code></p> <p>These are shared libraries in Unix-based systems. They contain pre-compiled object code that can be loaded into memory at run-time. Shared libraries allow multiple programs to share the same code in memory, which can save memory and disk space. When a program is built, it links to the shared library, and the library is loaded when the program is run.</p> </li> <li> <p><code>.dll</code></p> <p>These are dynamic link libraries in Windows-based systems. They are similar to shared libraries in Unix-based systems, in that they contain pre-compiled object code that can be loaded into memory at run-time. Like shared libraries, they allow multiple programs to share the same code in memory. When a program is built, it links to the <code>.dll</code> file, and the library is loaded when the program is run. <code>.dll</code> files are used in Windows-based systems, whereas Unix-based systems use <code>.so</code> files.</p> </li> </ul>"},{"location":"articles/project-layout/#12-directory-types","title":"1.2. Directory Types","text":"<p>The categories of files and folders that can be included in a project layout, depending on the nature of the project and the tools being used. These categories can be combined and nested in various ways to create a project layout that best suits.</p>"},{"location":"articles/project-layout/#121-source-code","title":"1.2.1. Source Code","text":"<p>This category includes all the files that contain the actual code for the project, such as HTML, CSS, JavaScript, Python, Java, or any other programming language files.</p> <ul> <li><code>/src</code> <p>This includes all the files related to the code that runs the project, such as scripts, libraries, and configuration files.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 module1.py\n\u2502   \u2514\u2500\u2500 module2.py\n</code></pre> <ul> <li><code>/internal</code> <p>This directory contains the internal source code of the project, organized into modules or components. The internal folder can be structured similarly to the <code>/src</code> folder, with sub-folders for each package.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 package1/\n\u2502   \u2502   \u251c\u2500\u2500 file1.go\n\u2502   \u2502   \u2514\u2500\u2500 file2.go\n\u2502   \u2514\u2500\u2500 package2/\n\u2502       \u251c\u2500\u2500 file1.go\n\u2502       \u2514\u2500\u2500 file2.go\n</code></pre>"},{"location":"articles/project-layout/#122-documentation","title":"1.2.2. Documentation","text":"<ul> <li><code>/docs</code> <p>This includes any documentation related to the project, such as user guides, API documentation, and technical specifications.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 adr/\n\u2502   \u251c\u2500\u2500 user_manual.pdf\n\u2502   \u2514\u2500\u2500 technical_specifications.md\n</code></pre>"},{"location":"articles/project-layout/#123-tests","title":"1.2.3. Tests","text":"<ul> <li><code>/test</code> <p>This includes all the files related to testing the project, such as unit tests, integration tests, or any other type of tests.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 main_test.py\n\u2502   \u251c\u2500\u2500 module1_test.py\n\u2502   \u2514\u2500\u2500 module2_test.py\n</code></pre>"},{"location":"articles/project-layout/#124-configuration","title":"1.2.4. Configuration","text":"<ul> <li><code>/config</code> <p>This includes any files that configure the project environment or settings, such as configuration files for a web server, database settings, or other system settings.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 config.ini\n\u2502   \u2514\u2500\u2500 logging.conf\n</code></pre>"},{"location":"articles/project-layout/#125-resources","title":"1.2.5. Resources","text":"<p>The project resources category is used for files and folders that provide support for the project development process, but are not directly part of the project source code or documentation.</p> <ul> <li><code>/assets</code> <p>This includes any non-code assets used by the project, such as images, icons, or other multimedia resources.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 assets/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 logo.png\n\u2502   \u2502   \u2514\u2500\u2500 background.jpg\n\u2502   \u2514\u2500\u2500 icons/\n\u2502       \u251c\u2500\u2500 icon1.svg\n\u2502       \u2514\u2500\u2500 icon2.svg\n</code></pre> <ul> <li><code>/tools</code> <p>This directory contains various tools or utilities used in the development process, such as linters, formatters, or IDE plugins.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 devops/\n\u2502   \u2514\u2500\u2500 generator/\n</code></pre> <ul> <li><code>/scripts</code> <p>This includes any files or scripts used to build and package the project, such as build scripts, installers, or deployment scripts. Further, this directory contains various scripts that automate tasks related to the project.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 setup.sh\n\u2502   \u251c\u2500\u2500 install.sh\n\u2502   \u251c\u2500\u2500 build_pipeline.sh\n\u2502   \u2514\u2500\u2500 deploy_pipeline.sh\n</code></pre>"},{"location":"articles/project-layout/#126-data","title":"1.2.6. Data","text":"<ul> <li><code>/data</code> <p>This includes any data files used by the project, such as datasets, images, and audio or video files.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 datasets1.csv\n\u2502   \u2514\u2500\u2500 datasets2.csv\n</code></pre> <ul> <li><code>/db</code> <p>This folder contains any database-related files used by the project, such as SQL scripts or database configuration files.</p> </li> </ul> <p>Example:</p> <pre><code>  project/\n  \u251c\u2500\u2500 db/\n  \u2502   \u251c\u2500\u2500 schema/\n  \u2502   \u251c\u2500\u2500 quere/\n  \u2502   \u251c\u2500\u2500 migration/\n  \u2502   \u251c\u2500\u2500 sql/\n</code></pre>"},{"location":"articles/project-layout/#127-libraries","title":"1.2.7. Libraries","text":"<p>This category includes any external libraries or dependencies that are required for the project, such as jQuery, Bootstrap, or React.</p> <ul> <li><code>/external</code> <p>This includes any third-party libraries or tools used by the project, such as dependencies managed by a package manager.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 external/\n\u2502   \u251c\u2500\u2500 lodash/\n\u2502   \u2502   \u2514\u2500\u2500 lodash.go\n\u2502   \u251c\u2500\u2500 library2/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n</code></pre> <ul> <li><code>/lib</code> <p>This includes any compiled or packaged files generated by the project, such as executables, shared libraries, or distribution archives.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 library1/\n\u2502       \u2514\u2500\u2500 libproject.a\n</code></pre>"},{"location":"articles/project-layout/#128-artifacts","title":"1.2.8. Artifacts","text":"<ul> <li><code>/build</code> <p>This folder contains all the build artifacts created during the build and deployment process, such as compiled binaries, packaged code, and deployment artifacts. These artifacts may be stored here temporarily before being deployed to a production environment.</p> </li> </ul> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 build/\n\u2502   \u251c\u2500\u2500 binary/\n\u2502   \u2502   \u251c\u2500\u2500 app-linux-amd64\n\u2502   \u2502   \u251c\u2500\u2500 app-windows-amd64.exe\n\u2502   \u2502   \u2514\u2500\u2500 app-darwin-amd64\n\u2502   \u251c\u2500\u2500 library/\n\u2502   \u2502   \u251c\u2500\u2500 liblinux-amd64.a\n\u2502   \u2502   \u251c\u2500\u2500 windows-amd64.dll\n\u2502   \u2502   \u2514\u2500\u2500 app-darwin-amd64\n\u2502   \u251c\u2500\u2500 deployment-packages/\n</code></pre>"},{"location":"articles/project-layout/#13-structure-types","title":"1.3. Structure Types","text":"<p>There are several types of project layout structures that can be used, depending on the nature of the project and the preferences of the development team. These structure types can be combined and customized in various ways to create a project layout that best suits. It's important to choose a structure that is both functional and maintainable over time.</p>"},{"location":"articles/project-layout/#131-flat-structure","title":"1.3.1. Flat Structure","text":"<p>In a flat structure, all the files and folders are placed in the root directory, without any subdirectories.</p> <p>This structure is typically used for small projects that don't require much organization or separation of concerns.</p> <pre><code>/project\n\u251c\u2500\u2500 file1.js\n\u251c\u2500\u2500 file2.js\n\u251c\u2500\u2500 file3.js\n\u2514\u2500\u2500 main.js\n</code></pre> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 component1.js\n\u251c\u2500\u2500 component2.js\n\u251c\u2500\u2500 service1.js\n\u251c\u2500\u2500 service2.js\n\u251c\u2500\u2500 util1.js\n\u251c\u2500\u2500 util2.js\n\u251c\u2500\u2500 image1.png\n\u251c\u2500\u2500 image2.png\n\u251c\u2500\u2500 style1.css\n\u251c\u2500\u2500 style2.css\n\u251c\u2500\u2500 webpack.config.js\n\u251c\u2500\u2500 database.config.js\n\u251c\u2500\u2500 subcomponent1.test.js\n\u251c\u2500\u2500 subcomponent2.test.js\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 API.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 ...\n</code></pre> <p>In the example, the project directory contains all project files in a flat structure without any subdirectories. All components, services, utilities, assets, configuration files, test files, and documentation files are placed directly in the root directory of the project.</p>"},{"location":"articles/project-layout/#132-hierarchical-structure","title":"1.3.2. Hierarchical Structure","text":"<p>In a hierarchical structure, files and folders are organized into a tree-like structure, with subdirectories that represent different levels of abstraction in the project.</p> <p>The hierarchical structure allows for greater organization and separation of concerns, which can make it easier to manage and scale a project as it grows in size and complexity. The hierarchical structure organizes project resources based on their functional or logical relationships, making it easier to navigate and understand the project's structure. However, it can also be more challenging to navigate and understand for new contributors to the project.</p> <p>Hierarchical structure promotes modularity, reusability, and maintainability by separating components, services, utilities, and test files into distinct directories.</p> <pre><code>/project\n\u251c\u2500\u2500 /src\n\u2502   \u251c\u2500\u2500 /app\n\u2502   \u2502   \u251c\u2500\u2500 server.js\n\u2502   \u2502   \u2514\u2500\u2500 server.test.js\n\u2502   \u251c\u2500\u2500 /controllers\n\u2502   \u2502   \u251c\u2500\u2500 file1.js\n\u2502   \u2502   \u251c\u2500\u2500 file2.js\n\u2502   \u2502   \u2514\u2500\u2500 file3.test.js\n\u2502   \u251c\u2500\u2500 /models\n\u2502   \u2502   \u251c\u2500\u2500 file4.js\n\u2502   \u2502   \u251c\u2500\u2500 file5.js\n\u2502   \u2502   \u2514\u2500\u2500 file6.test.js\n\u2502   \u2514\u2500\u2500 /routes\n\u2502       \u251c\u2500\u2500 file7.js\n\u2502       \u251c\u2500\u2500 file8.js\n\u2502       \u2514\u2500\u2500 file9.test.js\n\u2502\n\u251c\u2500\u2500 /test\n\u2502   \u251c\u2500\u2500 integration.test.js\n\u2502   \u2514\u2500\u2500 e2e.test.js\n\u2502\n\u251c\u2500\u2500 /config\n\u2502   \u251c\u2500\u2500 config1.js\n\u2502   \u2514\u2500\u2500 config2.js\n</code></pre> <p>Example:</p> <pre><code>project/\n. `Hierarchical Structure`\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u251c\u2500\u2500 component1.js\n\u2502   \u2502   \u251c\u2500\u2500 component2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 service1.js\n\u2502   \u2502   \u251c\u2500\u2500 service2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 util1.js\n\u2502   \u2502   \u251c\u2500\u2500 util2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 index.js\n\u2502\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 webpack.config.js\n\u2502   \u251c\u2500\u2500 database.config.js\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u251c\u2500\u2500 component1.test.js\n\u2502   \u2502   \u251c\u2500\u2500 component2.test.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 service1.test.js\n\u2502   \u2502   \u251c\u2500\u2500 service2.test.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 util1.test.js\n\u2502       \u251c\u2500\u2500 util2.test.js\n\u2502       \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 API.md\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 assets/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.png\n\u2502   \u2502   \u251c\u2500\u2500 image2.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u2502   \u251c\u2500\u2500 style1.css\n\u2502   \u2502   \u251c\u2500\u2500 style2.css\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 ...\n</code></pre> <p>In the example, the project directory contains various subdirectories organized in a hierarchical structure:</p> <ul> <li>The <code>src/</code> directory contains the main source code files.</li> <li>The <code>components/</code> directory holds subdirectories for each component, with further subdirectories for their respective subcomponents.</li> <li>The <code>services/</code> directory contains subdirectories for each service, with further subdirectories for their respective submodules.</li> <li>The <code>utils/</code> directory includes utility files.</li> <li>The <code>config/</code> directory contains configuration files for the project, such as webpack and database configurations.</li> <li>The <code>tests/</code> directory mirrors the structure of the <code>src/</code> directory, with separate subdirectories for components, services, and corresponding test files for subcomponents and submodules.</li> <li>The <code>docs/</code> directory holds project documentation, including a <code>README.md</code> file, an <code>API.md</code> file, and potentially other documentation files.</li> <li>The <code>assets/</code> directory consists of subdirectories for images, stylesheets, and other project-related assets.</li> <li>The root directory also includes files like <code>.gitignore</code>, <code>LICENSE</code>, and other project-specific files.</li> </ul>"},{"location":"articles/project-layout/#133-modular-structure","title":"1.3.3. Modular Structure","text":"<p>A modular structure organizes files and folders into modules, which are self-contained units of code that can be imported and used in other parts of the project. The modular structure allows for better organization and separation of concerns, making it easier to maintain and expand the project over time. Each module can be developed and tested separately, and changes to one module are less likely to affect other parts of the project.</p> <p>The modular structure helps in organizing the project's codebase into logical units, promoting reusability, maintainability, and testability. This structure is useful for medium-sized projects with multiple components that have distinct responsibilities and functionality.</p> <pre><code>/project\n\u251c\u2500\u2500 /module1\n\u2502   \u251c\u2500\u2500 file1.js\n\u2502   \u251c\u2500\u2500 file2.js\n\u2502   \u2514\u2500\u2500 file3.test.js\n\u2502\n\u251c\u2500\u2500 /module2\n\u2502   \u251c\u2500\u2500 file4.js\n\u2502   \u251c\u2500\u2500 file5.js\n\u2502   \u2514\u2500\u2500 file6.test.js\n\u2502\n\u251c\u2500\u2500 /module3\n\u2502   \u251c\u2500\u2500 file7.js\n\u2502   \u251c\u2500\u2500 file8.js\n\u2502   \u2514\u2500\u2500 file9.test.js\n\u2502\n\u2514\u2500\u2500 main.js\n</code></pre> <p>Example:</p> <pre><code>project/\n\u251c\u2500\u2500 src/\n\u2502   . `Modular Structure`\n\u2502   \u251c\u2500\u2500 module1/\n\u2502   \u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component1.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component2.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service1.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service2.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 util1.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 util2.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 index.js\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 module2/\n\u2502   \u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component1.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component2.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service1.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service2.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 util1.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 util2.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 index.js\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 main.js\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 module1/\n\u2502   \u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 utils/\n\u2502   \u2502       \u251c\u2500\u2500 util1.test.js\n\u2502   \u2502       \u251c\u2500\u2500 util2.test.js\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 module2/\n\u2502   \u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 utils/\n\u2502   \u2502       \u251c\u2500\u2500 util1.test.js\n\u2502   \u2502       \u251c\u2500\u2500 util2.test.js\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 main.test.js\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 API.md\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 webpack.config.js\n\u2502   \u251c\u2500\u2500 database.config.js\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 assets/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.png\n\u2502   \u2502   \u251c\u2500\u2500 image2.png\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u2502   \u251c\u2500\u2500 style1.css\n\u2502   \u2502   \u251c\u2500\u2500 style2.css\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 ...\n</code></pre> <p>In the example, the project directory contains various subdirectories representing different aspects of the project:</p> <ul> <li>The <code>src/</code> directory contains modules (<code>module1</code> and <code>module2</code>), each with their own set of components, services, utilities, and an <code>index.js</code> file.</li> <li>The <code>tests/</code> directory mirrors the <code>src/</code> directory structure, with corresponding test files for each module component, service, and utility.</li> <li>The <code>docs/</code> directory holds project documentation, including a <code>README.md</code> file and an <code>API.md</code> file.</li> <li>The <code>config/</code> directory contains configuration files for the project, such as webpack configuration and database configuration.</li> <li>The <code>assets/</code> directory includes subdirectories for images, stylesheets, and other project-related assets.</li> <li>The <code>packages/</code> directory represents external packages or libraries used in the project.</li> <li>The root directory also contains files like <code>.gitignore</code>, <code>LICENSE</code>, and other project-specific files.</li> </ul>"},{"location":"articles/project-layout/#134-layered-structure","title":"1.3.4. Layered Structure","text":"<p>In a layered structure, files and folders are organized into layers or tiers that represent different levels of abstraction in the project. The layered structure helps in organizing the project codebase based on different responsibilities and concerns, promoting separation of concerns, modularity, and testability. Each layer focuses on specific aspects of the application, making it easier to understand and maintain the code.</p> <p>This structure is used for larger projects that require more organization and separation of concerns, and is often used in enterprise-level applications. The layered structure allows for a high degree of separation of concerns and modularity, which can make it easier to manage and scale a large, complex project. However, it can also be more challenging to navigate and understand for new contributors to the project.</p> <pre><code>/project\n\u251c\u2500\u2500 /presentation\n\u2502   \u251c\u2500\u2500 /controllers\n\u2502   \u2502   \u251c\u2500\u2500 file1.js\n\u2502   \u2502   \u2514\u2500\u2500 file2.js\n\u2502   \u251c\u2500\u2500 /views\n\u2502   \u2502   \u251c\u2500\u2500 file3.js\n\u2502   \u2502   \u2514\u2500\u2500 file4.js\n\u2502   \u2514\u2500\u2500 /templates\n\u2502       \u251c\u2500\u2500 file5.js\n\u2502       \u2514\u2500\u2500 file6.js\n\u2502\n\u251c\u2500\u2500 /application\n\u2502   \u251c\u2500\u2500 /services\n\u2502   \u2502   \u251c\u2500\u2500 file7.js\n\u2502   \u2502   \u251c\u2500\u2500 file8.js\n\u2502   \u2502   \u2514\u2500\u2500 file9.test.js\n\u2502   \u251c\u2500\u2500 /usecases\n\u2502   \u2502   \u251c\u2500\u2500 file10.js\n\u2502   \u2502   \u251c\u2500\u2500 file11.js\n\u2502   \u2502   \u2514\u2500\u2500 file12.test.js\n\u2502   \u2514\u2500\u2500 /repositories\n\u2502       \u251c\u2500\u2500 file13.js\n\u2502       \u2514\u2500\u2500 file14.test.js\n\u2502\n\u251c\u2500\u2500 /domain\n\u2502   \u251c\u2500\u2500 /entities\n\u2502   \u2502   \u251c\u2500\u2500 file15.js\n\u2502   \u2502   \u2514\u2500\u2500 file16.js\n\u2502   \u251c\u2500\u2500 /objects\n\u2502   \u2502   \u251c\u2500\u2500 file17.js\n\u2502   \u2502   \u2514\u2500\u2500 file18.js\n</code></pre> <p>Example:</p> <pre><code>project/\n. `Layered Structure`\n\u251c\u2500\u2500 presentation/\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u251c\u2500\u2500 component1.js\n\u2502   \u2502   \u251c\u2500\u2500 component2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 screens/\n\u2502   \u2502   \u251c\u2500\u2500 screen1.js\n\u2502   \u2502   \u251c\u2500\u2500 screen2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 styles/\n\u2502   \u2502   \u251c\u2500\u2500 style1.css\n\u2502   \u2502   \u251c\u2500\u2500 style2.css\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 domain/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 model1.js\n\u2502   \u2502   \u251c\u2500\u2500 model2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 service1.js\n\u2502   \u2502   \u251c\u2500\u2500 service2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 repositories/\n\u2502   \u2502   \u251c\u2500\u2500 repository1.js\n\u2502   \u2502   \u251c\u2500\u2500 repository2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 infrastructure/\n\u2502   \u251c\u2500\u2500 database/\n\u2502   \u2502   \u251c\u2500\u2500 dbConnection.js\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model1Schema.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model2Schema.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 migrations/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 migration1.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 migration2.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 externalAPIs/\n\u2502   \u2502   \u251c\u2500\u2500 api1.js\n\u2502   \u2502   \u251c\u2500\u2500 api2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 service1.js\n\u2502   \u2502   \u251c\u2500\u2500 service2.js\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 presentation/\n\u2502   \u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 component2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 screens/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 screen1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 screen2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 domain/\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 service2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 repositories/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 repository1.test.js\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 repository2.test.js\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 API.md\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 ...\n</code></pre> <p>In the example, the project directory contains various subdirectories organized in a layered structure:</p> <ul> <li>The <code>presentation/</code> directory represents the presentation layer, responsible for user interface components, screens, and styles.</li> <li>The <code>components/</code> directory holds individual component files.</li> <li>The <code>screens/</code> directory contains files representing different screens of the application.</li> <li> <p>The <code>styles/</code> directory holds CSS or styling files specific to the presentation layer.</p> </li> <li> <p>The <code>domain/</code> directory represents the domain layer, which encapsulates the business logic and models of the application.</p> </li> <li>The <code>models/</code> directory includes files representing domain models.</li> <li>The <code>services/</code> directory contains files representing domain services.</li> <li> <p>The <code>repositories/</code> directory holds files representing data repositories or data access objects (DAOs).</p> </li> <li> <p>The <code>infrastructure/</code> directory represents the infrastructure layer, which includes code related to databases, external APIs, and other infrastructure-related concerns.</p> </li> <li>The <code>database/</code> directory contains files related to database connectivity, schema models, and migrations.</li> <li>The <code>externalAPIs/</code> directory holds files representing integrations with external APIs.</li> <li> <p>The <code>services/</code> directory includes files representing infrastructure-specific services.</p> </li> <li> <p>The <code>tests/</code> directory mirrors the structure of the respective layers, with separate subdirectories for presentation, domain, and infrastructure tests.</p> </li> <li> <p>The <code>docs/</code> directory contains project documentation, including a <code>README.md</code> file, an <code>API.md</code> file, and potentially other documentation files.</p> </li> <li> <p>The root directory also includes files like <code>.gitignore</code>, <code>LICENSE</code>, and other project-specific files.</p> </li> </ul>"},{"location":"articles/project-layout/#135-component-based-structure","title":"1.3.5. Component-based Structure","text":"<p>The component-based structure promotes reusability, separation of concerns, and modularity by organizing the codebase around components and their related files. It makes it easier to locate and manage components, pages, services, and other project resources.</p> <pre><code>/src\n\u251c\u2500\u2500 /io\n\u2502   \u251c\u2500\u2500 io.h\n\u2502   \u251c\u2500\u2500 io.c\n\u2502   \u2514\u2500\u2500 io_test.c\n\u251c\u2500\u2500 /math\n\u2502   \u251c\u2500\u2500 math.h\n\u2502   \u251c\u2500\u2500 math.c\n\u2502   \u2514\u2500\u2500 math_test.c\n\u251c\u2500\u2500 /utils\n\u2502   \u251c\u2500\u2500 utils.h\n\u2502   \u251c\u2500\u2500 utils.c\n\u2502   \u2514\u2500\u2500 utils_test.c\n</code></pre>"},{"location":"articles/project-layout/#136-functional-based-structure","title":"1.3.6. Functional-based Structure","text":"<p>The function-based structure organizes the codebase around individual functions and features, making it easier to locate and manage specific functionality. It promotes modularity, testability, and the ability to focus on individual functions or features within the project.</p> <pre><code>/src\n\u251c\u2500\u2500 /utils\n\u2502   \u251c\u2500\u2500 io.h\n\u2502   \u251c\u2500\u2500 io.c\n\u2502   \u251c\u2500\u2500 io_test.c\n\u2502   \u251c\u2500\u2500 math.h\n\u2502   \u251c\u2500\u2500 math.c\n\u2502   \u251c\u2500\u2500 math_test.c\n\u2502   \u251c\u2500\u2500 utils.h\n\u2502   \u251c\u2500\u2500 utils.c\n\u2502   \u2514\u2500\u2500 utils_test.c\n</code></pre>"},{"location":"articles/project-layout/#137-task-based-structure","title":"1.3.7. Task-based Structure","text":"<p>The task-based structure organizes the project's codebase and resources around specific tasks or functionalities, making it easier to locate and manage task-specific files. It promotes modularity, testability, and the ability to focus on individual tasks within the project.</p> <pre><code>/src\n\u251c\u2500\u2500 /read\n\u2502   \u251c\u2500\u2500 read.h\n\u2502   \u251c\u2500\u2500 read.c\n\u2502   \u2514\u2500\u2500 read_test.c\n\u251c\u2500\u2500 /process\n\u2502   \u251c\u2500\u2500 process.h\n\u2502   \u251c\u2500\u2500 process.c\n\u2502   \u2514\u2500\u2500 process_test.c\n\u251c\u2500\u2500 /write \n\u2502   \u251c\u2500\u2500 write.h\n\u2502   \u251c\u2500\u2500 write.c\n\u2502   \u2514\u2500\u2500 write_test.c\n</code></pre>"},{"location":"articles/project-layout/#2-principle","title":"2. Principle","text":"<p>A project layout should be designed with the needs of the development team and the project in mind. The layout should be easy to understand, consistent, and well-documented, and should facilitate collaboration and efficient development practices.</p> <ul> <li> <p>Simplicity</p> <p>A good project layout should be simple and easy to understand. The layout should be easy to navigate and should not require users to have an in-depth understanding of the project's inner workings.</p> </li> <li> <p>Consistency</p> <p>The layout should be consistent across all files and directories. This makes it easier for users to find what they are looking for and reduces confusion.</p> </li> <li> <p>Modularity</p> <p>The layout should be modular, with each directory containing files that are related to a specific aspect of the project. This makes it easier to maintain and update the project, as changes can be made to a specific module without affecting the entire project.</p> </li> <li> <p>Scalability</p> <p>The layout should be scalable, meaning that it can be easily adapted to accommodate new features or functionality as the project grows.</p> </li> <li> <p>Flexibility</p> <p>The layout should be flexible enough to accommodate different development workflows and methodologies. For example, if the team uses a specific build system or testing framework, the project layout should be able to accommodate these tools.</p> </li> <li> <p>Documentation</p> <p>The project layout should be well-documented, with clear explanations of each directory and file. This makes it easier for new team members to get up to speed quickly and reduces confusion.</p> </li> <li> <p>Compatibility</p> <p>The layout should be compatible with the tools and systems used by the team. For example, if the team uses Git for version control, the layout should be compatible with Git's file structure and branching model.</p> </li> </ul>"},{"location":"articles/project-layout/#3-best-practice","title":"3. Best Practice","text":"<p>By following these best practices, developers can create a project layout that is easy to manage and maintain, and that facilitates efficient development practices.</p> <ul> <li> <p>Use a standard directory structure</p> <p>Use a standard directory structure that is commonly used in the programming community. This makes it easier for other developers to understand the layout of the project and find the files they need.</p> </li> <li> <p>Keep the layout simple</p> <p>The project layout should be simple and easy to understand. Avoid overly complex directory structures that may confuse developers.</p> </li> <li> <p>Separate source code from build artifacts</p> <p>Keep source code in a separate directory from build artifacts, such as object files and executables. This makes it easier to clean the build directory without affecting the source code.</p> </li> <li> <p>Use descriptive names</p> <p>Use descriptive names that accurately reflect the contents of each directory and file. Consistent naming makes it easier for team members to find what they are looking for and reduces confusion.</p> </li> <li> <p>Use version control</p> <p>Use a version control system like Git to manage the project. This makes it easier to track changes, collaborate with other developers, and revert changes if necessary.</p> </li> <li> <p>Document the layout</p> <p>Document the project layout in a README file or other documentation. This makes it easier for other developers to understand the layout and find the files they need.</p> </li> <li> <p>Include a clear README file</p> <p>Include a clear README file in the root directory that provides an overview of the project and instructions for building and running it. This makes it easier to get up quickly.</p> </li> <li> <p>Be consistent</p> <p>Be consistent with naming conventions, file organization, and directory structure throughout the project. This makes it easier for developers to find files and understand the layout.</p> </li> <li> <p>Use modular design</p> <p>Use a modular design to separate different aspects of the project. This makes it easier to manage and maintain the project, and allows for easier testing and debugging.</p> </li> <li> <p>Keep third-party dependencies in a separate directory</p> <p>Store third-party dependencies in a separate directory, such as <code>/external</code> or <code>/vendor</code>. This makes it easier to manage dependencies and avoid version conflicts.</p> </li> <li> <p>Include tests</p> <p>Store tests in a separate directory, such as <code>/tests</code>. This makes it easier to run tests and ensure that the project functions as expected.</p> </li> <li> <p>Use relative paths</p> <p>Use relative paths when referencing files and directories within the project. This makes it easier to move the project to a different directory or system without breaking file references.</p> </li> <li> <p>Minimize the number of directories</p> <p>Use the minimum number of directories needed to organize the project. Too many directories can make it difficult to navigate and understand the project.</p> </li> <li> <p>Keep the layout flexible</p> <p>Keep the project layout flexible enough to accommodate changes and updates over time. Don't be afraid to make changes to the layout as needed to improve organization and efficiency.</p> </li> </ul>"},{"location":"articles/project-layout/#4-terminology","title":"4. Terminology","text":"<p>Terminologies are used to describe the various components of a project layout and are important to understand when working on a software development project.</p> <ul> <li> <p>Directory</p> <p>A directory is a folder or container that holds files and other directories.</p> </li> <li> <p>File</p> <p>A file is a collection of data that is stored in a directory. Files can be text, code, images, or any other type of data.</p> </li> <li> <p>Root directory</p> <p>The root directory is the top-level directory in a file system. In Unix-based systems, the root directory is denoted by a forward slash (/).</p> </li> <li> <p>Source code</p> <p>Source code is the human-readable code that is written by developers to create software applications.</p> </li> <li> <p>Header file</p> <p>A header file is a file that contains function and variable declarations that are used in the source code.</p> </li> <li> <p>Build artifacts</p> <p>Build artifacts are the files that are generated during the build process. These can include object files, executables, and libraries.</p> </li> <li> <p>Test code</p> <p>Test code is code that is written to test the functionality of the software application.</p> </li> <li> <p>Documentation</p> <p>Documentation is written material that explains how the software application works, how to use it, and how to troubleshoot any issues.</p> </li> <li> <p>Dependency</p> <p>A dependency is a library or other software component that is required for the software application to run.</p> </li> <li> <p>Configuration</p> <p>Configuration files are files that contain settings and parameters that are used to configure the software application.</p> </li> <li> <p>Module</p> <p>A module is a self-contained unit of code that can be reused across multiple projects.</p> </li> <li> <p>Package</p> <p>A package is a collection of related modules, files, and directories that are grouped together for easy distribution and installation. In many programming languages, packages are used to organize code into reusable and shareable components, and they often have a specific directory structure and naming conventions. Packages can be installed and managed using package managers, which automate the process of downloading and installing dependencies and other required files.</p> </li> </ul>"},{"location":"articles/project-layout/#5-references","title":"5. References","text":"<ul> <li>Sentenz\u00a0build systems\u00a0article.</li> <li>Sentenz\u00a0docs as code\u00a0article.</li> <li>Sentenz\u00a0static site generators\u00a0article.</li> <li>Sentenz\u00a0data serialization formats\u00a0article.</li> </ul>"},{"location":"articles/risk-management/","title":"Risk Management","text":"<p>Risk management involves identifying, assessing, and mitigating potential risks that could impact the success of a software project. It includes steps like risk identification, analysis, prioritization, and implementing strategies to minimize or address those risks. Effective risk management helps ensure the project stays on track and delivers the desired outcomes.</p> <ul> <li>1. Category</li> <li>1.1. Risks</li> <li>1.2. Standards</li> <li>1.3. Frameworks</li> <li>1.4. Tools</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/risk-management/#1-category","title":"1. Category","text":""},{"location":"articles/risk-management/#11-risks","title":"1.1. Risks","text":"<p>Risks requires specific strategies and actions to effectively manage and mitigate potential risks throughout the software development lifecycle.</p> <ol> <li> <p>Project Risks</p> <p>Risks related to project planning, scheduling, resource allocation, and budgeting.</p> </li> <li> <p>Technical Risks</p> <p>Risks associated with the technical aspects of the software, such as technology selection, architecture, performance, and integration.</p> </li> <li> <p>Requirement Risks</p> <p>Risks arising from incomplete or changing requirements, leading to scope creep or unclear expectations.</p> </li> <li> <p>Resource Risks</p> <p>Risks related to the availability and skill levels of the development team, as well as external dependencies.</p> </li> <li> <p>Schedule Risks</p> <p>Risks that could cause delays in the project timeline, such as unexpected obstacles or dependencies.</p> </li> <li> <p>Budget Risks</p> <p>Risks related to cost overruns, budget constraints, or unanticipated expenses.</p> </li> <li> <p>Quality Risks</p> <p>Risks that may affect the quality of the software, including defects, testing issues, and inadequate user experience.</p> </li> <li> <p>Communication Risks</p> <p>Risks stemming from miscommunication or lack of collaboration among team members, stakeholders, or users.</p> </li> <li> <p>Market Risks</p> <p>Risks linked to changes in market conditions, user needs, or competitor actions that could impact the software's relevance and success.</p> </li> <li> <p>Legal and Compliance Risks</p> <p>Risks associated with intellectual property, licensing, data privacy, and regulatory compliance.</p> </li> <li> <p>Security Risks</p> <p>Risks related to vulnerabilities, data breaches, and cyber threats that could compromise the security of the software and its users.</p> </li> <li> <p>Change Management Risks</p> <p>Risks associated with managing changes in the software, such as updates, upgrades, or migrations.</p> </li> </ol>"},{"location":"articles/risk-management/#12-standards","title":"1.2. Standards","text":"<p>Standards offer valuable guidance and best practices for identifying, assessing, treating, and monitoring risks across various domains and industries. The choice of standard may depend on the specific industry, context, and scope of risk management needed for a particular project or organization.</p> <ol> <li> <p>ISO 31000</p> <p>Provides principles and guidelines for effective risk management practices that can be applied to any type of organization and industry.</p> </li> <li> <p>ISO/IEC 27001</p> <p>Focuses on information security management and includes risk assessment and management as integral components.</p> </li> <li> <p>ISO 22301</p> <p>Specifically addresses business continuity management and helps organizations prepare for and respond to disruptive events.</p> </li> <li> <p>ISO 15288</p> <p>Focuses on systems and software engineering life cycle processes, including risk management.</p> </li> <li> <p>ISO 14971</p> <p>Specifically for medical devices, this standard provides guidance on risk management in the development and use of medical devices.</p> </li> <li> <p>NIST SP 800-30</p> <p>A guide from the National Institute of Standards and Technology (NIST) that provides risk assessment guidance, particularly in the context of information technology.</p> </li> <li> <p>ISO 19600</p> <p>Focuses on compliance management systems, which can include risk management related to legal and regulatory compliance.</p> </li> <li> <p>ISO 20000</p> <p>Addresses service management, including risks related to the management and delivery of IT services.</p> </li> <li> <p>IEC 62443</p> <p>Specifically designed for the security of industrial automation and control systems, it provides guidelines and best practices for cybersecurity and risk management in industrial environments.</p> </li> </ol>"},{"location":"articles/risk-management/#13-frameworks","title":"1.3. Frameworks","text":"<p>Frameworks provide structured approaches to identifying, assessing, mitigating, and monitoring risks, helping organizations make informed decisions to manage uncertainties effectively. The choice of framework depends on the specific needs, goals, and industry context of the organization.</p> <ol> <li> <p>COSO ERM Framework</p> <p>The Committee of Sponsoring Organizations of the Treadway Commission's Enterprise Risk Management framework provides a comprehensive approach to managing risks across an organization.</p> </li> <li> <p>ISO 31000</p> <p>While it is primarily a standard, ISO 31000 also provides a framework for risk management practices that can be adapted to various industries and contexts.</p> </li> <li> <p>PMI Risk Management Framework</p> <p>From the Project Management Institute, this framework outlines processes for identifying, assessing, responding to, and monitoring risks in projects.</p> </li> <li> <p>FAIR (Factor Analysis of Information Risk)</p> <p>A quantitative risk assessment framework that helps organizations analyze and prioritize information security risks.</p> </li> <li> <p>CRAMM (CCTA Risk Analysis and Management Method)</p> <p>A risk assessment and management methodology specifically designed for information technology and security.</p> </li> <li> <p>M_o_R (Management of Risk)</p> <p>A framework developed by AXELOS for risk management in projects, programs, and portfolios.</p> </li> <li> <p>Octave (Operationally Critical Threat, Asset, and Vulnerability Evaluation)</p> <p>A framework focused on information security risk assessment and management.</p> </li> <li> <p>FRAP (Facilitated Risk Analysis Process)</p> <p>A simple and structured framework that facilitates group-based risk assessment discussions.</p> </li> <li> <p>ITIL (Information Technology Infrastructure Library)</p> <p>While primarily a framework for IT service management, ITIL also includes guidance on managing risks related to IT services.</p> </li> <li> <p>IRAM (Information Risk Assessment Methodology)</p> <p>A framework designed to assess and manage information security risks in organizations.</p> </li> </ol>"},{"location":"articles/risk-management/#14-tools","title":"1.4. Tools","text":"<p>Tools help streamline the risk management process, making it easier to identify, assess, mitigate, and monitor risks throughout the software development lifecycle. The choice of tool depends on factors such as the organization's needs, preferences, and the complexity of the software project.</p> <ol> <li> <p>Jira</p> <p>A popular project management and issue tracking tool that can be customized to manage and track software risks and mitigation efforts.</p> </li> <li> <p>Trello</p> <p>A visual collaboration tool that can be used to create risk boards and track risk-related tasks and actions.</p> </li> <li> <p>RiskWatch</p> <p>A platform that offers risk assessment and management solutions for various industries, including software development.</p> </li> <li> <p>RiskyProject</p> <p>Software specifically designed for risk management, providing tools for risk analysis, assessment, and mitigation.</p> </li> <li> <p>Microsoft Project</p> <p>A project management software that can be used for planning and managing software development projects, including risk management.</p> </li> <li> <p>Risk Register</p> <p>Excel or Google Sheets templates can be customized to create and maintain a risk register to document and track risks, their impacts, and mitigation strategies.</p> </li> <li> <p>Risk Assessment Tools</p> <p>Various specialized software tools provide quantitative risk assessment capabilities, such as Monte Carlo simulations.</p> </li> <li> <p>Lucidchart</p> <p>A diagramming tool that can be used to create visual representations of risks, their relationships, and mitigation strategies.</p> </li> <li> <p>Risk Management Software</p> <p>Dedicated risk management software solutions that offer features like risk identification, assessment, analysis, reporting, and collaboration.</p> </li> <li> <p>Confluence</p> <p>A collaboration and documentation tool that can be used to create and maintain risk-related documentation and share information among team members.</p> </li> </ol>"},{"location":"articles/risk-management/#2-principles","title":"2. Principles","text":"<p>The principles of risk management, as outlined in ISO 31000, provide a foundation for effective and systematic risk management practices. These principles guide organizations in managing risks in a structured and consistent manner.</p> <p>By following these principles, organizations can establish a robust and adaptable risk management approach that helps them identify, assess, mitigate, and monitor risks effectively, leading to better decision-making and achievement of objectives.</p> <ul> <li> <p>Integration into Organizational Processes</p> <p>Risk management should be integrated into an organization's overall governance, management, and decision-making processes.</p> </li> <li> <p>Structured and Comprehensive Approach</p> <p>Adopt a structured and comprehensive approach to risk management that addresses risks across the organization.</p> </li> <li> <p>Customization</p> <p>Tailor the risk management process to the organization's external and internal context, objectives, and needs.</p> </li> <li> <p>Inclusive Process</p> <p>Involve stakeholders at all levels in the risk management process to ensure a diversity of perspectives and expertise.</p> </li> <li> <p>Dynamic and Iterative</p> <p>Risk management should be an ongoing and iterative process that adapts to changing circumstances and information.</p> </li> <li> <p>Transparent and Informed Decisions</p> <p>Ensure that decisions are based on the best available information and are transparent and well-informed.</p> </li> <li> <p>Balanced Decision-Making</p> <p>Consider the potential benefits, costs, and uncertainties when making risk-informed decisions.</p> </li> <li> <p>Continual Improvement</p> <p>Regularly review and improve the risk management framework and processes to enhance effectiveness.</p> </li> <li> <p>Clear Communication</p> <p>Communicate risks, risk management activities, and decisions to relevant stakeholders in a clear and timely manner.</p> </li> <li> <p>Human and Cultural Factors</p> <p>Consider human behavior, attitudes, and the organization's culture when managing risks.</p> </li> <li> <p>Legal and Ethical Framework</p> <p>Ensure risk management practices adhere to applicable legal and ethical standards.</p> </li> <li> <p>Review and Evaluation</p> <p>Conduct regular reviews and evaluations of the risk management process to assess its performance and make improvements.</p> </li> </ul>"},{"location":"articles/risk-management/#3-best-practice","title":"3. Best Practice","text":"<p>By following these best practices, organizations can create a proactive and structured approach to risk management, leading to better decision-making, reduced negative impacts, and improved overall performance.</p> <ul> <li> <p>Risk Identification</p> <p>Thoroughly identify and document potential risks that could impact the project, process, or organization.</p> </li> <li> <p>Risk Assessment</p> <p>Evaluate each identified risk's likelihood and potential impact to prioritize and focus on the most critical ones.</p> </li> <li> <p>Risk Mitigation Planning</p> <p>Develop strategies and action plans to reduce, avoid, or transfer identified risks. Assign responsibilities and timelines for implementation.</p> </li> <li> <p>Regular Monitoring</p> <p>Continuously monitor and track identified risks to ensure that mitigation measures are effective and risks are under control.</p> </li> <li> <p>Clear Communication</p> <p>Maintain open and transparent communication with stakeholders regarding risks, their potential impacts, and the progress of mitigation efforts.</p> </li> <li> <p>Cross-functional Involvement</p> <p>Involve individuals from various disciplines and departments to gain diverse perspectives and expertise in risk management.</p> </li> <li> <p>Risk Tolerance</p> <p>Define and communicate the organization's risk tolerance levels to guide decision-making and risk response strategies.</p> </li> <li> <p>Scenario Planning</p> <p>Develop scenarios to understand potential outcomes of different risk situations and plan appropriate responses.</p> </li> <li> <p>Regular Review</p> <p>Periodically review and update risk assessments and mitigation plans to reflect changing circumstances and new information.</p> </li> <li> <p>Lessons Learned</p> <p>Analyze past projects or incidents to identify lessons learned and apply those insights to current and future risk management efforts.</p> </li> <li> <p>Data-Driven Approach</p> <p>Use data and analytics to inform risk assessments, track trends, and make informed decisions.</p> </li> <li> <p>Continual Improvement</p> <p>Continuously refine risk management processes based on experience, feedback, and changing organizational needs.</p> </li> <li> <p>Documentation</p> <p>Maintain comprehensive documentation of risk assessments, mitigation plans, decisions, and outcomes.</p> </li> <li> <p>Training and Awareness</p> <p>Provide training and raise awareness among employees about risk management concepts and practices.</p> </li> <li> <p>Crisis Management Plan</p> <p>Develop a clear and effective crisis management plan to handle severe risks that may escalate into crises.</p> </li> </ul>"},{"location":"articles/risk-management/#4-terminology","title":"4. Terminology","text":"<p>Terms provide a foundation for understanding and discussing risk management concepts and practices across different industries and contexts.</p> <ul> <li> <p>Risk</p> <p>The effect of uncertainty on objectives, often characterized by potential events or situations that may have positive or negative impacts.</p> </li> <li> <p>Risk Management</p> <p>The process of identifying, assessing, prioritizing, and mitigating risks to achieve objectives and make informed decisions.</p> </li> <li> <p>Risk Assessment</p> <p>The process of evaluating the likelihood and potential impact of identified risks.</p> </li> <li> <p>Risk Analysis</p> <p>The detailed examination of risks to understand their causes, consequences, and potential outcomes.</p> </li> <li> <p>Risk Mitigation</p> <p>The process of taking actions to reduce the probability or impact of a risk.</p> </li> <li> <p>Risk Response</p> <p>The strategy or plan put in place to address a specific risk, which may involve avoiding, accepting, transferring, or mitigating the risk.</p> </li> <li> <p>Risk Tolerance</p> <p>The level of risk an organization is willing to accept to achieve its objectives.</p> </li> <li> <p>Risk Appetite</p> <p>The amount of risk an organization is willing to take on to achieve its goals and objectives.</p> </li> <li> <p>Risk Register</p> <p>A comprehensive list of identified risks, including their descriptions, likelihood, potential impact, and mitigation strategies.</p> </li> <li> <p>Scenario Analysis</p> <p>Exploring different potential outcomes and their implications based on various risk scenarios.</p> </li> <li> <p>Risk Control</p> <p>Measures and actions implemented to manage and minimize the impact of risks.</p> </li> <li> <p>Risk Communication</p> <p>The process of sharing information about risks, their potential impacts, and mitigation efforts with stakeholders.</p> </li> <li> <p>Residual Risk</p> <p>The level of risk that remains after mitigation efforts have been applied.</p> </li> <li> <p>Risk Owner</p> <p>The individual or entity responsible for the management and mitigation of a specific risk.</p> </li> <li> <p>Risk Indicator</p> <p>A measurable or observable factor that provides insight into the potential presence or magnitude of a risk.</p> </li> <li> <p>Risk Response Plan</p> <p>A documented strategy outlining how a specific risk will be managed, including actions, responsibilities, and timelines.</p> </li> <li> <p>Risk Matrix</p> <p>A visual tool used to assess and prioritize risks based on their likelihood and potential impact.</p> </li> <li> <p>Risk Assessment Framework</p> <p>A structured approach for conducting risk assessments, often involving guidelines, processes, and tools.</p> </li> <li> <p>Contingency Plan</p> <p>A plan outlining steps to be taken if a specific risk or event occurs, aimed at minimizing negative impacts.</p> </li> <li> <p>Risk Transfer</p> <p>The process of shifting the financial burden of a risk to another party, such as through insurance or contracts.</p> </li> </ul>"},{"location":"articles/sanitizer/","title":"Sanitizer","text":"<p>Sanitizers are dynamic analysis tools that detect bugs such as buffer overflows or accesses, dangling pointers or different types of undefined behavior at runtime and prevent security vulnerabilities in C/C++ code.</p> <ul> <li>1. Category</li> <li>1.1. Valgrind<ul> <li>1.1.1. Memcheck</li> <li>1.1.5. Helgrind</li> <li>1.1.6. DRD</li> <li>1.1.2. Cachegrind</li> <li>1.1.3. Callgrind</li> <li>1.1.4. Massif</li> </ul> </li> <li>1.2. Google Sanitizer<ul> <li>1.2.1. AddressSanitizer (ASan)</li> <li>1.2.2. LeakSanitizer (LSan)</li> <li>1.2.5. MemorySanitizer (MSan)</li> <li>1.2.3. UndefinedBehaviorSanitizer (UBSan)</li> <li>1.2.4. ThreadSanitizer (TSan)</li> </ul> </li> </ul>"},{"location":"articles/sanitizer/#1-category","title":"1. Category","text":""},{"location":"articles/sanitizer/#11-valgrind","title":"1.1. Valgrind","text":"<p>Valgrind is a dynamic analysis tool that can be used to detect memory leaks, or buffer overflows errors in C/C++ code.</p> <p>NOTE Valgrind tools are not designed to be used simultaneously. Each tool serves a specific purpose and provides different types of analysis or debugging capabilities. Run one tool at a time to get the desired information about a program's behavior or performance.</p> <p>Benefits and Features:</p> <ul> <li> <p>Linux</p> <p>Support for <code>x86</code>, <code>AMD64</code>, <code>ARM32</code>, <code>ARM64</code>, <code>PPC32</code>, <code>PPC64BE</code>, <code>PPC64LE</code>, <code>S390X</code>, <code>MIPS32</code>, <code>MIPS64</code>.</p> </li> <li> <p>Android</p> <p>Support for <code>ARM</code>, <code>ARM64</code>, <code>MIPS32</code>, <code>X86</code>.</p> </li> <li> <p>Solaris</p> <p>Support for <code>X86</code>, <code>AMD64</code>.</p> </li> <li> <p>FreeBSD</p> <p>Support for <code>X86</code>, <code>AMD64</code>.</p> </li> <li> <p>MacOSX</p> <p>Support for <code>AMD64</code>.</p> </li> </ul> <p>Files and Folders:</p> <ul> <li><code>.valdrindrc</code> <p>Place <code>.valdrindrc</code> file in the root directory, Valgrind will automatically apply these settings when it runs. Remember to adjust the settings according to the specific needs of each project.</p> </li> </ul> <pre><code>--tool=memcheck --leak-check=yes\n--tool=cachegrind\n--tool=callgrind\n--tool=massif\n--tool=helgrind\n--tool=drd\n</code></pre>"},{"location":"articles/sanitizer/#111-memcheck","title":"1.1.1. Memcheck","text":"<p>Memcheck detects memory-management problems, primarily aimed at C and C++ programs.</p> <p>Files and Folders:</p> <ul> <li><code>my_program.c</code> <p>Create a memory leak by allocating memory and not freeing it.</p> </li> </ul> <pre><code>#include &lt;stdlib.h&gt;\n\nint main() {\n    int *memory_leak = malloc(sizeof(int));\n    return 0; // Triggering memory leak error\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile and Check <p>Compiling a program with gcc and checking for memory leaks with Memcheck.</p> </li> </ul> <pre><code>gcc -g -o my_program my_program.c\nvalgrind --tool=memcheck --leak-check=yes ./my_program\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>==12345== Memcheck, a memory error detector\n==12345== Command: ./my_program\n==12345==\n==12345== HEAP SUMMARY:\n==12345==     in use at exit: 4 bytes in 1 blocks\n==12345==   total heap usage: 1 allocs, 0 frees, 4 bytes allocated\n==12345==\n==12345== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==12345==    at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==12345==    by 0x400576: main (in /home/user/my_program)\n==12345==\n==12345== LEAK SUMMARY:\n==12345==    definitely lost: 4 bytes in 1 blocks\n==12345==    indirectly lost: 0 bytes in 0 blocks\n==12345==      possibly lost: 0 bytes in 0 blocks\n==12345==    still reachable: 0 bytes in 0 blocks\n==12345==         suppressed: 0 bytes in 0 blocks\n==12345==\n==12345== For counts of detected and suppressed errors, rerun with: -v\n==12345== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\n</code></pre>"},{"location":"articles/sanitizer/#115-helgrind","title":"1.1.5. Helgrind","text":"<p>Helgrind is a thread debugger for finding data races in multithreaded programs.</p> <p>Files and Folders:</p> <ul> <li><code>my_program.c</code> <p>Creates data races by having two threads increment a shared variable.</p> </li> </ul> <pre><code>#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt;\n\nint shared_variable = 0;\n\nvoid* thread_function(void* arg) {\n    shared_variable++;\n    return NULL;\n}\n\nint main() {\n    pthread_t thread1, thread2;\n    pthread_create(&amp;thread1, NULL, thread_function, NULL);\n    pthread_create(&amp;thread2, NULL, thread_function, NULL);\n    pthread_join(thread1, NULL);\n    pthread_join(thread2, NULL);\n    printf(\"Shared variable: %d\\n\", shared_variable);\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile and Detect <p>Detecting data races in a multithreaded program with Helgrind.</p> </li> </ul> <pre><code>gcc -g -pthread -o my_program my_program.c\nvalgrind --tool=helgrind ./my_program\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>==12345== Helgrind, a thread error detector\n==12345== Command: ./my_program\n==12345==\n==12345== Possible data race during read of size 4 at 0xXXXXXX by thread #2\n==12345==    at 0x4005XX: thread_function (my_program.c:XX)\n==12345==    by 0x4C2XXXX: start_thread (pthread_create.c:XXX)\n==12345==    by 0x5XXXXXX: clone (clone.S:XX)\n==12345==\n==12345== This conflicts with a previous write of size 4 by thread #1\n==12345==    at 0x4005XX: thread_function (my_program.c:XX)\n==12345==    by 0x4C2XXXX: start_thread (pthread_create.c:XXX)\n==12345==    by 0x5XXXXXX: clone (clone.S:XX)\n==12345==\n</code></pre>"},{"location":"articles/sanitizer/#116-drd","title":"1.1.6. DRD","text":"<p>DRD is a thread error detector that helps in identifying issues in multithreaded programs.</p> <p>Files and Folders:</p> <ul> <li><code>my_program.c</code> <p>Creates data races by having two threads increment a shared variable.</p> </li> </ul> <pre><code>#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt;\n\nint shared_variable = 0;\n\nvoid* thread_function(void* arg) {\n    shared_variable++;\n    return NULL;\n}\n\nint main() {\n    pthread_t thread1, thread2;\n    pthread_create(&amp;thread1, NULL, thread_function, NULL);\n    pthread_create(&amp;thread2, NULL, thread_function, NULL);\n    pthread_join(thread1, NULL);\n    pthread_join(thread2, NULL);\n    printf(\"Shared variable: %d\\n\", shared_variable);\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile and Detect <p>Detecting a thread error detector DRD</p> </li> </ul> <pre><code>gcc -g -pthread -o my_program my_program.c\nvalgrind --tool=drd ./my_program\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>==12345== DRD, a thread error detector\n==12345== Command: ./my_program\n==12345==\n==12345== Possible data race during read of size 4 at 0xXXXXXX by thread #2\n==12345==    at 0x4005XX: thread_function (my_program.c:XX)\n==12345==    by 0x4C2XXXX: start_thread (pthread_create.c:XXX)\n==12345==    by 0x5XXXXXX: clone (clone.S:XX)\n==12345==\n==12345== This conflicts with a previous write of size 4 by thread #1\n==12345==    at 0x4005XX: thread_function (my_program.c:XX)\n==12345==    by 0x4C2XXXX: start_thread (pthread_create.c:XXX)\n==12345==    by 0x5XXXXXX: clone (clone.S:XX)\n==12345==\n</code></pre>"},{"location":"articles/sanitizer/#112-cachegrind","title":"1.1.2. Cachegrind","text":"<p>Cachegrind is a cache profiler that analyzes cache use and identifies bottlenecks.</p> <p>Files and Folders:</p> <ul> <li><code>my_program.c</code> <p>Creates cache performance issues by accessing an array of integers.</p> </li> </ul> <pre><code>#include &lt;stdlib.h&gt;\n\nint main() {\n    int i, sum = 0;\n    int *array = malloc(1000 * sizeof(int));\n    for (i = 0; i &lt; 1000; i++) {\n        sum += array[i];\n    }\n    free(array);\n    return sum;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile and Profile <p>Profiling cache usage with Cachegrind.</p> </li> </ul> <pre><code>gcc -g -o my_program my_program.c\nvalgrind --tool=cachegrind ./my_program\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>==12345== Cachegrind, a cache and branch-prediction profiler\n==12345== Command: ./my_program\n==12345==\n--12345-- Cachegrind, a cache and branch-prediction profiler\n--12345-- For interactive control, run 'cg_annotate'\n--12345-- For a textual report, run 'cg_annotate --auto=yes'\n</code></pre>"},{"location":"articles/sanitizer/#113-callgrind","title":"1.1.3. Callgrind","text":"<p>Callgrind is an extension to Cachegrind, it provides call-graph generating and profiling.</p> <p>Files and Folders:</p> <ul> <li><code>my_program.c</code> <p>Creates a call-stack by calling a function.</p> </li> </ul> <pre><code>#include &lt;stdio.h&gt;\n\nvoid function() {\n    printf(\"Function called\\n\");\n}\n\nint main() {\n    function();\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile and Generate <p>Generating a call-graph with Callgrind.</p> </li> </ul> <pre><code>gcc -g -o my_program my_program.c\nvalgrind --tool=callgrind ./my_program\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>==12345== Callgrind, a call-graph generating cache profiler\n==12345== Command: ./my_program\n==12345==\n--12345-- Callgrind, a call-graph generating cache profiler\n--12345-- For interactive control, run 'callgrind_control'\n--12345-- For a textual report, run 'callgrind_annotate'\n</code></pre>"},{"location":"articles/sanitizer/#114-massif","title":"1.1.4. Massif","text":"<p>Massif is a heap profiler that measures how much heap memory a program uses.</p> <p>Files and Folders:</p> <ul> <li><code>my_program.c</code> <p>Allocates and frees memory to analyze heap usage with Massif.</p> </li> </ul> <pre><code>#include &lt;stdlib.h&gt;\n\nint main() {\n    void *block = malloc(1024 * 1024); // Allocate 1MB of memory\n    free(block);\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile and Analyze <p>Analyzing heap usage with Massif.</p> </li> </ul> <pre><code>gcc -g -o my_program my_program.c\nvalgrind --tool=massif ./my_program\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>==12345== Massif, a heap profiler\n==12345== Command: ./my_program\n==12345==\n--12345-- Massif, a heap profiler\n--12345-- For a detailed snapshot, run 'ms_print'\n</code></pre>"},{"location":"articles/sanitizer/#12-google-sanitizer","title":"1.2. Google Sanitizer","text":"<p>The Google Sanitizers are a collection of tools integrated into compilers, designed to detect various types of bugs.</p> <p>NOTE <code>Google Sanitizers</code> are not designed to be used simultaneously. Each sanitizer serves a specific purpose and provides different types of analysis or debugging capabilities. Compile and run a program with one sanitizer at a time to get the desired information about a program\u2019s behavior or performance. Using multiple sanitizers at once could lead to conflicts and potentially inaccurate results.</p> <p>Benefits and Features:</p> <ul> <li> <p>LLVM/Clang</p> <p>Support for <code>AddressSanitizer (ASan)</code>, <code>LeakSanitizer (LSan)</code>, <code>UndefinedBehaviorSanitizer (UBSan)</code>, <code>ThreadSanitizer (TSan)</code>, <code>MemorySanitizer (MSan)</code>, <code>DataFlowSanitizer (DFSan)</code>.</p> </li> <li> <p>GCC/G++</p> <p>Support for <code>AddressSanitizer (ASan)</code>, <code>LeakSanitizer (LSan)</code>, <code>UndefinedBehaviorSanitizer (UBSan)</code>, <code>ThreadSanitizer (TSan)</code>, <code>MemorySanitizer (MSan)</code>.</p> </li> <li> <p>MSVC</p> <p>Support for <code>AddressSanitizer (ASan)</code>.</p> </li> </ul>"},{"location":"articles/sanitizer/#121-addresssanitizer-asan","title":"1.2.1. AddressSanitizer (ASan)","text":"<p>AddressSanitizer (ASan) is a dynamic analysis tool that detects memory access errors, such as out-of-bounds memory accesses and use-after-free bugs.</p> <p>Files and Folders:</p> <ul> <li><code>my_file.cpp</code> <p>Out-of-bounds access, triggering ASan error.</p> </li> </ul> <pre><code>#include &lt;iostream&gt;\n\nint main() {\n    int* array = new int[10];\n    array[11] = 42; // Triggering ASan error\n    delete[] array;\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile <p>Compiling with the sanitizer flag <code>-fsanitize=address</code>.</p> </li> </ul> <pre><code>g++ -fsanitize=address -std=c++11 my_file.cpp -o my_executable\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>=================================================================\n==12345==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60200000001a at pc 0x000000400781 bp 0x7ffd1c8b4d30 sp 0x7ffd1c8b4d28\nWRITE of size 1 at 0x60200000001a thread T0\n    #0 0x400780 in main /path/to/my_program.c:5\n    #1 0x7f2e1d3a082f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)\n    #2 0x400608 in _start (/path/to/my_program+0x400608)\n\n0x60200000001a is located 0 bytes to the right of 10-byte region [0x602000000010,0x60200000001a)\nallocated by thread T0 here:\n    #0 0x7f2e1d7a7602 in malloc (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x98602)\n    #1 0x40076d in main /path/to/my_program.c:4\n    #2 0x7f2e1d3a082f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /path/to/my_program.c:5 main\nShadow bytes around the buggy address:\n  0x0c047fff7fd0: fa fa 00 00 00 00 00 00 00 00 00 00 00 00 00 fa\n  0x0c047fff7fe0: fa fa 00 00 00 00 00 00 00 00 00 00 00 00 00 fa\n  0x0c047fff7ff0: fa fa 00 00 00 00 00 00 00 00 00 00 00 00 00 fa\n  0x0c047fff8000: fa fa 00 00 00 00 00 00 00 00 00 00 00 00 00 fa\n  0x0c047fff8010: fa fa 00 00 00 00 00 00 00 00 00 00 00 00 00 fa\n=&gt;0x0c047fff8020: fa fa 00 00 00 00 00 00 00 00 00 00 00[02]fa fa\n  0x0c047fff8030: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c047fff8040: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c047fff8050: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c047fff8060: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c047fff8070: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack partial redzone:   f4\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n==12345==ABORTING\n</code></pre>"},{"location":"articles/sanitizer/#122-leaksanitizer-lsan","title":"1.2.2. LeakSanitizer (LSan)","text":"<p>LeakSanitizer (LSan) is a dynamic analysis tool that detects memory leaks.</p> <p>Files and Folders:</p> <ul> <li><code>my_file.cpp</code> <p>Missing delete statement, triggering LSan error (leak).</p> </li> </ul> <pre><code>#include &lt;iostream&gt;\n\nint main() {\n    int* leaked_data = new int;\n    // Triggering LSan error\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile <p>Compiling with the sanitizer flag <code>-fsanitize=leak</code>.</p> </li> </ul> <pre><code>g++ -fsanitize=leak -std=c++11 my_file.cpp -o my_executable\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>=================================================================\n==12345==ERROR: LeakSanitizer: detected memory leaks\n\nDirect leak of 10 byte(s) in 1 object(s) allocated from:\n    #0 0x4c2b0d in malloc (/path/to/my_program+0x4c2b0d)\n    #1 0x4a5b4c in main /path/to/my_program.c:4:23\n    #2 0x7f6e38c5783f in __libc_start_main /build/glibc-e6zv40/glibc-2.23/csu/../csu/libc-start.c:291\n\nSUMMARY: LeakSanitizer: 10 byte(s) leaked in 1 allocation(s).\n</code></pre>"},{"location":"articles/sanitizer/#125-memorysanitizer-msan","title":"1.2.5. MemorySanitizer (MSan)","text":"<p>MemorySanitizer (MSan) is a dynamic analysis tool that detects uninitialized memory reads.</p> <p>Files and Folders:</p> <ul> <li><code>my_file.cpp</code> <p>Use of uninitialized memory, triggering MSan error.</p> </li> </ul> <pre><code>#include &lt;iostream&gt;\n\nint main() {\n    int* uninitialized_data = new int;\n    std::cout &lt;&lt; *uninitialized_data &lt;&lt; std::endl; // Triggering MSan error\n    delete uninitialized_data;\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile <p>Compiling with the sanitizer flag <code>-fsanitize=memory</code>.</p> </li> </ul> <pre><code>g++ -fsanitize=memory -std=c++11 my_file.cpp -o my_executable\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>==12345==WARNING: MemorySanitizer: use-of-uninitialized-value\n    #0 0x4a5b2c in main /path/to/my_program.c:5:12\n    #1 0x7f6e38c5783f in __libc_start_main /build/glibc-e6zv40/glibc-2.23/csu/../csu/libc-start.c:291\n    #2 0x419a68 in _start (/path/to/my_program+0x419a68)\n\n  Uninitialized value was created by a stack allocation\n    #0 0x4a5a40 in main /path/to/my_program.c:3:5\n\nSUMMARY: MemorySanitizer: use-of-uninitialized-value /path/to/my_program.c:5:12 in main\nExiting\n</code></pre>"},{"location":"articles/sanitizer/#123-undefinedbehaviorsanitizer-ubsan","title":"1.2.3. UndefinedBehaviorSanitizer (UBSan)","text":"<p>UndefinedBehaviorSanitizer (UBSan) is a dynamic analysis tool that detects undefined behavior, such as signed integer overflow and division by zero.</p> <p>Files and Folders:</p> <ul> <li><code>my_file.cpp</code> <p>Division by zero, triggering UBSan error.</p> </li> </ul> <pre><code>#include &lt;iostream&gt;\n\nint main() {\n    int x = 10;\n    int y = 0;\n    int result = x / y; // Triggering UBSan error\n    std::cout &lt;&lt; result &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile <p>Compiling with the sanitizer flag <code>-fsanitize=undefined</code>.</p> </li> </ul> <pre><code>g++ -fsanitize=undefined -std=c++11 my_file.cpp -o my_executable\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>/path/to/my_program.c:6:12: runtime error: division by zero\n    #0 0x4a5b2c in main /path/to/my_program.c:6:12\n    #1 0x7f6e38c5783f in __libc_start_main /build/glibc-e6zv40/glibc-2.23/csu/../csu/libc-start.c:291\n    #2 0x419a68 in _start (/path/to/my_program+0x419a68)\n\nSUMMARY: UndefinedBehaviorSanitizer: division by zero /path/to/my_program.c:6:12 in main\n</code></pre>"},{"location":"articles/sanitizer/#124-threadsanitizer-tsan","title":"1.2.4. ThreadSanitizer (TSan)","text":"<p>ThreadSanitizer (TSan) is a tool that detects data races and synchronization bugs in multithreaded code.</p> <p>Files and Folders:</p> <ul> <li><code>my_file.cpp</code> <p>Data race, triggering TSan error.</p> </li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\nint shared_data = 0;\n\nvoid ThreadFunction() {\n    shared_data++; // Triggering TSan error\n}\n\nint main() {\n    std::thread t1(ThreadFunction);\n    std::thread t2(ThreadFunction);\n    t1.join();\n    t2.join();\n    return 0;\n}\n</code></pre> <p>Commands and Operations:</p> <ul> <li>Compile <p>Compiling with the sanitizer flag <code>-fsanitize=thread</code>.</p> </li> </ul> <pre><code>g++ -fsanitize=thread -std=c++11 -pthread my_file.cpp -o my_executable\n</code></pre> <p>Output and Result:</p> <ul> <li><code>output.log</code></li> </ul> <pre><code>WARNING: ThreadSanitizer: data race (pid=12345)\n  Write of size 4 at 0x0000014c3b20 by thread T1:\n    #0 increment /path/to/my_program.c:6 (my_program+0x000000400f2d)\n\n  Previous write of size 4 at 0x0000014c3b20 by thread T2:\n    #1 increment /path/to/my_program.c:6 (my_program+0x000000400f2d)\n\n  Location is global 'shared_variable' of size 4 at 0x0000014c3b20 (my_program+0x0000014c3b20)\n\nThread T1 (tid=12346, running) created by main thread at:\n    #2 main /path/to/my_program.c:14 (my_program+0x0000004010f1)\n\nThread T2 (tid=12347, running) created by main thread at:\n    #3 main /path/to/my_program.c:15 (my_program+0x0000004010f1)\n\nSUMMARY: ThreadSanitizer: data race /path/to/my_program.c:6 in increment\n</code></pre>"},{"location":"articles/serial-communication/","title":"Serial Communication // TODO","text":""},{"location":"articles/software-analysis/","title":"Software Analysis","text":"<p>Software Analysis is the process of evaluating software systems structure, behavior, and quality. The purpose of software analysis is to identify potential issues, risks, applications to identify potential risks, improve quality and reliability, and ensure compliance with organizational policies and industry regulations.</p> <ul> <li>1. Category</li> <li>1.1. Code Analysis<ul> <li>1.1.1. Requirements Analysis</li> <li>1.1.2. Static Analysis</li> <li>1.1.3. Dynamic Analysis</li> <li>1.1.4. Security Analysis</li> <li>1.1.5. Code Quality Analysis</li> </ul> </li> <li>1.2. Software Composition Analysis<ul> <li>1.2.1. Component Identification</li> <li>1.2.2. Vulnerability Analysis</li> <li>1.2.3. License Compliance Analysis</li> <li>1.2.4. Bill of Materials (BOM)</li> </ul> </li> <li>1.3. Tools<ul> <li>1.3.1. Linter</li> <li>1.3.2. Sanitizer</li> </ul> </li> <li>2. References</li> </ul>"},{"location":"articles/software-analysis/#1-category","title":"1. Category","text":""},{"location":"articles/software-analysis/#11-code-analysis","title":"1.1. Code Analysis","text":"<p>Code Analysis is a process of evaluating the quality and security of software source code. It involves examining the code for potential issues, including coding errors, performance problems, and security vulnerabilities.</p> <p>Software analysis can be performed at various stages of the software development life cycle (SDLC), including during requirements analysis, design, coding, testing, and maintenance. In modern DevSecOps environment, code analysis has galvanized the shift left paradigm.</p>"},{"location":"articles/software-analysis/#111-requirements-analysis","title":"1.1.1. Requirements Analysis","text":"<p>Requirements analysis is a crucial phase in the software development life cycle (SDLC) that involves understanding, documenting, and validating the requirements of a software system. The goal is to elicit and analyze user needs and expectations to ensure that the resulting software meets those requirements effectively.</p> <p>Requirements analysis sets the foundation for the development process, guiding subsequent phases such as design, implementation, and testing. Conducting requirements analysis reduces the risk of scope creep, improves software quality, and delivers a product that meets stakeholder expectations.</p> <p>Features of Requirements Analysis:</p> <ol> <li> <p>Elicitation</p> <p>This involves gathering requirements from stakeholders, including end-users, customers, and other relevant parties.</p> <p>Techniques for Requirements Elicitation:</p> <p>NOTE Combination of techniques is often used to ensure comprehensive requirements elicitation. The choice of techniques should be tailored to the specific project and stakeholders involved, considering factors such as time constraints, project complexity, and availability of resources.</p> <ul> <li> <p>Interviews</p> <p>One-on-one or group interviews with stakeholders are conducted to gather their perspectives, insights, and requirements. It allows for in-depth discussions, clarifications, and follow-up questions to understand their needs and expectations.</p> </li> <li> <p>Questionnaires and Surveys</p> <p>Questionnaires and surveys are useful for collecting a large amount of information from a wide range of stakeholders. They can be distributed electronically or in print format and enable stakeholders to provide their requirements in a structured manner.</p> </li> <li> <p>Workshops and Focus Groups</p> <p>These interactive sessions involve bringing together multiple stakeholders to collaborate, brainstorm, and discuss requirements. Workshops foster open communication, knowledge sharing, and consensus building among participants.</p> </li> <li> <p>Observation</p> <p>Observing users in their natural work environment can provide valuable insights into their tasks, workflows, and pain points. This technique helps in understanding the context of use and identifying requirements that might not be explicitly expressed by stakeholders.</p> </li> <li> <p>Document Analysis</p> <p>Analyzing existing documentation such as user manuals, business process documents, and system specifications can provide insights into current requirements or serve as a starting point for requirements gathering.</p> </li> <li> <p>Use Cases and User Stories</p> <p>Use cases and user stories are narrative descriptions of system behavior from an end-user perspective. They help capture functional requirements by describing specific interactions and scenarios in which the software will be used.</p> </li> <li> <p>Ethnographic Studies</p> <p>Ethnographic studies involve immersing analysts in the environment where the software will be used, enabling them to understand the users' needs, behaviors, and cultural context. This technique is particularly useful for gathering requirements in complex or specialized domains.</p> </li> <li> <p>Brainstorming</p> <p>Brainstorming sessions encourage creative thinking and idea generation. Participants freely share their requirements, ideas, and suggestions, fostering a collaborative environment that can lead to innovative solutions.</p> </li> <li> <p>Protocols and JAD (Joint Application Development) Sessions</p> <p>These structured sessions involve intensive collaboration between stakeholders, end-users, and development teams to elicit requirements, resolve conflicts, and reach consensus in a facilitated environment.</p> </li> </ul> </li> <li> <p>Requirements Documentation</p> <p>Once the requirements are elicited, they need to be documented in a clear and structured manner. This typically involves creating artifacts such as requirement documents, use cases, user stories, and process models. The documentation should capture functional requirements (what the software should do) as well as non-functional requirements (performance, security, usability, etc.).</p> </li> <li> <p>Analysis and Prioritization</p> <p>The elicited requirements are then analyzed to identify any conflicts, inconsistencies, or missing information. It is important to ensure that the requirements are complete, consistent, and feasible.</p> <p>Techniques for Requirements Prioritization:</p> <ul> <li>MoSCoW Method</li> <li>Must-have     &gt; Requirements that are essential for the software system to be considered successful. They represent the core functionality or features that are crucial and must be implemented in the current version.</li> <li>Should-have     &gt; Requirements that important but not critical for the current release. They are desirable features that would enhance the system usability or value.</li> <li>Could-have     &gt; Requirements that are nice-to-have features that are desirable but not necessary for the current release. They provide additional functionality or benefits that can be considered if time and resources permit.</li> <li> <p>Won't-have (or Would-like-to-have)     &gt; Requirements that are explicitly agreed upon not to be included in the current release. They are typically deferred to future versions or not pursued at all due to resource constraints, time limitations, or other project constraints. Won't-have requirements are generally revisited and considered in subsequent releases or iterations.</p> </li> <li> <p>Kano Model</p> <p>The Kano model helps categorize requirements based on their impact on customer satisfaction. It classifies requirements into five categories: Must-be, One-dimensional, Attractive, Indifferent, and Reverse. Must-be requirements are basic expectations that must be met. One-dimensional requirements directly influence customer satisfaction. Attractive requirements are unexpected features that delight customers. Indifferent requirements have no significant impact, and Reverse requirements may have a negative impact if included.</p> </li> </ul> </li> <li> <p>Validation and Verification</p> <p>Requirements validation ensures that the documented requirements accurately reflect the stakeholders needs. Verification involves reviewing the requirements for clarity, correctness, and feasibility.</p> <p>Techniques for Requirements Validation and Verification:</p> <p>NOTE Combination of techniques to validate and verify software systems, ensuring that they meet the specified requirements, function correctly, and adhere to quality standards. The selection of techniques depends on the nature of the software, project constraints, and the specific objectives of the validation and verification activities.</p> <ul> <li> <p>Reviews and Inspections</p> <p>These techniques involve systematic examination of software artifacts, such as requirements documents, design specifications, source code, and test cases, to identify errors, inconsistencies, and quality issues. Reviews and inspections can be conducted by a team of stakeholders, developers, or independent reviewers.</p> </li> <li> <p>Walkthroughs</p> <p>In a walkthrough, the software artifacts are presented to stakeholders or subject matter experts who provide feedback and identify potential issues. It is a collaborative process where the presenter explains the software and receives input and suggestions for improvement.</p> </li> <li> <p>Prototyping</p> <p>Prototyping involves building a simplified or partial version of the software system to validate and refine the requirements. Users and stakeholders can interact with the prototype to provide feedback on its functionality, usability, and design, helping to refine and validate the requirements.</p> </li> </ul> </li> <li> <p>Requirements Traceability</p> <p>Establishing traceability links between requirements and other SDLC artifacts is important to track and manage changes. Traceability helps ensure that all requirements are addressed and provides a basis for impact analysis when changes occur.</p> </li> <li> <p>Communication and Collaboration</p> <p>Effective communication and collaboration with stakeholders, developers, and other team members are crucial throughout the requirements analysis process. Clear channels of communication should be established, and feedback loops should be implemented to gather continuous input and address concerns.</p> </li> </ol>"},{"location":"articles/software-analysis/#112-static-analysis","title":"1.1.2. Static Analysis","text":"<p>Static analysis is a software analysis technique that examines software artifacts, such as source code, design documents, or models, without executing the program. It analyzes the structure, syntax, and semantics of the code to identify potential issues, vulnerabilities, and quality concerns. Static analysis helps detect defects, improve code quality, and ensure adherence to coding standards.</p> <p>Static analysis can be performed manually by developers or using specialized software tools. The tools apply a set of predefined rules and algorithms to analyze the code and generate reports highlighting potential issues.</p> <p>NOTE Static analysis has limitations and may generate false positives or miss certain types of issues. Static analysis should be used in conjunction with other testing and analysis techniques to achieve a comprehensive assessment of software quality.</p> <p>Features of Static Analysis:</p> <ol> <li> <p>Code Quality Analysis</p> <p>Static analysis tools can perform code quality checks to identify coding issues that might lead to bugs or make the code difficult to understand and maintain. This includes detecting coding violations, such as unused variables, dead code, inconsistent naming conventions, and potential memory leaks.</p> </li> <li> <p>Bug Detection</p> <p>Static analysis tools can identify common coding errors, such as null pointer dereferences, array out-of-bounds access, and resource leaks. By analyzing the code paths and performing data flow analysis, they can detect potential defects and help improve code reliability.</p> </li> <li> <p>Security Vulnerability Detection</p> <p>Static analysis is effective in identifying security vulnerabilities and weaknesses in the code that could be exploited by attackers. It can detect issues like SQL injection, cross-site scripting (XSS), buffer overflows, and insecure cryptographic practices. By analyzing the code for security flaws, static analysis helps developers address potential risks early in the development process.</p> </li> <li> <p>Performance Analysis</p> <p>Static analysis tools can analyze the code and provide insights into potential performance bottlenecks and inefficiencies. This includes identifying expensive function calls, unnecessary memory allocations, or loops that could be optimized. By optimizing the code based on static analysis findings, developers can improve the software's overall performance.</p> </li> <li> <p>Compliance and Standards Enforcement</p> <p>Static analysis can enforce coding standards and industry best practices by flagging violations during the analysis process. It helps ensure that the code adheres to specific coding guidelines, naming conventions, and other standards defined by the organization or industry.</p> </li> <li> <p>Documentation and Visualization</p> <p>Static analysis tools often provide documentation and visual representations of the code structure, dependencies, and relationships. This can assist developers in understanding complex codebases, identifying architectural issues, and facilitating code reviews.</p> </li> </ol>"},{"location":"articles/software-analysis/#113-dynamic-analysis","title":"1.1.3. Dynamic Analysis","text":"<p>Dynamic analysis is a software analysis technique that involves observing and analyzing the behavior of a software system during its execution. It helps uncover defects, validate functionality, memory leaks, assess performance, and understand system behavior under different conditions.</p> <p>Executing the software and analyzing its runtime characteristics complement software analysis techniques and assists in delivering robust, reliable, and high-performing software applications.</p> <p>Elemants of Dynamic Analysis:</p> <ol> <li> <p>Testing</p> <p>Dynamic analysis plays a crucial role in software testing. It involves designing and executing test cases to exercise the software system and observe its behavior. By comparing the actual results with expected outcomes, dynamic analysis helps identify bugs, logic errors, and functional issues. Various testing techniques, such as unit testing, integration testing, system testing, and acceptance testing, utilize dynamic analysis to validate the software against specified requirements.</p> </li> <li> <p>Debugging</p> <p>Dynamic analysis is instrumental in the debugging process, which involves identifying, analyzing, and fixing software defects. By observing the program's execution flow, variable values, and error conditions, dynamic analysis helps pinpoint the root causes of issues and provides insights for troubleshooting and resolving them.</p> </li> <li> <p>Performance Analysis</p> <p>Dynamic analysis techniques, such as profiling, are used to evaluate the performance characteristics of a software system. Profiling tools collect runtime data, such as method execution times, memory usage, and resource utilization, to identify performance bottlenecks, memory leaks, and inefficient algorithms. This information helps optimize the software for better efficiency and resource management.</p> </li> <li> <p>Code Coverage Analysis</p> <p>Dynamic analysis can determine the extent to which the code is exercised during testing. Code coverage analysis measures the percentage of code statements, branches, or paths that are executed during testing. It helps assess the thoroughness of testing and identifies areas of the code that have not been adequately tested, potentially indicating gaps in test coverage.</p> </li> <li> <p>Security Testing</p> <p>Dynamic analysis techniques are used to assess the security of software systems. This includes techniques like penetration testing, where the system is subjected to controlled attacks to identify vulnerabilities and potential security flaws. Dynamic analysis helps identify security weaknesses, such as input validation issues, privilege escalation, or access control vulnerabilities.</p> </li> <li> <p>Performance Profiling</p> <p>Dynamic analysis can profile the runtime behavior of a software system to identify performance bottlenecks, memory leaks, and inefficiencies. Profiling tools collect data on function execution times, method calls, memory usage, and I/O operations. This information helps developers optimize the software for improved performance.</p> </li> </ol>"},{"location":"articles/software-analysis/#114-security-analysis","title":"1.1.4. Security Analysis","text":"<p>Security analysis is a software analysis technique focused on assessing the security posture of a software system. It involves identifying vulnerabilities, weaknesses, and potential risks in the software design, implementation, configuration, and deployment. The objective of security analysis is to uncover security flaws and recommend mitigations to protect the system against unauthorized access, data breaches, and malicious attacks.</p> <p>The findings from security analysis are used to prioritize and address identified vulnerabilities through security patches, code fixes, configuration changes, and the implementation of additional security controls. It is an ongoing process that should be performed regularly to ensure that the software system remains secure and resilient to evolving threats.</p> <p>Features of Security Analysis:</p> <ol> <li> <p>Threat Modeling</p> <p>Security analysis often starts with threat modeling, which involves identifying potential threats, attack vectors, and security risks to the software system. This helps in understanding the system's security requirements and designing appropriate security controls.</p> </li> <li> <p>Penetration Testing</p> <p>Penetration testing, also known as ethical hacking, involves simulating real-world attacks to assess the security vulnerabilities of the software system. Skilled security professionals attempt to exploit weaknesses and gain unauthorized access to the system. The findings are then used to improve security measures and address identified vulnerabilities.</p> </li> <li> <p>Vulnerability Assessment</p> <p>Vulnerability assessment involves scanning the software system for known security vulnerabilities and weaknesses. This includes using automated tools to identify common vulnerabilities, such as outdated software versions, misconfigurations, weak authentication mechanisms, or inadequate access controls.</p> </li> <li> <p>Code Review</p> <p>Security analysis often includes reviewing the source code of the software system to identify coding errors and security vulnerabilities. Manual or automated code review techniques can be employed to check for common coding mistakes, such as input validation issues, SQL injection, cross-site scripting (XSS), buffer overflows, or insecure cryptographic practices.</p> </li> <li> <p>Security Standards and Compliance</p> <p>Security analysis may involve evaluating the software system's compliance with security standards and regulations, such as the Payment Card Industry Data Security Standard (PCI DSS) or the General Data Protection Regulation (GDPR). This includes assessing the system's adherence to security best practices, privacy requirements, and industry-specific security guidelines.</p> </li> <li> <p>Secure Design and Architecture Review</p> <p>Security analysis may involve evaluating the design and architecture of the software system to identify potential security weaknesses. This includes analyzing the security controls, access control mechanisms, data protection measures, and overall resilience of the system against various attack vectors.</p> </li> <li> <p>Secure Configuration Assessment</p> <p>Security analysis may assess the security configuration of the software system and associated components. This includes reviewing configurations of databases, servers, firewalls, network devices, and other infrastructure components to ensure that they are appropriately configured and hardened against security threats.</p> </li> </ol>"},{"location":"articles/software-analysis/#115-code-quality-analysis","title":"1.1.5. Code Quality Analysis","text":"<p>Code quality analysis is a software analysis technique that focuses on evaluating the quality of the source code of a software system. It involves assessing various aspects of the code, such as its structure, readability, maintainability, adherence to coding standards, and potential for defects. The objective of code quality analysis is to identify coding issues, improve the quality of the codebase, and facilitate long-term maintainability and extensibility.</p> <p>Code quality analysis can be done manually through code reviews or using automated code analysis tools and linters. The tools scan the codebase, apply predefined rules and metrics, and generate reports highlighting areas of concern and improvement opportunities.</p> <p>Conducting code quality analysis identifies and addresses coding issues early in the development process, leading to better code quality, maintainability, and long-term software sustainability.</p> <ol> <li> <p>Coding Standards and Best Practices</p> <p>Code quality analysis often involves enforcing and assessing adherence to coding standards and best practices. Coding standards define guidelines for code formatting, naming conventions, indentation, comments, and other coding style aspects. Tools and analysis techniques are used to check if the code follows the prescribed coding standards, which helps improve code consistency and readability.</p> </li> <li> <p>Code Complexity</p> <p>Code quality analysis assesses the complexity of the code to identify areas that might be difficult to understand, test, or maintain. Code complexity metrics, such as cyclomatic complexity or nesting depth, provide quantitative measures of code complexity. High complexity can indicate a higher likelihood of bugs and decreased maintainability. By identifying complex code sections, developers can refactor or simplify them for better code quality.</p> </li> <li> <p>Code Smells and Anti-patterns</p> <p>Code quality analysis helps identify code smells, which are indicators of potential design or implementation issues. Code smells include duplicated code, long methods, excessive dependencies, and improper use of language features. Detecting code smells helps in refactoring the code to improve its readability, maintainability, and extensibility. It also helps avoid anti-patterns, which are common but ineffective or harmful coding practices.</p> </li> <li> <p>Unused Code and Dead Code</p> <p>Code quality analysis identifies unused code or dead code, which refers to code segments that are not executed or have no impact on the program's behavior. Removing unused or dead code helps reduce code clutter, improve performance, and enhance maintainability.</p> </li> <li> <p>Dependencies and Coupling</p> <p>Code quality analysis examines the dependencies and coupling between code modules. High coupling and excessive dependencies can lead to code fragility and difficulties in making changes or enhancements. By analyzing the code for tight coupling and excessive dependencies, developers can identify areas where code modularization or decoupling is required.</p> </li> <li> <p>Testability</p> <p>Code quality analysis considers the testability of the code, ensuring that it can be effectively and efficiently tested. It involves evaluating factors such as code modularity, separation of concerns, and the presence of clear interfaces and unit testability. Improving testability helps in achieving better test coverage and ease of test maintenance.</p> </li> <li> <p>Documentation and Comments</p> <p>Code quality analysis assesses the presence and quality of code documentation and comments. Well-documented code improves understandability and maintainability, especially for future developers or maintainers. It ensures that the code's purpose, functionality, and usage are clearly explained.</p> </li> </ol>"},{"location":"articles/software-analysis/#12-software-composition-analysis","title":"1.2. Software Composition Analysis","text":"<p>Software Composition Analysis (SCA) is a software analysis technique that focuses on identifying, analyzing and managing the composition of third-party and open-source software components used in a software system.</p>"},{"location":"articles/software-analysis/#121-component-identification","title":"1.2.1. Component Identification","text":"<p>Component identification involves identifying and cataloging the third-party and open-source components used in a software system. This process helps in gaining an understanding of the software's composition, tracking dependencies, and assessing the potential risks associated with the components.</p> <p>Component identification captures relevant metadata about a component, such as version numbers, licenses, and any associated security vulnerabilities or patches. This information helps in subsequent steps of software composition analysis, such as vulnerability assessment and license compliance analysis.</p> <p>Features of Component Identification:</p> <ol> <li> <p>Build and Dependency Management Tools</p> <p>Use tools like Maven, Gradle, or npm (for JavaScript projects) to examine the dependency configuration files and retrieve a list of dependencies explicitly declared in the project.</p> </li> <li> <p>Package Managers</p> <p>If the software system uses a package manager, such as pip for Python, npm for JavaScript, or NuGet for .NET, inspect the package manifest files (e.g., requirements.txt, package.json, or .csproj) to obtain a list of installed dependencies.</p> </li> <li> <p>Software Bill of Materials (SBOM)</p> <p>In some cases, software vendors or providers may provide a Software Bill of Materials, which is a formal list of components used in the software system. This document can be obtained from the vendor or generated using specialized tools.</p> </li> <li> <p>Reverse Engineering</p> <p>If the source code or documentation is not readily available, reverse engineering techniques can be employed to identify components used in a software system. This involves decompiling binaries or inspecting runtime behavior to determine the components utilized.</p> </li> </ol>"},{"location":"articles/software-analysis/#122-vulnerability-analysis","title":"1.2.2. Vulnerability Analysis","text":"<p>Vulnerability analysis is a crucial step in software composition analysis that involves assessing the security vulnerabilities present in the third-party and open-source components used in a software system. The goal is to identify and understand potential weaknesses or flaws in the components that may be exploited by attackers.</p> <p>Regular vulnerability analysis should be performed throughout the software development lifecycle to ensure that any new vulnerabilities or security issues are promptly addressed. Conducting vulnerability analysis mitigate the risks associated with the components used in their software systems and enhance the security posture.</p> <p>Features of Vulnerability Analysis:</p> <ol> <li> <p>Vulnerability Databases</p> <p>Access well-known vulnerability databases, such as the National Vulnerability Database (NVD), Common Vulnerabilities and Exposures (CVE), or vendor-specific databases. These databases contain information about known vulnerabilities, including their severity, impact, and affected versions.</p> </li> <li> <p>Component Mapping</p> <p>Match the identified components from the component identification step with the entries in the vulnerability databases. Determine if the components have any known vulnerabilities listed in the databases. Pay attention to the specific component versions and any associated patches or fixes provided by the component vendors.</p> </li> <li> <p>Vulnerability Scanning Tools</p> <p>Utilize vulnerability scanning tools or automated analysis tools specifically designed for identifying vulnerabilities in software components. These tools can examine the software system and its components, comparing them against known vulnerability databases to identify any matches.</p> </li> <li> <p>Security Bulletins and Advisories</p> <p>Monitor security bulletins and advisories released by component vendors or security organizations. These sources provide information about newly discovered vulnerabilities, patches, and recommended security practices. Stay updated on any security announcements related to the components used in the software system.</p> </li> <li> <p>Risk Prioritization</p> <p>Evaluate the severity and potential impact of identified vulnerabilities. Prioritize the vulnerabilities based on factors such as the level of access required, the likelihood of exploitation, and the potential impact on the confidentiality, integrity, or availability of the software system.</p> </li> <li> <p>Remediation and Mitigation</p> <p>Develop a plan to address the identified vulnerabilities. This may involve updating the components to the latest patched versions, applying recommended security configurations, or implementing additional security controls to mitigate the risks associated with the vulnerabilities.</p> </li> <li> <p>Security Analysis</p> <p>Security analysis techniques are used to identify vulnerabilities and weaknesses in software systems that could be exploited by attackers. This category includes techniques such as penetration testing, vulnerability scanning, code review for security flaws, and threat modeling.</p> </li> </ol>"},{"location":"articles/software-analysis/#123-license-compliance-analysis","title":"1.2.3. License Compliance Analysis","text":"<p>License compliance analysis is an process in software analysis that focuses on ensuring compliance with the licenses of third-party and open-source components used in a software system. It involves evaluating the licenses associated with the components to ensure that their usage aligns with the terms and conditions specified by the license agreements.</p> <p>Conducting license compliance analysis ensures  legal obligations, maintain good relationships with the open-source community, and minimize the risks associated with license non-compliance.</p> <p>NOTE License compliance analysis is important to ensure legal and ethical usage of third-party and open-source components. Non-compliance with software licenses can result in legal issues, reputational damage, and complications in software distribution.</p> <p>Features of License Compliance Analysis:</p> <ol> <li> <p>License Identification</p> <p>Identify the licenses associated with each component used in the software system. This information can be obtained from the component's documentation, source code files, or the component's project website. Look for license files (e.g., LICENSE.txt) or license statements within the source code headers.</p> </li> <li> <p>License Classification</p> <p>Classify the licenses into different categories based on their terms and conditions. Common license categories include permissive licenses (e.g., MIT, BSD), copyleft licenses (e.g., GNU GPL), and reciprocal licenses (e.g., Apache License 2.0). Understand the obligations and restrictions imposed by each license category.</p> </li> <li> <p>License Compatibility Analysis</p> <p>Analyze the compatibility of different licenses used in the software system. Identify potential conflicts or incompatibilities that may arise due to the combination of different licenses. Some licenses have specific requirements when combined with other licenses, and it's important to ensure that the software system's licenses can be harmonized.</p> </li> <li> <p>License Obligations</p> <p>Review the specific obligations and requirements imposed by each license. This can include obligations to provide attribution, distribute source code modifications, or make certain disclosures. Ensure that the software system complies with these obligations and that the necessary actions are taken to fulfill them.</p> </li> <li> <p>License Restrictions</p> <p>Identify any restrictions or limitations imposed by the licenses. For example, some licenses may restrict the use of the software for commercial purposes, require the release of modifications under the same license, or impose restrictions on the distribution of the software. Ensure that the software system's usage aligns with these restrictions.</p> </li> <li> <p>License Compliance Tools</p> <p>Utilize automated tools or services designed to assist in license compliance analysis. These tools can scan the software codebase, identify the licenses used, and provide reports on license compliance status. They can help in identifying any potential non-compliance issues and simplifying the process of managing license obligations.</p> </li> <li> <p>Remediation and Mitigation</p> <p>Develop a plan to address any license compliance issues that are identified. This may involve obtaining necessary permissions or licenses, ensuring proper attribution and notices, or considering alternative components with more suitable licenses. Take appropriate actions to rectify any non-compliance and bring the software system into full license compliance.</p> </li> <li> <p>Ongoing Compliance Management</p> <p>Establish processes and practices to ensure ongoing license compliance. This includes maintaining an up-to-date inventory of component licenses, monitoring for changes or updates in license terms, and incorporating license compliance into software development and release workflows.</p> </li> </ol>"},{"location":"articles/software-analysis/#124-bill-of-materials-bom","title":"1.2.4. Bill of Materials (BOM)","text":"<p>A Bill of Materials (BOM) is a comprehensive list or inventory of all the materials, components, parts, and sub-assemblies required to manufacture or build a product. It provides detailed information about each item, including its name, quantity, description, and sometimes additional attributes such as part numbers or suppliers. BOMs are commonly used in various industries, including manufacturing, engineering, construction, and software development.</p> <p>BOMs enhances the ability of organizations to manage and reduce cyber risks throughout the software and hardware supply chain. It promotes transparency, collaboration, and effective risk mitigation in the context of software development, deployment, and operations.</p> <p>Types of BOM:</p> <ol> <li> <p>Software Bill of Materials (SBOM)</p> <p>An SBOM is a comprehensive inventory of the components and dependencies used in a software system. It includes information about the software components, their versions, licenses, and dependencies. SBOMs are crucial for managing security, licensing, and overall risk associated with software supply chain management.</p> </li> <li> <p>Software-as-a-Service Bill of Materials (SaaSBOM)</p> <p>An SaaSBOM is similar to an SBOM but specifically tailored for software-as-a-service (SaaS) offerings. It provides a detailed inventory of the components, services, and dependencies used in a SaaS solution. SaaSBOMs help customers understand the underlying components and potential risks associated with the SaaS service they are using.</p> </li> <li> <p>Hardware Bill of Materials (HBOM)</p> <p>An HBOM is a list of all the components, parts, and materials required to manufacture a hardware product. It includes information about the electronic components, mechanical parts, packaging materials, and other hardware elements needed for production. HBOMs are commonly used in the manufacturing and electronics industries.</p> </li> <li> <p>Operations Bill of Materials (OBOM)</p> <p>An OBOM is used in the context of operations management and supply chain management. It includes a list of materials, tools, equipment, and resources required to perform specific operational tasks or processes. OBOMs help in planning and managing the resources needed for efficient operations.</p> </li> </ol> <p>Standards of BOM:</p> <ol> <li> <p>OWASP CycloneDX</p> <p>OWASP CycloneDX emphasizes supply chain security and risk reduction. It provides advanced capabilities for creating BOMs that cover various aspects of the software stack, including software, SaaS, hardware, and operations.</p> <p>CycloneDX is designed to be extensible, allowing for the inclusion of additional metadata and information beyond the core BOM data. This flexibility enables organizations to customize and enhance the BOMs to meet their specific requirements.</p> <p>CycloneDX supports the generation of Vulnerability Disclosure Reports (VDR) and the exchange of Vulnerability Exploitability eXchange (VEX) information. These features enhance vulnerability management and facilitate timely remediation of security issues.</p> </li> <li> <p>SPDX</p> <p>SPDX focuses on providing a standardized format for describing software components and their licenses. It aims to facilitate license compliance and software asset management by providing a common language for sharing and exchanging software package data.</p> <p>SPDX places a strong emphasis on capturing license information for each software component. It includes standardized license identifiers, license text, and other related licensing details. SPDX also supports the declaration of license conflicts and exceptions.</p> <p>SPDX is widely adopted and supported by various tools and systems in the software development ecosystem. It enables easy integration with build systems, package managers, and software repositories, facilitating the automation of BOM generation and license compliance checks.</p> </li> </ol> <p>Benefits of BOM:</p> <ol> <li> <p>License Compliance</p> <p>The BOM helps in identifying the licenses associated with each component and ensuring compliance with their terms and conditions. It allows organizations to understand the licensing requirements, obligations, and restrictions of the software components they use.</p> </li> <li> <p>Vulnerability Management</p> <p>By maintaining an up-to-date BOM, organizations can track the versions of components and be aware of any known security vulnerabilities associated with them. This enables proactive vulnerability management and helps in mitigating risks.</p> </li> <li> <p>Change Management</p> <p>The BOM provides visibility into the dependencies between components, allowing organizations to assess the impact of changes and plan accordingly. It helps in managing version upgrades, patches, or replacements of components.</p> </li> <li> <p>Supply Chain Management</p> <p>BOMs are useful for managing relationships with component suppliers or vendors. They provide a centralized view of the components used and their associated suppliers, facilitating communication, support, and maintenance activities.</p> </li> </ol>"},{"location":"articles/software-analysis/#13-tools","title":"1.3. Tools","text":""},{"location":"articles/software-analysis/#131-linter","title":"1.3.1. Linter","text":"<p>A linter is a software tool that analyzes source code to identify and flag potential programming errors, coding standards violations, and other types of quality issues.</p> <p>Commonly used linter tools are:</p> <ul> <li> <p>ESLint</p> <p>A popular JavaScript linter that supports various coding styles and helps enforce consistent coding practices.</p> </li> <li> <p>Pylint</p> <p>A linter for Python code, designed to identify common programming errors and improve code quality.</p> </li> <li> <p>RuboCop</p> <p>A linter for Ruby code that checks for coding style violations, security issues, and performance bottlenecks.</p> </li> <li> <p>Clang-Tidy</p> <p>A linter for C++ code that uses Clang and LLVM libraries to identify potential coding issues and improve code quality.</p> </li> <li> <p>JSHint</p> <p>A JavaScript linter that can be used in a variety of development environments, including web browsers and Node.js.</p> </li> <li> <p>golangci-lint</p> <p>A linter tool for Go code, used to detect and fix coding errors, enforce coding conventions, and improve code quality in Go projects.</p> </li> <li> <p>Flake8</p> <p>A Python linter that combines three existing linters into a single tool (PyFlakes, McCabe, and PEP 8).</p> </li> <li> <p>CSSLint</p> <p>A linter for CSS code that checks for coding style violations, security issues, and performance bottlenecks.</p> </li> </ul>"},{"location":"articles/software-analysis/#132-sanitizer","title":"1.3.2. Sanitizer","text":"<p>A sanitizer is a software tool that is used to identify and remove or correct malicious or potentially dangerous content from data inputs, such as user-generated input in a web application.</p> <p>Commonly used sanitizer tools are:</p> <ul> <li> <p>HTMLPurifier</p> <p>A PHP library that removes malicious code from HTML input, including XSS attacks and other types of malicious content.</p> </li> <li> <p>OWASP Java Encoder</p> <p>A Java library that provides a comprehensive set of functions for encoding and escaping user input to prevent XSS attacks.</p> </li> <li> <p>Input Sanitizer</p> <p>A JavaScript library that provides a simple and flexible API for sanitizing user input to prevent XSS attacks.</p> </li> <li> <p>Anti-Samy</p> <p>An XML and XHTML sanitizer that removes malicious code from user input and ensures that the output is secure and well-formed.</p> </li> <li> <p>XSS-Sanitizer</p> <p>A Python library that provides a simple API for sanitizing user input to prevent XSS attacks.</p> </li> <li> <p>Ruby Sanitizer</p> <p>A Ruby library that provides a simple API for sanitizing user input, including functions for removing malicious code, encoding special characters, and removing dangerous attributes.</p> </li> <li> <p>sqlmap</p> <p>An open-source tool that automates the process of identifying and exploiting SQL injection vulnerabilities in web applications.</p> </li> </ul> <p>Sanitizers are tools that help identify  detect bugs such as buffer overflows or accesses, dangling pointer or different types of undefined behavior at runtime and prevent security vulnerabilities in C/C++ code.</p> <p>Commonly used sanitizer tools for C/C++ are:</p> <ul> <li> <p>Address Sanitizer (ASan)</p> <p>A tool that detects memory access errors, such as out-of-bounds memory accesses and use-after-free bugs, in C/C++ code.</p> </li> <li> <p>Undefined Behavior Sanitizer (UBSan)</p> <p>A tool that detects undefined behavior, such as signed integer overflow and division by zero, in C/C++ code.</p> </li> <li> <p>Thread Sanitizer (TSan)</p> <p>A tool that detects data races and other synchronization bugs in multithreaded C/C++ code.</p> </li> <li> <p>Memory Sanitizer (MSan)</p> <p>A tool that detects uninitialized memory reads in C/C++ code.</p> </li> <li> <p>Control Flow Integrity (CFI)</p> <p>A security mechanism that helps prevent code injection attacks by enforcing constraints on the flow of control in C/C++ code.</p> </li> <li> <p>Valgrind</p> <p>A dynamic analysis tool that can be used to detect memory leaks, buffer overflows, and other types of programming errors in C/C++ code.</p> </li> </ul>"},{"location":"articles/software-analysis/#2-references","title":"2. References","text":"<ul> <li>Sentenz Application Security Testing (AST) article.</li> </ul>"},{"location":"articles/software-architecture/","title":"Software Architecture","text":"<p>Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations.</p> <ul> <li>1. Clean Architecture</li> <li>2. Onion Architecture</li> <li>3. Hexagonal Architecture</li> <li>4. Layered / N-tier Architecture</li> <li>5. References</li> </ul>"},{"location":"articles/software-architecture/#1-clean-architecture","title":"1. Clean Architecture","text":"<p>Clean Architecture was introduced by Robert Martin (aka Uncle Bob) in 2012. Clean architecture attempts to provide a cost-effective method for developing quality code that is changeable and has fewer dependencies.</p> <p>Clean architecture is a software design philosophy that separates the elements of a design into ring levels. An important goal of clean architecture is to provide developers with a way to organize code in such a way that it encapsulates the business logic but keeps it separate from the delivery mechanism.</p> <p>The main rule of clean architecture is that code dependencies can only move from the outer levels inward. Code on the inner layers can have no knowledge of functions on the outer layers. The variables, functions and classes (any entities) that exist in the outer layers can not be mentioned in the more inward levels. It is recommended that data formats also stay separate between levels.</p>"},{"location":"articles/software-architecture/#2-onion-architecture","title":"2. Onion Architecture","text":"<p>Onion Architecture was introduced by Jeffrey Palermo in 2008. It expanded on the idea to define a \u201cCore\u201d within the application and various layers surrounding it. The core \u201cDomain Model\u201d represents enterprise-wide business rules. In the next layer up are \u201cDomain Services\u201d such as abstract repositories (still leaving the implementation details such as a database connection to the outer layers). Further up is \u201cApplication Services\u201d which defined the business processes of the application. In the outer-most layer are the user interface, connections to external infrastructure, and automated tests. Like ports-and-adapters, this pattern leaves the connections to all external dependencies such as databases, APIs, and user interfaces at the edge so they can be switched out.</p>"},{"location":"articles/software-architecture/#3-hexagonal-architecture","title":"3. Hexagonal Architecture","text":"<p>Hexagonal Architecture was introduced by Alistair Cockburn in 2005. The most bare bones application of the principal is Hexagonal or Ports and Adapters architecture. Confusingly, the pattern really has nothing to do with hexagons, it\u2019s how it\u2019s usually drawn. The fact that it has 6 sides is arbitrary.</p> <p>The concept behind Hexagonal Architecture is that the core application logic is written with only a concept of external dependencies it has. In object-oriented terms this means it declares and references and interface, but leaves the implementation of that interface out of the core logic. This can be thought of as a <code>port</code> such a display port or USB port. The outer layer of the application then creates an <code>adapter</code> which plugs into the port, so if there was a database port, the adapter would plug into that port and provide a connection to a particular database. If want to change the database, write a new adapter. User interfaces would also be ports which could be filled with various adapters without modifying the core logic.</p>"},{"location":"articles/software-architecture/#4-layered-n-tier-architecture","title":"4. Layered / N-tier Architecture","text":"<p>An n-tier architecture divides an application into logical layers and physical tiers.</p> <p>Layers are a way to separate responsibilities and manage dependencies. Each layer has a specific responsibility. A higher layer can use services in a lower layer, but not the other way around.</p> <ul> <li>Presentation layer (aka UI layer)</li> <li>Application layer (aka service layer)</li> <li>Business logic layer (aka domain layer)</li> <li>Data access layer (aka persistence layer)</li> </ul> <p>Tiers are physically separated, running on separate machines. A tier can call to another tier directly, or use asynchronous messaging (message queue). Although each layer might be hosted in its own tier, that's not required. Several layers might be hosted on the same tier. Physically separating the tiers improves scalability and resiliency, but also adds latency from the additional network communication.</p> <p>A traditional three-tier application has a presentation tier, a middle tier, and a database tier. The middle tier is optional. More complex applications can have more than three tiers. The diagram above shows an application with two middle tiers, encapsulating different areas of functionality.</p> <p>An N-tier application can have a closed layer architecture or an open layer architecture:</p> <ul> <li>In a closed layer architecture, a layer can only call the next layer immediately down.</li> <li>In an open layer architecture, a layer can call any of the layers below it.</li> </ul> <p>A closed layer architecture limits the dependencies between layers. However, it might create unnecessary network traffic, if one layer passes requests along to the next layer.</p> <p>N-tier architectures are typically implemented as infrastructure-as-service (IaaS) applications, with each tier running on a separate set of VMs. However, an N-tier application doesn't need to be pure IaaS. Often, it's advantageous to use managed services for some parts of the architecture, particularly caching, messaging, and data storage.</p> <p>Consider an N-tier architecture for:</p> <ul> <li>Basic web applications.</li> <li>Migrating an on-premises application to Azure with minimal refactoring.</li> <li>Unified development of on-premises and cloud applications.</li> </ul> <p>N-tier architectures are common in traditional on-premises applications.</p>"},{"location":"articles/software-architecture/#5-references","title":"5. References","text":"<ul> <li>Medium Onion vs Clean vs Hexagonal Architecture article.</li> </ul>"},{"location":"articles/software-design-principles/","title":"Software Design Principles","text":"<p>Software design principles are fundamental concepts and guidelines that help developers create well-designed, maintainable, and scalable software systems. These principles serve as a foundation for making informed design decisions and improving the quality of software.</p> <ul> <li>1. Category</li> <li>1.1. Design Principles<ul> <li>1.1.1. SOLID</li> <li>1.1.1.1. SRP</li> <li>1.1.1.2. OCP</li> <li>1.1.1.3. LSP</li> <li>1.1.1.4. ISP</li> <li>1.1.1.5. DIP</li> <li>1.1.2. GRASP</li> <li>1.1.2.1. Creator</li> <li>1.1.2.2. Controller</li> <li>1.1.2.3. Information Expert</li> <li>1.1.2.4. High Cohesion</li> <li>1.1.2.5. Low Coupling</li> <li>1.1.2.6. Polymorphism</li> <li>1.1.2.7. Indirection</li> <li>1.1.2.8. Pure Fabrication</li> <li>1.1.2.9. Protected Variations</li> <li>1.1.3. Abstraction</li> <li>1.1.4. Separation of Concerns</li> <li>1.1.5. Composition over Inheritance</li> <li>1.1.6. Separation of Interface and Implementation</li> <li>1.1.7. Convention over Configuration</li> <li>1.1.8. Coupling</li> <li>1.1.9. Cohesion</li> <li>1.1.10. Modularity</li> <li>1.1.11. Encapsulation</li> <li>1.1.12. Principle of Least Astonishment</li> <li>1.1.13. Principle of Least Privilege</li> <li>1.1.14. Inversion of Control</li> <li>1.1.15. Law of Demeter</li> <li>1.1.16. Law of Conservation of Complexity</li> <li>1.1.17. Law of Simplicity</li> <li>1.1.18. Law of Readability</li> <li>1.1.19. Law of Clarity</li> </ul> </li> <li>1.2. Coding Principles<ul> <li>1.2.1. KISS</li> <li>1.2.2. DRY</li> <li>1.2.3. YAGNI</li> <li>1.2.4. Defensive Programming</li> <li>1.2.5. Single Point of Responsibility</li> <li>1.2.6. Design by Contract</li> <li>1.2.7. Command-Query Separation</li> </ul> </li> <li>1.3. Process Principles<ul> <li>1.3.1. Waterfall Model</li> <li>1.3.2. V Model</li> <li>1.3.3. Agile</li> <li>1.3.4. Lean Software Development</li> <li>1.3.5. Kanban</li> <li>1.3.6. Scrum</li> <li>1.3.7. Extreme Programming</li> <li>1.3.8. Feature-Driven Development</li> </ul> </li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/software-design-principles/#1-category","title":"1. Category","text":"<p>Software design principles can be broadly categorized into three main categories. By following these principles, software developers can create high-quality software applications that are easy to maintain, scalable, and efficient.</p> <p>NOTE While these principles provide guidelines for software development, they are not strict rules that must be followed in every situation. The key is to understand the principles and apply them appropriately to the specific context of the software project.</p>"},{"location":"articles/software-design-principles/#11-design-principles","title":"1.1. Design Principles","text":"<p>Design principles are a set of guidelines that deal with the design of a software application, including its architecture, structure, and organization. By following these design principles, software developers can create software applications that are modular, scalable, and easy to maintain. These principles help to reduce complexity and make the code more flexible, reusable, and efficient.</p>"},{"location":"articles/software-design-principles/#111-solid","title":"1.1.1. SOLID","text":"<p>SOLID is an acronym for a set of five design principles as guidelines for writing clean, maintainable, and scalable object-oriented code. These principles promote modular design, flexibility, and ease of understanding and modification.</p>"},{"location":"articles/software-design-principles/#1111-srp","title":"1.1.1.1. SRP","text":"<p>The Single Responsibility Principle (SRP) is a design principle in object-oriented programming that states that a class should have only one responsibility or reason to change. In other words, a class should have only one job to do.</p> <p>The idea behind SRP is that when a class has only one responsibility, it becomes easier to maintain, test, and modify. When a class has multiple responsibilities, it becomes more difficult to make changes without affecting other parts of the system. This can lead to code that is tightly coupled, hard to test, and difficult to understand.</p> <p>NOTE Applying SRP involves analyzing the responsibilities of a class or module and ensuring that it has only one reason to change. If a class has multiple responsibilities, it can be refactored into separate classes, each with its own single responsibility.</p> <p>Adherence of SRP:</p> <ol> <li> <p>Separation of Concerns</p> <p>This principle emphasizes keeping different concerns or responsibilities separate from each other. Each class or module should be responsible for one specific concern, and the responsibilities should not overlap or be tightly coupled. By separating concerns, you can ensure that each class has a single responsibility, making the code more maintainable and reducing the risk of unintended side effects.</p> </li> <li> <p>High Cohesion</p> <p>High cohesion refers to the idea that the responsibilities within a class or module should be closely related and form a logical unit. Classes with high cohesion have a clear and focused purpose, with methods and properties that work together to fulfill that purpose. By designing classes with high cohesion, you can ensure that each class has a single, well-defined responsibility.</p> </li> <li> <p>Encapsulation</p> <p>Encapsulation is the practice of hiding internal details and providing controlled access to the functionality of a class or module. By encapsulating data and behavior within a class, you can ensure that each class is responsible for its own state and operations. This helps in maintaining a clear separation of responsibilities and avoids the scattering of code related to a single responsibility throughout the system.</p> </li> <li> <p>Single Level of Abstraction</p> <p>This principle suggests that each method or function should have a single level of abstraction, meaning it should operate at a specific level of detail or granularity. Methods should not mix low-level implementation details with high-level business logic. By maintaining a single level of abstraction, you can achieve better readability, maintainability, and understandability of the codebase.</p> </li> <li> <p>Dependency Injection</p> <p>Dependency Injection helps manage dependencies between classes by inverting the responsibility of creating and providing dependencies. By relying on external sources to provide necessary dependencies, you ensure that classes have a single responsibility without being burdened with creating or managing dependencies themselves.</p> </li> </ol> <p>Examples of SRP:</p> <ol> <li> <p>Separation of Concerns</p> <p>Bad Example:</p> <pre><code>public class Customer\n{\n    public void AddCustomer()\n    {\n        // Code to add a new customer to the database\n        // Code to send a confirmation email\n        // Code to log the customer creation\n        // ...\n    }\n}\n</code></pre> <p>In the bad example, the <code>Customer</code> class is responsible for multiple concerns, including adding a customer, sending emails, and logging. This violates the principle of separation of concerns.</p> <p>Good Example:</p> <pre><code>public class Customer\n{\n    public void AddCustomer()\n    {\n        // Code to add a new customer to the database\n    }\n}\n\npublic class EmailService\n{\n    public void SendConfirmationEmail(Customer customer)\n    {\n        // Code to send a confirmation email\n    }\n}\n\npublic class Logger\n{\n    public void Log(string message)\n    {\n        // Code to log a message\n    }\n}\n</code></pre> <p>In the good example, the concerns are separated into different classes. The <code>Customer</code> class now focuses solely on adding a customer, while the <code>EmailService</code> handles sending emails, and the <code>Logger</code> class takes care of logging. Each class has a single responsibility, promoting maintainability and modularity.</p> </li> <li> <p>High Cohesion:</p> <p>Bad Example:</p> <pre><code>public class Product\n{\n    public void UpdateProduct()\n    {\n        // Code to update product information in the database\n        // Code to calculate product statistics\n        // Code to send notifications to customers\n        // ...\n    }\n}\n</code></pre> <p>In the bad example, the <code>Product</code> class has multiple responsibilities, including updating product information, calculating statistics, and sending notifications. This leads to low cohesion.</p> <p>Good Example:</p> <pre><code>public class Product\n{\n    public void UpdateProduct()\n    {\n        // Code to update product information in the database\n    }\n}\n\npublic class ProductStatisticsCalculator\n{\n    public void CalculateStatistics(Product product)\n    {\n        // Code to calculate product statistics\n    }\n}\n\npublic class NotificationService\n{\n    public void SendNotifications(Product product)\n    {\n        // Code to send notifications to customers\n    }\n}\n</code></pre> <p>In the good example, the responsibilities are separated into different classes. The <code>Product</code> class focuses solely on updating product information, while the <code>ProductStatisticsCalculator</code> calculates statistics, and the <code>NotificationService</code> handles sending notifications. Each class has a clear and distinct purpose, improving maintainability and understandability.</p> </li> <li> <p>Encapsulation</p> <p>Bad Example:</p> <pre><code>public class Employee\n{\n    public string Name;\n    public int Age;\n    public decimal Salary;\n\n    public void SaveEmployee()\n    {\n        // Code to save employee details to the database\n    }\n}\n</code></pre> <p>In the bad example, the fields of the <code>Employee</code> class are publicly accessible, violating encapsulation.</p> <p>Good Example:</p> <pre><code>public class Employee\n{\n    private string name;\n    private int age;\n    private decimal salary;\n\n    public Employee(string name, int age, decimal salary)\n    {\n        this.name = name;\n        this.age = age;\n        this.salary = salary;\n    }\n\n    public void SaveEmployee()\n    {\n        // Code to save employee details to the database\n    }\n}\n</code></pre> <p>In the good example, the fields are declared as private, and access to them is provided through properties or methods. This ensures that the internal state of the <code>Employee</code> class is encapsulated and can only be modified through controlled means, promoting data integrity and maintainability.</p> </li> <li> <p>Single Level of Abstraction</p> <p>Bad Example:</p> <pre><code>public class MathOperations\n{\n    public double CalculateCircleArea(double radius)\n    {\n        double pi = 3.14159;\n        double area = pi * radius * radius;\n        Console.WriteLine(\"The area of the circle is: \" + area);\n\n        return area;\n    }\n}\n</code></pre> <p>In the bad example, the <code>CalculateCircleArea</code> method combines the calculation of the circle's area with the printing of the result. This violates the principle of single level of abstraction, as the method is responsible for both calculation and printing.</p> <p>Good Example:</p> <pre><code>public class MathOperations\n{\n    public double CalculateCircleArea(double radius)\n    {\n        return Math.PI * radius * radius;\n    }\n\n    public void PrintCircleArea(double area)\n    {\n        Console.WriteLine(\"The area of the circle is: \" + area);\n    }\n}\n</code></pre> <p>In the good example, the calculation and printing are separated into two methods: <code>CalculateCircleArea</code> solely calculates the area, and <code>PrintCircleArea</code> handles the printing. Each method now focuses on a single level of abstraction, making the code more maintainable and readable.</p> </li> <li> <p>Dependency Injection</p> <p>Bad Example:</p> <pre><code>public class Order\n{\n    private DatabaseService dbService;\n\n    public Order()\n    {\n        dbService = new DatabaseService();\n    }\n\n    public void PlaceOrder()\n    {\n        // Code to place the order\n        dbService.SaveOrderToDatabase();\n    }\n}\n</code></pre> <p>In the bad example, the <code>Order</code> class has a direct dependency on the <code>DatabaseService</code> class, making it tightly coupled. This makes it difficult to replace or mock the database service for testing purposes.</p> <p>Good Example:</p> <pre><code>public class Order\n{\n    private IDatabaseService dbService;\n\n    public Order(IDatabaseService dbService)\n    {\n        this.dbService = dbService;\n    }\n\n    public void PlaceOrder()\n    {\n        // Code to place the order\n        dbService.SaveOrderToDatabase();\n    }\n}\n\npublic interface IDatabaseService\n{\n    void SaveOrderToDatabase();\n}\n</code></pre> <p>In the good example, the dependency on the database service is inverted using dependency injection. The <code>Order</code> class depends on the <code>IDatabaseService</code> interface, and the actual implementation is passed in through the constructor. This promotes loose coupling and allows for easier testing and substitution of dependencies.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1112-ocp","title":"1.1.1.2. OCP","text":"<p>The Open-Closed Principle (OCP) is a design principle that states that software entities (classes, modules, functions, etc.) should be open for extension but closed for modification. The behavior of a software entity should be easily extended without modifying its existing code.</p> <p>In other words, the principle encourages designing code in a way that allows adding new features or behaviors without modifying existing code. Instead of directly modifying existing code, the OCP suggests extending it through the addition of new code. This promotes code reuse, reduces the risk of introducing bugs, and makes the software more flexible and adaptable to changes.</p> <p>Adherence of OCP:</p> <ol> <li> <p>Abstraction</p> <p>The use of abstraction is crucial in following the OCP. Defining abstract classes, interfaces or protocols, can create a clear separation between the public interface and the implementation details of a class. This allows for extensions to be added through new implementations of the abstraction without modifying existing code.</p> </li> <li> <p>Inheritance and Polymorphism</p> <p>Inheritance and polymorphism play a significant role in achieving the OCP.  Designing classes to inherit from abstract classes or implement interfaces, establish a common contract that allows for the substitution of objects at runtime. This allows new functionality to be added by creating new subclasses or implementations without modifying the existing code that relies on the abstraction.</p> </li> <li> <p>Dependency Injection</p> <p>Depend on abstractions or interfaces rather than concrete implementations. This allows new implementations to be injected or swapped without modifying existing code. Dependency Injection frameworks, such as Spring or .NET Core DI, help facilitate this technique.</p> </li> <li> <p>Decorator Pattern</p> <p>The decorator pattern allows for extending the functionality of an object by wrapping it with one or more decorator objects. Decorators add new behavior while keeping the original object's interface unchanged.</p> </li> <li> <p>Strategy Pattern</p> <p>The Strategy pattern is commonly used to adhere to the OCP. It involves encapsulating different algorithms or behaviors into separate classes, each implementing a common interface. The context class then uses the selected strategy through composition and delegation. Adding new strategies does not require modifying the existing code, as the context is open for extension but closed for modification.</p> </li> </ol> <p>Examples of OCP in C#:</p> <ol> <li> <p>Abstraction</p> <pre><code>public abstract class Shape\n{\n    public abstract double CalculateArea();\n}\n\npublic class Circle : Shape\n{\n    private double radius;\n\n    public Circle(double radius)\n    {\n        this.radius = radius;\n    }\n\n    public override double CalculateArea()\n    {\n        return Math.PI * Math.Pow(radius, 2);\n    }\n}\n\npublic class Rectangle : Shape\n{\n    private double width;\n    private double height;\n\n    public Rectangle(double width, double height)\n    {\n        this.width = width;\n        this.height = height;\n    }\n\n    public override double CalculateArea()\n    {\n        return width * height;\n    }\n}\n</code></pre> <p>In the example, the <code>Shape</code> class is an abstract class that defines a contract for calculating the area of different shapes. The <code>Circle</code> and <code>Rectangle</code> classes inherit from <code>Shape</code> and provide their own implementations of the <code>CalculateArea()</code> method. By using abstraction, the code adheres to the Open/Closed Principle as new shapes can be added by creating new subclasses without modifying existing code.</p> </li> <li> <p>Inheritance and Polymorphism</p> <pre><code>public abstract class Animal\n{\n    public abstract void MakeSound();\n}\n\npublic class Dog : Animal\n{\n    public override void MakeSound()\n    {\n        Console.WriteLine(\"Woof!\");\n    }\n}\n\npublic class Cat : Animal\n{\n    public override void MakeSound()\n    {\n        Console.WriteLine(\"Meow!\");\n    }\n}\n\n// Usage example\nAnimal dog = new Dog();\ndog.MakeSound(); // Output: Woof!\n\nAnimal cat = new Cat();\ncat.MakeSound(); // Output: Meow!\n</code></pre> <p>The <code>Animal</code> class is an abstract class that defines a common interface for different animal types. The <code>Dog</code> and <code>Cat</code> classes inherit from <code>Animal</code> and override the <code>MakeSound()</code> method to provide their specific implementation. Through inheritance and polymorphism, the code adheres to the Open/Closed Principle as new animal types can be added by creating new subclasses without modifying the existing code that relies on the abstraction.</p> </li> <li> <p>Dependency Injection</p> <pre><code>// Abstraction\npublic interface ILogger\n{\n    void Log(string message);\n}\n\n// Concrete implementation\npublic class ConsoleLogger : ILogger\n{\n    public void Log(string message)\n    {\n        Console.WriteLine(message);\n    }\n}\n\n// Client class using Dependency Injection\npublic class UserService\n{\n    private readonly ILogger logger;\n\n    public UserService(ILogger logger)\n    {\n        this.logger = logger;\n    }\n\n    public void CreateUser(string username)\n    {\n        // Logic to create a user\n        logger.Log($\"User '{username}' created successfully.\");\n    }\n}\n\n// Usage\nvar logger = new ConsoleLogger();\nvar userService = new UserService(logger);\nuserService.CreateUser(\"John\");\n</code></pre> <p>In the example, the <code>UserService</code> class depends on the <code>ILogger</code> interface through constructor injection. By relying on the abstraction, we can easily swap the implementation of the logger without modifying the <code>UserService</code> class.</p> </li> <li> <p>Decorator Pattern</p> <pre><code>// Component interface\npublic interface IShape\n{\n    void Draw();\n}\n\n// Concrete component\npublic class Circle : IShape\n{\n    public void Draw()\n    {\n        Console.WriteLine(\"Drawing a circle.\");\n    }\n}\n\n// Decorator class\npublic class ShapeDecorator : IShape\n{\n    private readonly IShape decoratedShape;\n\n    public ShapeDecorator(IShape shape)\n    {\n        decoratedShape = shape;\n    }\n\n    public void Draw()\n    {\n        decoratedShape.Draw();\n        Console.WriteLine(\"Decorating the shape.\");\n    }\n}\n\n// Usage\nvar circle = new Circle();\nvar decoratedCircle = new ShapeDecorator(circle);\ndecoratedCircle.Draw();\n</code></pre> <p>In the example, the <code>ShapeDecorator</code> class wraps the original <code>Circle</code> object and adds additional behavior before or after invoking the <code>Draw</code> method. It enhances the functionality of the original shape without modifying its implementation.</p> </li> <li> <p>Strategy Pattern</p> <pre><code>public interface IShippingStrategy\n{\n    decimal CalculateShippingCost(decimal orderAmount);\n}\n\npublic class FedExShippingStrategy : IShippingStrategy\n{\n    public decimal CalculateShippingCost(decimal orderAmount)\n    {\n        // Calculation logic specific to FedEx shipping\n        return orderAmount * 0.1m;\n    }\n}\n\npublic class UPSShippingStrategy : IShippingStrategy\n{\n    public decimal CalculateShippingCost(decimal orderAmount)\n    {\n        // Calculation logic specific to UPS shipping\n        return orderAmount * 0.2m;\n    }\n}\n\npublic class ShippingCalculator\n{\n    private IShippingStrategy shippingStrategy;\n\n    public ShippingCalculator(IShippingStrategy shippingStrategy)\n    {\n        this.shippingStrategy = shippingStrategy;\n    }\n\n    public decimal CalculateShipping(decimal orderAmount)\n    {\n        return shippingStrategy.CalculateShippingCost(orderAmount);\n    }\n}\n\n// Usage example\nShippingCalculator fedExShipping = new ShippingCalculator(new FedExShippingStrategy());\ndecimal fedExCost = fedExShipping.CalculateShipping(100); // Calculate shipping cost using FedEx strategy\n\nShippingCalculator upsShipping = new ShippingCalculator(new UPSShippingStrategy());\ndecimal upsCost = upsShipping.CalculateShipping(100); // Calculate shipping cost using UPS strategy\n</code></pre> <p>The Strategy pattern is used to encapsulate different shipping strategies (IShippingStrategy) and allow the client code (ShippingCalculator) to select and use the desired strategy. The <code>ShippingCalculator</code> class accepts an <code>IShippingStrategy</code> instance through dependency injection, and its <code>CalculateShipping()</code> method uses the injected strategy to calculate the shipping cost. By employing the Strategy pattern, the code adheres to the Open/Closed Principle as new shipping strategies can be added by implementing the <code>IShippingStrategy</code> interface without modifying the existing code.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1113-lsp","title":"1.1.1.3. LSP","text":"<p>The Liskov Substitution Principle (LSP) is a fundamental principle in object-oriented programming, named after Barbara Liskov. It states that objects of a superclass should be replaceable with objects of its subclasses without affecting the correctness of the program. In other words, any instance of a derived class should be able to be used in place of an instance of its base class without causing unexpected behavior.</p> <p>The LSP is based on the concept of behavioral subtyping, which means that a subclass should conform to the contract defined by its superclass. To adhere to LSP, it's important to ensure that subclasses honor the contract defined by their base class. They should not weaken or modify the preconditions, postconditions, or invariants defined by the base class.</p> <p>LSP ensures that objects can be substituted or interchanged with their subtypes without affecting the behavior of the program. This allows for flexibility in designing class hierarchies and promotes code reuse.</p> <p>Adherence of LSP:</p> <ol> <li> <p>Inheritance and Polymorphism</p> <p>Inheritance and polymorphism play a significant role in adhering to the LSP. Derived classes should be substitutable for their base classes, which means they should follow the same contract and exhibit behavior that is compatible with the base class. Polymorphism allows objects to be treated uniformly through their common interface, enabling substitution without breaking the code.</p> </li> <li> <p>Modularity and Extensibility</p> <p>LSP promotes modularity by allowing new derived classes to be added to the system without impacting existing code. It enables developers to extend the behavior of the system through inheritance without breaking the existing functionality.</p> </li> <li> <p>Design by Contract</p> <p>Design by Contract is a software design approach that involves specifying preconditions, postconditions, and invariants for methods or functions. It helps enforce the expected behavior of classes and their derived classes. By adhering to design contracts, derived classes can be substituted for their base classes without violating the contracts, ensuring compliance with the LSP.</p> </li> </ol> <p>Examples of LSP:</p> <ol> <li> <p>Inheritance and Polymorphism</p> <pre><code>// Base class\npublic abstract class Animal\n{\n    public abstract void MakeSound();\n}\n\n// Derived classes\npublic class Dog : Animal\n{\n    public override void MakeSound()\n    {\n        Console.WriteLine(\"Woof!\");\n    }\n}\n\npublic class Cat : Animal\n{\n    public override void MakeSound()\n    {\n        Console.WriteLine(\"Meow!\");\n    }\n}\n\n// Usage\nList&lt;Animal&gt; animals = new List&lt;Animal&gt;();\nanimals.Add(new Dog());\nanimals.Add(new Cat());\n\nforeach (Animal animal in animals)\n{\n    animal.MakeSound();\n}\n</code></pre> <p>In the example, the <code>Animal</code> class is an abstract base class, and <code>Dog</code> and <code>Cat</code> are derived classes. The <code>MakeSound()</code> method is declared as abstract in the base class and overridden in the derived classes. By using a collection of <code>Animal</code> objects, we can iterate through them and invoke the <code>MakeSound()</code> method on each object, regardless of its specific derived class. This demonstrates the polymorphic behavior enabled by LSP.</p> </li> <li> <p>Modularity and Extensibility</p> <pre><code>// Base class\npublic abstract class Vehicle\n{\n    public abstract void StartEngine();\n    public abstract void StopEngine();\n}\n\n// Derived classes\npublic class Car : Vehicle\n{\n    public override void StartEngine()\n    {\n        Console.WriteLine(\"Car engine started.\");\n    }\n\n    public override void StopEngine()\n    {\n        Console.WriteLine(\"Car engine stopped.\");\n    }\n}\n\npublic class Motorcycle : Vehicle\n{\n    public override void StartEngine()\n    {\n        Console.WriteLine(\"Motorcycle engine started.\");\n    }\n\n    public override void StopEngine()\n    {\n        Console.WriteLine(\"Motorcycle engine stopped.\");\n    }\n}\n\n// Usage\nvoid DriveVehicle(Vehicle vehicle)\n{\n    vehicle.StartEngine();\n    // Perform driving operations\n    vehicle.StopEngine();\n}\n\nVehicle car = new Car();\nDriveVehicle(car);\n\nVehicle motorcycle = new Motorcycle();\nDriveVehicle(motorcycle);\n</code></pre> <p>In the example, the <code>Vehicle</code> class is an abstract base class, and <code>Car</code> and <code>Motorcycle</code> are derived classes representing different types of vehicles. The <code>DriveVehicle()</code> method accepts a <code>Vehicle</code> object and performs common operations, regardless of the specific vehicle type. This demonstrates modularity and extensibility, as new types of vehicles can be easily added without modifying the existing code.</p> </li> <li> <p>Design by Contract</p> <pre><code>using System;\nusing System.Diagnostics.Contracts;\n\npublic class BankAccount\n{\n    private decimal balance;\n\n    public BankAccount(decimal initialBalance)\n    {\n        Contract.Requires(initialBalance &gt;= 0, \"Initial balance must be non-negative.\");\n        this.balance = initialBalance;\n    }\n\n    public decimal Balance\n    {\n        get { return balance; }\n    }\n\n    public void Deposit(decimal amount)\n    {\n        Contract.Requires(amount &gt; 0, \"Deposit amount must be positive.\");\n        Contract.Ensures(Balance == Contract.OldValue(Balance) + amount, \"Balance should increase by the deposit amount.\");\n\n        balance += amount;\n    }\n\n    public void Withdraw(decimal amount)\n    {\n        Contract.Requires(amount &gt; 0, \"Withdrawal amount must be positive.\");\n        Contract.Requires(amount &lt;= balance, \"Insufficient funds.\");\n        Contract.Ensures(Balance == Contract.OldValue(Balance) - amount, \"Balance should decrease by the withdrawal amount.\");\n\n        balance -= amount;\n    }\n}\n\npublic class Program\n{\n    public static void Main(string[] args)\n    {\n        BankAccount account = new BankAccount(1000);\n        Console.WriteLine(\"Initial balance: \" + account.Balance);\n\n        account.Deposit(500);\n        Console.WriteLine(\"Balance after deposit: \" + account.Balance);\n\n        account.Withdraw(200);\n        Console.WriteLine(\"Balance after withdrawal: \" + account.Balance);\n    }\n}\n</code></pre> <p>In the example, we have a <code>BankAccount</code> class that represents a bank account. It has a <code>balance</code> field to store the account balance. The <code>Deposit</code> and <code>Withdraw</code> methods are responsible for depositing and withdrawing funds from the account.</p> <p>To apply Design by Contract, we utilize the <code>System.Diagnostics.Contracts</code> namespace, which provides contract classes and attributes to specify the preconditions, postconditions, and invariants of our methods.</p> <p>In the <code>BankAccount</code> constructor, we use the <code>Contract.Requires</code> method to enforce the precondition that the initial balance must be non-negative.</p> <p>In the <code>Deposit</code> method, we use <code>Contract.Requires</code> to enforce the precondition that the deposit amount must be positive. We also use <code>Contract.Ensures</code> to specify the postcondition that the balance should increase by the deposit amount.</p> <p>Similarly, in the <code>Withdraw</code> method, we use <code>Contract.Requires</code> to enforce the precondition that the withdrawal amount must be positive and not exceed the account balance. We also use <code>Contract.Ensures</code> to specify the postcondition that the balance should decrease by the withdrawal amount.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1114-isp","title":"1.1.1.4. ISP","text":"<p>The Interface Segregation Principle (ISP) states that clients should not be forced to depend on interfaces they do not use. It emphasizes the importance of designing fine-grained and specific interfaces, tailored to the needs of the clients, rather than having large, monolithic interfaces that encompass unrelated functionality.</p> <p>NOTE The exact application of the ISP may vary depending on the specific requirements and context of the software being developed. The principle encourages careful consideration of the relationships between clients and interfaces, aiming to create interfaces that are cohesive, focused, and tailored to the needs of the clients that use them.</p> <p>Features of ISP:</p> <ol> <li> <p>Clients</p> <p>Clients refer to the classes or modules that depend on an interface. The ISP emphasizes that clients should only be required to depend on the methods and functionality they actually use. It avoids imposing unnecessary dependencies on clients, which can lead to code coupling and potential issues when changes are made.</p> </li> <li> <p>Interfaces</p> <p>An interface represents a contract or set of methods that a class must implement. The ISP suggests that interfaces should be fine-grained and specific, containing only the methods that are relevant to a particular client or group of clients. By defining focused interfaces, clients can depend on only the subset of methods they require, reducing the impact of changes and promoting better decoupling.</p> </li> <li> <p>Segregation</p> <p>Segregation in this context means separating the responsibilities of interfaces. Rather than having a single interface with a broad range of methods, it is preferable to have multiple interfaces, each catering to a specific set of related methods. This allows clients to depend on interfaces that align with their needs, avoiding unnecessary dependencies and potential complications.</p> </li> </ol> <p>Adherence of ISP:</p> <ol> <li> <p>Interface Segregation</p> <p>The core principle behind ISP is to segregate interfaces based on the needs of clients. Instead of having a single large interface, create multiple smaller interfaces that are specific to different client requirements. This allows clients to depend on the interfaces they actually use, reducing unnecessary coupling and promoting better decoupling.</p> </li> <li> <p>Composition over Inheritance</p> <p>This principle suggests favoring composition (building complex objects by combining simpler objects) over inheritance. By using composition, you can create interfaces that expose only the necessary methods, ensuring that clients depend only on what they require. This approach allows for more flexible and modular designs that adhere to ISP.</p> </li> <li> <p>Separation of Concerns (SoC)</p> <p>SoC is a fundamental principle in software design that promotes dividing a system into distinct, self-contained modules, each responsible for a specific concern. Applying SoC helps adhere to ISP by ensuring that interfaces and classes have a clear and focused purpose, preventing the inclusion of unrelated methods.</p> </li> <li> <p>YAGNI (You Ain't Gonna Need It)</p> <p>YAGNI suggests avoiding premature inclusion of functionality that is not currently needed. Applying YAGNI helps in adhering to ISP by keeping interfaces minimal and focused on immediate requirements. Unnecessary methods can be omitted, preventing clients from depending on features they don't use.</p> </li> <li> <p>Adapter Pattern</p> <p>Use the Adapter pattern to adapt an existing interface into a client-specific interface. This allows clients to work with an interface that aligns with their requirements without modifying the original interface.</p> </li> <li> <p>Facade Pattern</p> <p>The Facade pattern can be employed to provide a simplified interface or facade that hides the complexity of underlying subsystems. By designing focused and specific interfaces in the facade, clients can interact with the subsystems through these interfaces without being exposed to unnecessary methods or functionality.</p> </li> </ol> <p>Examples of ISP in C#:</p> <ol> <li> <p>Interface Segregation</p> <pre><code>public interface IOrderProcessor\n{\n    void ProcessOrder(Order order);\n}\n\npublic interface IShippingService\n{\n    void ShipOrder(Order order);\n}\n\npublic interface IInvoiceService\n{\n    void GenerateInvoice(Order order);\n}\n\npublic class OrderProcessor : IOrderProcessor\n{\n    private readonly IShippingService _shippingService;\n    private readonly IInvoiceService _invoiceService;\n\n    public OrderProcessor(IShippingService shippingService, IInvoiceService invoiceService)\n    {\n        _shippingService = shippingService;\n        _invoiceService = invoiceService;\n    }\n\n    public void ProcessOrder(Order order)\n    {\n        // Process the order\n        _shippingService.ShipOrder(order);\n        _invoiceService.GenerateInvoice(order);\n    }\n}\n\npublic class ShippingService : IShippingService\n{\n    public void ShipOrder(Order order)\n    {\n        // Ship the order\n    }\n}\n\npublic class InvoiceService : IInvoiceService\n{\n    public void GenerateInvoice(Order order)\n    {\n        // Generate the invoice\n    }\n}\n\npublic class Order\n{\n    // Order properties and methods\n}\n</code></pre> <p>In the example, the <code>IOrderProcessor</code> interface represents the responsibility of processing an order. It depends on the <code>IShippingService</code> interface for shipping-related functionality and the <code>IInvoiceService</code> interface for invoice-related functionality. By having separate interfaces for shipping and invoice services, the <code>IOrderProcessor</code> interface adheres to the ISP, as clients can choose to depend on specific interfaces based on their needs.</p> <p>The <code>OrderProcessor</code> class implements the <code>IOrderProcessor</code> interface and uses instances of the <code>ShippingService</code> and <code>InvoiceService</code> classes to handle the shipping and invoice processing, respectively.</p> </li> <li> <p>Composition over Inheritance</p> <p>Bad Example:</p> <pre><code>public interface IAnimal\n{\n    void MakeSound();\n    void Fly();\n    void Swim();\n}\n\npublic class Bird : IAnimal\n{\n    public void MakeSound()\n    {\n        Console.WriteLine(\"Chirp chirp!\");\n    }\n\n    public void Fly()\n    {\n        Console.WriteLine(\"Flying...\");\n    }\n\n    public void Swim()\n    {\n        // Birds cannot swim, so this method is irrelevant and empty.\n    }\n}\n\n// Usage example\nIAnimal bird = new Bird();\nbird.MakeSound(); // Output: Chirp chirp!\nbird.Fly(); // Output: Flying...\nbird.Swim(); // This method is not applicable for birds, but it's still present and can be called.\n</code></pre> <p>In the bad implementation, the <code>IAnimal</code> interface contains methods like <code>Fly()</code> and <code>Swim()</code>. However, not all animals can fly or swim. In the case of the <code>Bird</code> class, it implements the <code>IAnimal</code> interface but leaves the <code>Swim()</code> method empty since birds cannot swim. This violates ISP as clients depending on <code>IAnimal</code> are forced to depend on irrelevant methods.</p> <p>Good Example:</p> <pre><code>public interface IAnimal\n{\n    void MakeSound();\n}\n\npublic interface IFlyable\n{\n    void Fly();\n}\n\npublic class Bird : IAnimal, IFlyable\n{\n    public void MakeSound()\n    {\n        Console.WriteLine(\"Chirp chirp!\");\n    }\n\n    public void Fly()\n    {\n        Console.WriteLine(\"Flying...\");\n    }\n}\n\n// Usage example\nIAnimal bird = new Bird();\nbird.MakeSound(); // Output: Chirp chirp!\n\nIFlyable flyingBird = new Bird();\nflyingBird.Fly(); // Output: Flying...\n</code></pre> <p>In the good implementation, the <code>IAnimal</code> interface is focused solely on the <code>MakeSound()</code> method, which is common to all animals. The <code>IFlyable</code> interface is introduced for animals that can fly, such as birds. By using composition (multiple interfaces), the code adheres to ISP. Clients can selectively depend on the interfaces they need, avoiding unnecessary dependencies and irrelevant methods.</p> </li> <li> <p>Separation of Concerns (SoC)</p> <p>Bad Example:</p> <pre><code>public interface IDataAccess\n{\n    void ReadData();\n    void WriteData();\n    void ProcessData();\n}\n\npublic class DatabaseAccess : IDataAccess\n{\n    public void ReadData()\n    {\n        Console.WriteLine(\"Reading data from the database...\");\n    }\n\n    public void WriteData()\n    {\n        Console.WriteLine(\"Writing data to the database...\");\n    }\n\n    public void ProcessData()\n    {\n        Console.WriteLine(\"Processing data...\");\n    }\n}\n\n// Usage example\nIDataAccess database = new DatabaseAccess();\ndatabase.ReadData(); // Output: Reading data from the database...\ndatabase.WriteData(); // Output: Writing data to the database...\ndatabase.ProcessData(); // Output: Processing data...\n</code></pre> <p>In the bad implementation, the <code>IDataAccess</code> interface contains methods for reading, writing, and processing data. This violates the principle of Separation of Concerns (SoC) as the interface combines multiple responsibilities into a single interface. Clients depending on <code>IDataAccess</code> are forced to depend on methods they might not need, which can lead to unnecessary dependencies and code bloat.</p> <p>Good Example:</p> <pre><code>public interface IDataReader\n{\n    void ReadData();\n}\n\npublic interface IDataWriter\n{\n    void WriteData();\n}\n\npublic interface IDataProcessor\n{\n    void ProcessData();\n}\n\npublic class DatabaseAccess : IDataReader, IDataWriter, IDataProcessor\n{\n    public void ReadData()\n    {\n        Console.WriteLine(\"Reading data from the database...\");\n    }\n\n    public void WriteData()\n    {\n        Console.WriteLine(\"Writing data to the database...\");\n    }\n\n    public void ProcessData()\n    {\n        Console.WriteLine(\"Processing data...\");\n    }\n}\n\n// Usage example\nIDataReader reader = new DatabaseAccess();\nreader.ReadData(); // Output: Reading data from the database...\n\nIDataWriter writer = new DatabaseAccess();\nwriter.WriteData(); // Output: Writing data to the database...\n\nIDataProcessor processor = new DatabaseAccess();\nprocessor.ProcessData(); // Output: Processing data...\n</code></pre> <p>In the good implementation, the responsibilities are separated into three separate interfaces: <code>IDataReader</code>, <code>IDataWriter</code>, and <code>IDataProcessor</code>. The <code>DatabaseAccess</code> class implements these interfaces individually, ensuring that clients can selectively depend on the specific interfaces they require. This adheres to SoC, promoting a more modular and maintainable design.</p> </li> <li> <p>YAGNI (You Ain't Gonna Need It)</p> <p>Bad Example:</p> <pre><code>public interface IShape\n{\n    void Draw();\n    void Resize();\n    void Rotate();\n}\n\npublic class Rectangle : IShape\n{\n    public void Draw()\n    {\n        Console.WriteLine(\"Drawing a rectangle...\");\n    }\n\n    public void Resize()\n    {\n        Console.WriteLine(\"Resizing a rectangle...\");\n    }\n\n    public void Rotate()\n    {\n        Console.WriteLine(\"Rotating a rectangle...\");\n    }\n}\n\n// Usage example\nIShape rectangle = new Rectangle();\nrectangle.Draw(); // Output: Drawing a rectangle...\nrectangle.Resize(); // Output: Resizing a rectangle...\nrectangle.Rotate(); // Output: Rotating a rectangle...\n</code></pre> <p>In the bad implementation, the <code>IShape</code> interface contains methods like <code>Resize()</code> and <code>Rotate()</code>. However, these methods might not be needed for all shapes, such as rectangles. This violates the principle of YAGNI as unnecessary methods are included in the interface, leading to potential dependencies and complexity that is not required.</p> <p>Good Example:</p> <pre><code>public interface IShape\n{\n    void Draw();\n}\n\npublic class Rectangle : IShape\n{\n    public void Draw()\n    {\n        Console.WriteLine(\"Drawing a rectangle...\");\n    }\n}\n\n// Usage example\nIShape rectangle = new Rectangle();\nrectangle.Draw(); // Output: Drawing a rectangle...\n</code></pre> <p>In the good implementation, the <code>IShape</code> interface only contains the necessary method, <code>Draw()</code>, which is common to all shapes. By following YAGNI, the interface is kept minimal and focused, ensuring that clients depend only on what is needed. This results in a simpler and more maintainable design.</p> </li> <li> <p>Adapter Pattern</p> <pre><code>public interface IFileReader\n{\n    string ReadFile(string filePath);\n}\n\npublic class FileDataProvider\n{\n    public string ReadFileData(string filePath)\n    {\n        // Implementation for reading file data\n    }\n}\n\npublic class FileReaderAdapter : IFileReader\n{\n    private readonly FileDataProvider _fileDataProvider;\n\n    public FileReaderAdapter(FileDataProvider fileDataProvider)\n    {\n        _fileDataProvider = fileDataProvider;\n    }\n\n    public string ReadFile(string filePath)\n    {\n        return _fileDataProvider.ReadFileData(filePath);\n    }\n}\n</code></pre> <p>In the example, the <code>IFileReader</code> interface defines a method for reading file content. The <code>FileDataProvider</code> class is an existing class that provides file-related functionality but does not directly implement the <code>IFileReader</code> interface. The <code>FileReaderAdapter</code> class acts as an adapter, wrapping the <code>FileDataProvider</code> and implementing the <code>IFileReader</code> interface. It delegates the file reading operation to the underlying <code>FileDataProvider</code> instance.</p> </li> <li> <p>Facade Pattern</p> <pre><code>// Subsystem classes\npublic class SubsystemA\n{\n    public void OperationA()\n    {\n        Console.WriteLine(\"Performing operation A\");\n    }\n}\n\npublic class SubsystemB\n{\n    public void OperationB()\n    {\n        Console.WriteLine(\"Performing operation B\");\n    }\n}\n\n// Facade class\npublic class Facade\n{\n    private readonly SubsystemA subsystemA;\n    private readonly SubsystemB subsystemB;\n\n    public Facade()\n    {\n        subsystemA = new SubsystemA();\n        subsystemB = new SubsystemB();\n    }\n\n    public void PerformOperation()\n    {\n        subsystemA.OperationA();\n        subsystemB.OperationB();\n    }\n}\n\n// Usage example\nclass Program\n{\n    static void Main(string[] args)\n    {\n        Facade facade = new Facade();\n        facade.PerformOperation();\n\n        // Output:\n        // Performing operation A\n        // Performing operation B\n    }\n}\n</code></pre> <p>In the example, we have two subsystem classes, <code>SubsystemA</code> and <code>SubsystemB</code>, which represent different operations or functionality. The <code>Facade</code> class serves as a simplified interface to these subsystems. It encapsulates the complexity of the subsystems and provides a unified method <code>PerformOperation()</code> that internally calls the specific operations of the subsystems.</p> <p>By using the Facade pattern, client code can interact with the subsystems through the Facade class without directly accessing the subsystem classes. This simplifies the usage and shields the clients from the internal complexities of the subsystems.</p> <p>In the <code>Main</code> method, we create an instance of the <code>Facade</code> class and call the <code>PerformOperation()</code> method. As a result, the operations of both subsystems (<code>SubsystemA</code> and <code>SubsystemB</code>) are executed in a unified and convenient way.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1115-dip","title":"1.1.1.5. DIP","text":"<p>The Dependency Inversion Principle (DIP) is a design principle that states that high-level modules should not depend on low-level modules. Instead, both should depend on abstractions. It promotes decoupling and the use of interfaces or abstract classes to define dependencies, allowing for flexibility, extensibility, and ease of testing.</p> <p>The DIP promotes the use of Abstractions, which are generalizations that define contracts or common behaviors. Abstractions can be represented by interfaces, abstract classes, or protocols, depending on the programming language or framework used. By depending on abstractions, modules become more flexible and interchangeable.</p> <p>Adherence of DIP:</p> <ol> <li> <p>Abstractions</p> <p>Depend on abstractions, not on concrete implementations. This means that high-level modules should define abstract interfaces or base classes that low-level modules can implement or inherit from. The high-level modules should depend on these abstractions, allowing for flexibility and interchangeability of implementations.</p> </li> <li> <p>Dependency Injection (DI)</p> <p>DI is a fundamental technique for adhering to the DIP. It involves injecting dependencies into a class from external sources rather than creating or instantiating them internally. DI promotes loose coupling and allows the client to depend on abstractions or interfaces rather than concrete implementations. There are several approaches to DI, such as constructor injection, property injection, and method injection.</p> </li> <li> <p>Inversion of Control (IoC) Containers</p> <p>IoC containers are tools that facilitate the management and resolution of dependencies in an application. They help enforce the DIP by automatically resolving and injecting dependencies based on predefined configurations. IoC containers eliminate the need for manual dependency resolution, making it easier to adhere to the DIP.</p> </li> <li> <p>Abstract Factories Pattern</p> <p>The Abstract Factory pattern provides an interface for creating families of related or dependent objects without specifying their concrete classes. This pattern allows clients to depend on the abstract factory interface rather than the specific implementation classes. By doing so, the DIP is maintained, and clients can work with different families of objects interchangeably.</p> </li> <li> <p>Service Locator Pattern</p> <p>Use a service locator pattern to centralize the retrieval of dependencies. The service locator acts as a registry or container that holds references to various dependencies and provides a way to look them up when needed. However, be cautious when using this technique, as it can introduce hidden dependencies and make the code harder to maintain and test.</p> </li> </ol> <p>Examples of DIP in C#:</p> <ol> <li> <p>Abstractions</p> <pre><code>public interface ILogger\n{\n    void Log(string message);\n}\n\npublic class FileLogger : ILogger\n{\n    public void Log(string message)\n    {\n        // Logging implementation to a file\n        Console.WriteLine($\"Logging to file: {message}\");\n    }\n}\n\npublic class DatabaseLogger : ILogger\n{\n    public void Log(string message)\n    {\n        // Logging implementation to a database\n        Console.WriteLine($\"Logging to database: {message}\");\n    }\n}\n\npublic class UserManager\n{\n    private readonly ILogger logger;\n\n    public UserManager(ILogger logger)\n    {\n        this.logger = logger;\n    }\n\n    public void CreateUser(string username, string password)\n    {\n        // User creation logic\n        logger.Log($\"User created: {username}\");\n    }\n}\n\npublic class Program\n{\n    static void Main(string[] args)\n    {\n        // Create an instance of UserManager with a specific ILogger implementation\n        ILogger fileLogger = new FileLogger();\n        var userManager = new UserManager(fileLogger);\n\n        // Usage example\n        userManager.CreateUser(\"john\", \"password\");\n\n        // Output:\n        // Logging to file: User created: john\n    }\n}\n</code></pre> <p>In the example, we have an <code>ILogger</code> interface that defines the contract for logging operations. We have two concrete implementations, <code>FileLogger</code> and <code>DatabaseLogger</code>, that implement the <code>ILogger</code> interface.</p> <p>The <code>UserManager</code> class represents a high-level module that depends on the <code>ILogger</code> abstraction. It receives an instance of the <code>ILogger</code> interface through its constructor, adhering to the DIP. This allows different implementations of the logger to be injected at runtime, making the <code>UserManager</code> class flexible and easily testable.</p> <p>In the <code>Main</code> method, we create an instance of <code>FileLogger</code> as the concrete implementation of <code>ILogger</code>. We then pass this instance to the <code>UserManager</code> constructor. When we call the <code>CreateUser</code> method on the <code>userManager</code> instance, it internally uses the <code>ILogger</code> interface to log the user creation event.</p> <p>By depending on the abstraction (<code>ILogger</code>) rather than concrete implementations, we achieve loose coupling and adherence to the Dependency Inversion Principle (DIP). This promotes flexibility, extensibility, and testability in our codebase.</p> </li> <li> <p>Dependency Injection (DI)</p> <pre><code>// Abstraction\npublic interface IService\n{\n    void PerformOperation();\n}\n\n// Concrete implementation\npublic class ConcreteService : IService\n{\n    public void PerformOperation()\n    {\n        Console.WriteLine(\"Performing operation in ConcreteService.\");\n    }\n}\n\n// Class using dependency injection\npublic class ClientClass\n{\n    private readonly IService _service;\n\n    public ClientClass(IService service)\n    {\n        _service = service;\n    }\n\n    public void DoSomething()\n    {\n        _service.PerformOperation();\n    }\n}\n\n// Usage\nvar concreteService = new ConcreteService();\nvar client = new ClientClass(concreteService);\nclient.DoSomething();\n</code></pre> <p>In the example, the <code>ClientClass</code> depends on the <code>IService</code> abstraction through constructor injection. The <code>ConcreteService</code> class implements the <code>IService</code> interface. By injecting the <code>ConcreteService</code> instance into the <code>ClientClass</code> constructor, the dependency is inverted, and the <code>ClientClass</code> is decoupled from the specific implementation details of <code>ConcreteService</code>.</p> </li> <li> <p>Inversion of Control (IoC) Containers</p> <pre><code>// Abstraction\npublic interface IService\n{\n    void PerformOperation();\n}\n\n// Concrete implementation\npublic class ConcreteService : IService\n{\n    public void PerformOperation()\n    {\n        Console.WriteLine(\"Performing operation in ConcreteService.\");\n    }\n}\n\n// Usage with IoC container (e.g., using Autofac)\nvar builder = new ContainerBuilder();\nbuilder.RegisterType&lt;ConcreteService&gt;().As&lt;IService&gt;();\nvar container = builder.Build();\n\nvar client = container.Resolve&lt;ClientClass&gt;();\nclient.DoSomething();\n</code></pre> <p>In the example, an IoC container (such as Autofac) is used to manage the dependencies. The <code>ClientClass</code> is resolved from the container, which automatically resolves the <code>IService</code> dependency and provides an instance of <code>ConcreteService</code>. The IoC container takes care of the dependency injection and inversion of control.</p> </li> <li> <p>Abstract Factory Pattern</p> <pre><code>// Abstract factory interface\npublic interface IAnimalFactory\n{\n    IAnimal CreateAnimal();\n}\n\n// Concrete animal factory implementing the abstract factory\npublic class DogFactory : IAnimalFactory\n{\n    public IAnimal CreateAnimal()\n    {\n        return new Dog();\n    }\n}\n\n// Concrete animal factory implementing the abstract factory\npublic class CatFactory : IAnimalFactory\n{\n    public IAnimal CreateAnimal()\n    {\n        return new Cat();\n    }\n}\n\n// Abstract animal interface\npublic interface IAnimal\n{\n    void MakeSound();\n}\n\n// Concrete animal class implementing the animal interface\npublic class Dog : IAnimal\n{\n    public void MakeSound()\n    {\n        Console.WriteLine(\"Woof!\");\n    }\n}\n\n// Concrete animal class implementing the animal interface\npublic class Cat : IAnimal\n{\n    public void MakeSound()\n    {\n        Console.WriteLine(\"Meow!\");\n    }\n}\n\n// Usage\nIAnimalFactory animalFactory = new DogFactory();\nIAnimal animal = animalFactory.CreateAnimal();\nanimal.MakeSound();\n\nanimalFactory = new CatFactory();\nanimal = animalFactory.CreateAnimal();\nanimal.MakeSound();\n</code></pre> <p>In the example, we have an abstract factory interface <code>IAnimalFactory</code> that defines a method <code>CreateAnimal()</code> for creating <code>IAnimal</code> instances. We have two concrete animal factories, <code>DogFactory</code> and <code>CatFactory</code>, each implementing the <code>IAnimalFactory</code> interface and providing the implementation for creating specific animals.</p> <p>The <code>IAnimal</code> interface represents the abstract animal, and we have two concrete animal classes, <code>Dog</code> and <code>Cat</code>, implementing this interface and providing their respective <code>MakeSound()</code> behavior.</p> <p>At runtime, we can create an instance of an animal factory, such as <code>DogFactory</code> or <code>CatFactory</code>, and use it to create an animal object using the <code>CreateAnimal()</code> method. We can then call the <code>MakeSound()</code> method on the animal instance, which will produce the corresponding sound.</p> </li> <li> <p>Service Locator Pattern</p> <pre><code>// Service locator class\npublic class AnimalServiceLocator\n{\n    private static readonly Dictionary&lt;string, IAnimal&gt; animals = new Dictionary&lt;string, IAnimal&gt;();\n\n    public static void RegisterAnimal(string animalName, IAnimal animal)\n    {\n        animals[animalName] = animal;\n    }\n\n    public static IAnimal GetAnimal(string animalName)\n    {\n        return animals.ContainsKey(animalName) ? animals[animalName] : null;\n    }\n}\n\n// Usage\nAnimalServiceLocator.RegisterAnimal(\"Dog\", new Dog());\nAnimalServiceLocator.RegisterAnimal(\"Cat\", new Cat());\n\nIAnimal animal = AnimalServiceLocator.GetAnimal(\"Dog\");\nanimal.MakeSound();\n\nanimal = AnimalServiceLocator.GetAnimal(\"Cat\");\nanimal.MakeSound();\n</code></pre> <p>In the example, we have a <code>AnimalServiceLocator</code> class that acts as a central registry for animal instances. We can register animals with their corresponding names using the <code>RegisterAnimal()</code> method, and retrieve the animal instance using the <code>GetAnimal()</code> method.</p> <p>At runtime, we can register instances of animals, such as a <code>Dog</code> or a <code>Cat</code>, with their names in the service locator. Later, we can retrieve the animal instance by providing the corresponding name and call the <code>MakeSound()</code> method on it.</p> </li> </ol>"},{"location":"articles/software-design-principles/#112-grasp","title":"1.1.2. GRASP","text":"<p>GRASP (General Responsibility Assignment Software Patterns) is a set of principles that helps in assigning responsibilities to objects in a software system. These principles provide guidelines for developing object-oriented software design by focusing on the interaction between objects and their responsibilities.</p> <p>GRASP patterns ensure that responsibilities are clearly defined and assigned to the appropriate parts of the system, creating a more maintainable, flexible, and scalable software architecture.</p>"},{"location":"articles/software-design-principles/#1121-creator","title":"1.1.2.1. Creator","text":"<p>The Creator pattern is a GRASP pattern that focuses on the problem of creating objects in a system. The Creator pattern assigns the responsibility of object creation to a single class or a group of related classes, known as <code>Factory</code>. This ensures that object creation is done in a centralized and controlled manner, promoting low coupling and high cohesion between classes.</p> <p>The Creator pattern is useful in situations where the creation of objects is complex, or when the creation of objects must be done in a specific sequence. It can also be used to enforce business rules related to object creation, such as ensuring that only a limited number of instances of a class can be created.</p> <p>Types of Creator:</p> <ol> <li> <p>Factory Method</p> <p>A factory method is a design pattern that is responsible for creating objects of a particular class. It allows the class to defer the instantiation to a subclass. The factory method pattern allows for flexible object creation and is useful when the client code does not know which exact subclass is required to create an object.</p> </li> <li> <p>Abstract Factory</p> <p>The abstract factory is a design pattern that provides an interface for creating families of related or dependent objects without specifying their concrete classes. It allows for the creation of a set of objects that work together and depend on each other, without specifying the exact implementation of those objects.</p> </li> </ol> <p>Examples of Creator in C#:</p> <ol> <li> <p>Factory Method</p> <pre><code>public abstract class Animal\n{\n    public abstract string Speak();\n}\n\npublic class Dog : Animal\n{\n    public override string Speak()\n    {\n        return \"Woof!\";\n    }\n}\n\npublic class Cat : Animal\n{\n    public override string Speak()\n    {\n        return \"Meow!\";\n    }\n}\n\npublic abstract class AnimalFactory\n{\n    public abstract Animal CreateAnimal();\n}\n\npublic class DogFactory : AnimalFactory\n{\n    public override Animal CreateAnimal()\n    {\n        return new Dog();\n    }\n}\n\npublic class CatFactory : AnimalFactory\n{\n    public override Animal CreateAnimal()\n    {\n        return new Cat();\n    }\n}\n</code></pre> <p>In the example, we have an abstract <code>Animal</code> class that has a <code>Speak</code> method. We also have two concrete implementations of the <code>Animal</code> class, <code>Dog</code> and <code>Cat</code>, which each have their own implementation of the <code>Speak</code> method.</p> <p>We also have an abstract <code>AnimalFactory</code> class, which has an abstract <code>CreateAnimal</code> method. We then have two concrete implementations of the <code>AnimalFactory</code> class, <code>DogFactory</code> and <code>CatFactory</code>, which each implement the <code>CreateAnimal</code> method to return a <code>Dog</code> or <code>Cat</code> object, respectively.</p> <p>By using the Factory Method pattern in this way, we can create objects of the <code>Dog</code> and <code>Cat</code> classes without having to know the exact implementation of those classes. We simply use the <code>CreateAnimal</code> method of the appropriate factory to create the desired object.</p> </li> <li> <p>Abstract Factory</p> <p>// TODO</p> </li> </ol>"},{"location":"articles/software-design-principles/#1122-controller","title":"1.1.2.2. Controller","text":"<p>The Controller pattern is commonly used in Model-View-Controller (MVC) architectures. The Controller receives input from the user interface, processes the input, and updates the Model and View accordingly. The Controller also handles any errors or exceptions that may occur during the processing of the input. The Controller pattern keeps the presentation logic separate from the business logic, enabling the application to be more modular, maintainable, and testable.</p> <p>In the context of the GRASP, the Controller pattern is a pattern that assigns the responsibility of handling system events and user actions to a single controller object. The Controller acts as an intermediary between the user interface and the domain objects.</p> <p>Examples of Controller in C#:</p> <ol> <li> <p>Dependency Injection</p> <pre><code>public class UserController : Controller\n{\n    private IUserService _userService;\n\n    public UserController(IUserService userService)\n    {\n        _userService = userService;\n    }\n\n    public ActionResult Index()\n    {\n        var users = _userService.GetAllUsers();\n        return View(users);\n    }\n\n    [HttpPost]\n    public ActionResult AddUser(User user)\n    {\n        _userService.AddUser(user);\n        return RedirectToAction(\"Index\");\n    }\n\n    [HttpPost]\n    public ActionResult DeleteUser(int id)\n    {\n        _userService.DeleteUser(id);\n        return RedirectToAction(\"Index\");\n    }\n}\n</code></pre> <p>In the example, the <code>UserController</code> is responsible for handling user actions related to user management. The <code>Index</code> action returns a view that displays all users, the <code>AddUser</code> action adds a new user to the system, and the <code>DeleteUser</code> action deletes a user from the system. The <code>IUserService</code> interface is injected into the <code>UserController</code> constructor, allowing for dependency injection and easier testing.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1123-information-expert","title":"1.1.2.3. Information Expert","text":"<p>Information Expert is a GRASP pattern that states that a responsibility should be assigned to the information expert, which is the class or module that has the most information required to fulfill the responsibility. This pattern helps to promote high cohesion and low coupling, by ensuring that each responsibility is assigned to the class or module that has the most relevant information.</p> <p>In practical terms, the Information Expert pattern can be applied when designing the responsibilities of classes or modules in an object-oriented system. When a new responsibility needs to be added, the designer should identify the class or module that has the most relevant information for that responsibility, and assign the responsibility to that class or module.</p> <p>Examples of Information Expert in C#:</p> <ol> <li> <p>Data Containers</p> <pre><code>public class Order\n{\n    private List&lt;Pizza&gt; pizzas;\n    private List&lt;Topping&gt; toppings;\n    private decimal discount;\n\n    public decimal CalculatePrice()\n    {\n        decimal totalPrice = 0;\n\n        // Calculate the total price of the pizzas\n        foreach (Pizza pizza in pizzas)\n        {\n            totalPrice += pizza.Price;\n        }\n\n        // Add the price of the toppings\n        foreach (Topping topping in toppings)\n        {\n            totalPrice += topping.Price;\n        }\n\n        // Apply any discounts\n        totalPrice -= totalPrice * discount;\n\n        return totalPrice;\n    }\n\n    // Other methods and properties of the Order class\n}\n\npublic class Pizza\n{\n    public decimal Price { get; set; }\n\n    // Other properties of the Pizza class\n}\n\npublic class Topping\n{\n    public decimal Price { get; set; }\n\n    // Other properties of the Topping class\n}\n</code></pre> <p>In the example, the <code>Order</code> class is responsible for calculating the price of the order, since it has access to all the necessary information. The <code>Pizza</code> and <code>Topping</code> classes are just simple data containers that hold the prices of the pizzas and toppings, respectively.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1124-high-cohesion","title":"1.1.2.4. High Cohesion","text":"<p>High Cohesion is a fundamental principle in software engineering that refers to the degree of relatedness of the responsibilities within a module. When the responsibilities within a module are strongly related and focused towards a single goal or purpose, we can say that the module has high cohesion.</p> <p>In the context of GRASP, high cohesion is achieved through the Creator pattern.</p> <p>Examples of High Cohesion in C#:</p> <ol> <li> <p>Creator Pattern</p> <pre><code>public class Order\n{\n    private int orderId;\n    private string customerName;\n    private DateTime orderDate;\n    private List&lt;OrderItem&gt; orderItems;\n\n    public Order(int orderId, string customerName, DateTime orderDate)\n    {\n        this.orderId = orderId;\n        this.customerName = customerName;\n        this.orderDate = orderDate;\n        this.orderItems = new List&lt;OrderItem&gt;();\n    }\n\n    public void AddOrderItem(OrderItem orderItem)\n    {\n        orderItems.Add(orderItem);\n    }\n\n    public void RemoveOrderItem(OrderItem orderItem)\n    {\n        orderItems.Remove(orderItem);\n    }\n\n    public decimal GetTotal()\n    {\n        decimal total = 0;\n        foreach (var orderItem in orderItems)\n        {\n            total += orderItem.Price * orderItem.Quantity;\n        }\n        return total;\n    }\n}\n\npublic class OrderItem\n{\n    private string itemName;\n    private decimal price;\n    private int quantity;\n\n    public OrderItem(string itemName, decimal price, int quantity)\n    {\n        this.itemName = itemName;\n        this.price = price;\n        this.quantity = quantity;\n    }\n\n    public string ItemName { get { return itemName; } }\n    public decimal Price { get { return price; } }\n    public int Quantity { get { return quantity; } }\n}\n</code></pre> <p>In the example, the <code>Order</code> class is responsible for creating and managing order items. The <code>Order</code> class has a high degree of cohesion because it is focused on a single responsibility, which is managing the order and its items. The <code>OrderItem</code> class is responsible only for holding the details of an order item, which is a single responsibility as well.</p> <p>The <code>AddOrderItem()</code> and <code>RemoveOrderItem()</code> methods ensure that the order items are added and removed in a controlled and consistent manner. The <code>GetTotal()</code> method calculates the total amount of the order based on the order items. By assigning the responsibility of creating and managing order items to the <code>Order</code> class, we achieve high cohesion and follow the Creator pattern.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1125-low-coupling","title":"1.1.2.5. Low Coupling","text":"<p>Low Coupling aims to reduce the dependencies between objects by minimizing the communication between them. Low coupling is essential to increase the flexibility, maintainability, and reusability of a system by reducing the impact of changes in one component on other components.</p> <p>In the context of GRASP, low coupling is a design principle that emphasizes reducing the dependencies between classes or modules.</p> <p>Examples of Low Coupling in C#:</p> <ol> <li> <p>Decoupling</p> <pre><code>public class Customer\n{\n    private readonly ILogger _logger;\n    private readonly IEmailService _emailService;\n\n    public Customer(ILogger logger, IEmailService emailService)\n    {\n        _logger = logger;\n        _emailService = emailService;\n    }\n\n    public void PlaceOrder(Order order)\n    {\n        try\n        {\n            // Code to place order\n            _emailService.SendEmail(\"Order Confirmation\", \"Your order has been placed.\");\n        }\n        catch (Exception ex)\n        {\n            _logger.LogError(ex.Message);\n            throw;\n        }\n    }\n}\n\npublic interface IEmailService\n{\n    void SendEmail(string subject, string body);\n}\n\npublic class EmailService : IEmailService\n{\n    public void SendEmail(string subject, string body)\n    {\n        // Code to send email\n    }\n}\n\npublic interface ILogger\n{\n    void LogError(string message);\n}\n\npublic class Logger : ILogger\n{\n    public void LogError(string message)\n    {\n        // Code to log error\n    }\n}\n</code></pre> <p>In the above code example, the <code>Customer</code> class has a low coupling with the <code>EmailService</code> and <code>Logger</code> classes. It depends on abstractions instead of concrete implementations, which makes it flexible and easier to maintain.</p> <p>The <code>Customer</code> class takes the <code>ILogger</code> and <code>IEmailService</code> interfaces in its constructor, which allows it to communicate with the <code>EmailService</code> and <code>Logger</code> classes through these interfaces. This way, the <code>Customer</code> class doesn't depend directly on the concrete implementations of these classes.</p> <p>By using the dependency inversion principle and depending on abstractions instead of concrete implementations, the <code>Customer</code> class is decoupled from the <code>EmailService</code> and <code>Logger</code> classes, which makes it easier to modify and maintain the code.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1126-polymorphism","title":"1.1.2.6. Polymorphism","text":"<p>Polymorphism is a concept in object-oriented programming that allows objects of different types to be treated as if they are the same type. This is achieved through inheritance and interface implementation, where a derived class can be used in place of its base class or interface.</p> <p>In the context of GRASP, the Polymorphism pattern is used to allow multiple implementations of the same interface or abstract class, which can be used interchangeably. This promotes flexibility and extensibility in the design, as new implementations can be added without affecting the existing code.</p> <p>Examples of Polymorphism in C#:</p> <ol> <li> <p>Abstract Class</p> <pre><code>// abstract class\npublic abstract class Animal {\n    public abstract void MakeSound();\n}\n\n// derived classes\npublic class Dog : Animal {\n    public override void MakeSound() {\n        Console.WriteLine(\"Woof!\");\n    }\n}\n\npublic class Cat : Animal {\n    public override void MakeSound() {\n        Console.WriteLine(\"Meow!\");\n    }\n}\n\n// client code\npublic class AnimalSound {\n    public void PlaySound(Animal animal) {\n        animal.MakeSound();\n    }\n}\n\n// usage\nAnimal dog = new Dog();\nAnimal cat = new Cat();\n\nAnimalSound animalSound = new AnimalSound();\nanimalSound.PlaySound(dog);  // output: Woof!\nanimalSound.PlaySound(cat);  // output: Meow!\n</code></pre> <p>In the example, the <code>Animal</code> abstract class defines the <code>MakeSound</code> method, which is implemented by the <code>Dog</code> and <code>Cat</code> classes. The <code>AnimalSound</code> class is the client code that takes an <code>Animal</code> object and calls its <code>MakeSound</code> method, without knowing the specific type of the object.</p> <p>This demonstrates the use of Polymorphism, where the <code>Dog</code> and <code>Cat</code> objects can be treated as if they are <code>Animal</code> objects, allowing the <code>PlaySound</code> method to be reused for different implementations of the <code>Animal</code> class. This promotes flexibility and extensibility in the design, as new implementations of <code>Animal</code> can be added without affecting the existing code.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1127-indirection","title":"1.1.2.7. Indirection","text":"<p>Indirection is a design pattern that adds a level of indirection between components, allowing them to interact without being tightly coupled to each other. The indirection layer acts as an intermediary, providing a consistent and stable interface that insulates the components from changes in each other's implementation details.</p> <p>In the context of GRASP, indirection is a design principle that suggests that a mediator object should be used to decouple two objects that need to communicate with each other. The mediator acts as an intermediary, coordinating the interactions between the objects, and helps to reduce the coupling between them.</p> <p>Examples of Indirection in C#:</p> <ol> <li> <p>Loose Coupling</p> <pre><code>public class ShoppingCart\n{\n    private List&lt;Item&gt; items = new List&lt;Item&gt;();\n\n    public void AddItem(Item item)\n    {\n        items.Add(item);\n    }\n\n    public void RemoveItem(Item item)\n    {\n        items.Remove(item);\n    }\n\n    public decimal CalculateTotal()\n    {\n        decimal total = 0;\n        foreach (var item in items)\n        {\n            total += item.Price;\n        }\n        return total;\n    }\n}\n\npublic class ShoppingCartMediator\n{\n    private ShoppingCart cart;\n\n    public ShoppingCartMediator(ShoppingCart cart)\n    {\n        this.cart = cart;\n    }\n\n    public void AddItem(Item item)\n    {\n        cart.AddItem(item);\n    }\n\n    public void RemoveItem(Item item)\n    {\n        cart.RemoveItem(item);\n    }\n\n    public decimal CalculateTotal()\n    {\n        return cart.CalculateTotal();\n    }\n}\n\npublic class Item\n{\n    public string Name { get; set; }\n    public decimal Price { get; set; }\n}\n</code></pre> <p>In the example, we have a <code>ShoppingCart</code> class that contains a list of <code>Item</code> objects, and provides methods for adding and removing items, as well as calculating the total price of all items in the cart.</p> <p>To reduce coupling between the <code>ShoppingCart</code> and other parts of the application, we introduce a <code>ShoppingCartMediator</code> class that acts as an intermediary between the <code>ShoppingCart</code> and the rest of the application. The <code>ShoppingCartMediator</code> class provides methods for adding and removing items from the cart, as well as calculating the total price, but it delegates these tasks to the <code>ShoppingCart</code> object.</p> <p>This design allows us to make changes to the <code>ShoppingCart</code> class without affecting the rest of the application, as long as the interface of the <code>ShoppingCartMediator</code> remains unchanged. It also allows us to reuse the <code>ShoppingCart</code> class in other parts of the application by simply creating a new <code>ShoppingCartMediator</code> object to act as an intermediary.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1128-pure-fabrication","title":"1.1.2.8. Pure Fabrication","text":"<p>Pure Fabrication is a GRASP pattern used in software development to identify the classes that don't represent a concept in the problem domain but are necessary to fulfill the requirements.</p> <p>A Pure Fabrication class is a class that doesn't correspond to a real-world entity or concept in the problem domain, but it exists to provide a service to other objects or classes in the system. It's an artificial entity created for the sole purpose of fulfilling a specific task or function. Pure Fabrication is useful when there is no other class in the system that naturally fits the responsibility of a particular operation.</p> <p>Types of Pure Fabrication:</p> <ol> <li> <p>Factory Classes</p> <p>These classes create and return instances of other classes. They don't have any real-world counterpart, but they are necessary to create objects when needed.</p> </li> <li> <p>Helper Classes</p> <p>These classes provide utility methods that are not related to any specific object or functionality. They are used by other objects or classes to perform certain operations.</p> </li> <li> <p>Mock Objects</p> <p>These are objects that simulate the behavior of real objects for testing purposes.</p> </li> </ol> <p>Examples of Pure Fabrication in Go:</p> <ol> <li> <p>Factory Classes</p> <p>// TODO</p> </li> <li> <p>Helper Classes</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\ntype MathHelper struct{}\n\nfunc (m *MathHelper) Multiply(a, b int) int {\n    return a * b\n}\n\ntype Product struct {\n    Name     string\n    Price    float64\n    Quantity int\n    Helper   *MathHelper\n}\n\nfunc (p *Product) TotalPrice() float64 {\n    return float64(p.Helper.Multiply(p.Quantity, int(p.Price*100))) / 100\n}\n\nfunc main() {\n    helper := &amp;MathHelper{}\n    product := &amp;Product{\n        Name:     \"Example Product\",\n        Price:    9.99,\n        Quantity: 3,\n        Helper:   helper,\n    }\n    fmt.Printf(\"Total Price for %d units of %s: $%.2f\\n\", product.Quantity, product.Name, product.TotalPrice())\n}\n</code></pre> <p>In the example, we have a <code>MathHelper</code> class that is a Pure Fabrication. It provides a single method <code>Multiply</code> that performs multiplication of two integers. We then have a <code>Product</code> class that has a <code>TotalPrice</code> method, which uses the <code>MathHelper</code> to perform some calculations to return the total price of the product. The <code>Product</code> class delegates the multiplication operation to the <code>MathHelper</code> class, which encapsulates the complex logic of the calculation. This promotes code reuse and makes it easier to maintain the code.</p> </li> <li> <p>Mock Objects</p> <p>// TODO</p> </li> </ol>"},{"location":"articles/software-design-principles/#1129-protected-variations","title":"1.1.2.9. Protected Variations","text":"<p>Protected Variations is a GRASP pattern that is used to identify points of variation in a system and encapsulate them to minimize the impact of changes on the rest of the system. The main idea behind this pattern is to isolate parts of the system that are likely to change in the future, and protect other parts of the system from these changes.</p> <p>Examples of Protected Variations in C#:</p> <ol> <li> <p>Encapsulation</p> <pre><code>public interface IDatabaseProvider\n{\n    void Connect();\n    void Disconnect();\n    // other database-related methods\n}\n\npublic class SqlServerProvider : IDatabaseProvider\n{\n    public void Connect()\n    {\n        // connect to SQL Server database\n    }\n\n    public void Disconnect()\n    {\n        // disconnect from SQL Server database\n    }\n\n    // implement other database-related methods\n}\n\npublic class MySqlProvider : IDatabaseProvider\n{\n    public void Connect()\n    {\n        // connect to MySQL database\n    }\n\n    public void Disconnect()\n    {\n        // disconnect from MySQL database\n    }\n\n    // implement other database-related methods\n}\n\npublic class DataService\n{\n    private readonly IDatabaseProvider _databaseProvider;\n\n    public DataService(IDatabaseProvider databaseProvider)\n    {\n        _databaseProvider = databaseProvider;\n    }\n\n    public void DoSomething()\n    {\n        _databaseProvider.Connect();\n        // do something\n        _databaseProvider.Disconnect();\n    }\n}\n</code></pre> <p>In the example, the <code>IDatabaseProvider</code> interface defines the contract for a database provider, and the <code>SqlServerProvider</code> and <code>MySqlProvider</code> classes encapsulate the variations in the database providers. The <code>DataService</code> class depends on the <code>IDatabaseProvider</code> interface, not on any specific implementation. This allows the system to easily switch between different database providers without impacting the rest of the system.</p> </li> </ol>"},{"location":"articles/software-design-principles/#113-abstraction","title":"1.1.3. Abstraction","text":"<p>Abstraction is a fundamental principle in software design that involves representing complex systems, concepts, or ideas in a simplified and generalized manner. It focuses on extracting essential characteristics and behaviors while hiding unnecessary details.</p> <p>Abstraction helps in managing complexity by allowing developers to work with higher-level concepts rather than getting bogged down in low-level details. It promotes code reusability and modularity by creating well-defined interfaces that can be implemented by different concrete types. Abstraction also improves code maintainability by decoupling different parts of the system and facilitating easier changes and updates.</p> <p>Types of Abstraction:</p> <ol> <li> <p>Abstract Classes</p> <p>An abstract class is a class that cannot be instantiated and is meant to be subclassed. It defines a common interface and may provide default implementations for some methods. Subclasses of an abstract class can provide concrete implementations of abstract methods and extend the functionality as per their specific requirements.</p> </li> <li> <p>Interfaces</p> <p>Interfaces define a contract that a type must adhere to, specifying a set of methods that the implementing type must implement. Interfaces provide a level of abstraction by allowing different types to be treated interchangeably based on the behaviors they provide.</p> </li> <li> <p>Abstract Data Types (ADTs)</p> <p>ADTs provide a high-level abstraction for representing data structures along with the operations that can be performed on them, without exposing the internal implementation details. ADTs encapsulate the data and the associated operations, allowing users to work with the data structure without being concerned about the underlying implementation.</p> </li> </ol> <p>Examples of Abstraction in Go:</p> <ol> <li> <p>Abstract Classes</p> <pre><code>type Shape interface {\n    Area() float64\n}\n\ntype Rectangle struct {\n    Length float64\n    Width  float64\n}\n\nfunc (r Rectangle) Area() float64 {\n    return r.Length * r.Width\n}\n\ntype Circle struct {\n    Radius float64\n}\n\nfunc (c Circle) Area() float64 {\n    return math.Pi * c.Radius * c.Radius\n}\n</code></pre> <p>In the example, the <code>Shape</code> interface defines an abstraction for calculating the area of different shapes. The <code>Rectangle</code> and <code>Circle</code> structs implement the <code>Shape</code> interface and provide their specific implementations of the <code>Area()</code> method.</p> </li> <li> <p>Interfaces</p> <pre><code>type Reader interface {\n    Read(p []byte) (n int, err error)\n}\n\ntype FileWriter struct {\n    // implementation details\n}\n\nfunc (fw FileWriter) Read(p []byte) (n int, err error) {\n    // read implementation\n}\n\ntype NetworkReader struct {\n    // implementation details\n}\n\nfunc (nr NetworkReader) Read(p []byte) (n int, err error) {\n    // read implementation\n}\n</code></pre> <p>In the example, the <code>Reader</code> interface defines the abstraction for reading data. The <code>FileWriter</code> and <code>NetworkReader</code> types both implement the <code>Reader</code> interface, allowing them to be used interchangeably wherever a <code>Reader</code> is required.</p> </li> <li> <p>Abstract Data Types (ADTs)</p> <pre><code>type Stack struct {\n    elements []interface{}\n}\n\nfunc (s *Stack) Push(item interface{}) {\n    // push implementation\n}\n\nfunc (s *Stack) Pop() interface{} {\n    // pop implementation\n}\n</code></pre> <p>In the example, the <code>Stack</code> struct provides an abstraction for a stack data structure. Users can push and pop elements without needing to know the specific implementation details of the stack.</p> </li> </ol>"},{"location":"articles/software-design-principles/#114-separation-of-concerns","title":"1.1.4. Separation of Concerns","text":"<p>Separation of Concerns is a design principle that states that a program should be divided into distinct sections or modules, each responsible for a single concern or aspect of the program's functionality. The idea is to keep different concerns separate and independent of each other, so that changes to one concern do not affect other concerns.</p> <p>This principle is important for creating software that is modular, maintainable, and easy to understand. By separating concerns, developers can focus on writing code that is specific to each concern, without having to worry about how it interacts with other parts of the program. This can make it easier to test and debug code, and can also make it easier to modify and extend the system as requirements change.</p> <p>Examples of SoC in C++:</p> <ol> <li> <p>Separate Handling</p> <p>Violation of SoC:</p> <p>Suppose we have a web application that allows users to search for books and view details about each book. A straightforward implementation might put all of the code for handling the search and display functionality in a single file, like this:</p> <pre><code>class BookSearchController {\npublic:\n  void handleSearchRequest(Request request, Response response) {\n    // retrieve search parameters from request\n    // query database for matching books\n    // render results in HTML and send response\n  }\n\n  void handleBookDetailsRequest(Request request, Response response) {\n    // retrieve book ID from request\n    // query database for book details\n    // render details in HTML and send response\n  }\n};\n</code></pre> <p>While this code might work, it violates the principle of separation of concerns. The <code>BookSearchController</code> class is responsible for handling both search requests and book details requests, which are two distinct concerns. This can make the code more difficult to understand and maintain.</p> <p>Adherence of SoC:</p> <p>A better approach would be to separate the search functionality and book details functionality into two separate modules or classes, like this:</p> <pre><code>class BookSearcher {\npublic:\n  std::vector&lt;Book&gt; searchBooks(std::string query) {\n    // query database for matching books\n    return results;\n  }\n};\n\nclass BookDetailsProvider {\npublic:\n  BookDetails getBookDetails(int bookId) {\n    // query database for book details\n    return details;\n  }\n};\n\nclass BookSearchController {\npublic:\n  void handleSearchRequest(Request request, Response response) {\n    // retrieve search parameters from request\n    BookSearcher searcher;\n    std::vector&lt;Book&gt; results = searcher.searchBooks(query);\n    // render results in HTML and send response\n  }\n};\n\nclass BookDetailsController {\npublic:\n  void handleBookDetailsRequest(Request request, Response response) {\n    // retrieve book ID from request\n    BookDetailsProvider provider;\n    BookDetails details = provider.getBookDetails(bookId);\n    // render details in HTML and send response\n  }\n};\n</code></pre> <p>In the example, we have separated the search functionality and book details functionality into two separate classes: <code>BookSearcher</code> and <code>BookDetailsProvider</code>. These classes are responsible for handling their respective concerns, and can be modified and tested independently of each other.</p> <p>The <code>BookSearchController</code> and <code>BookDetailsController</code> classes are responsible for handling requests and sending responses, but they rely on the <code>BookSearcher</code> and <code>BookDetailsProvider</code> classes to do the actual work. This separation of concerns makes the code easier to understand, modify, and test, and also allows for better code reuse.</p> </li> </ol>"},{"location":"articles/software-design-principles/#115-composition-over-inheritance","title":"1.1.5. Composition over Inheritance","text":"<p>Composition over Inheritance is a design principle that suggests that, in many cases, it is better to use composition (e.g. building complex objects by combining simpler objects) rather than inheritance (e.g. creating new classes that inherit properties and methods from existing classes) to reuse code and achieve polymorphic behavior.</p> <p>The principle encourages developers to favor object composition over class inheritance to achieve code reuse, flexibility, and maintainability. By using composition, developers can create objects that are composed of smaller, reusable components, rather than relying on large and complex inheritance hierarchies.</p> <p>Examples of CoI in C++:</p> <ol> <li> <p>Inheritance vs Composition</p> <p>Violation of CoI:</p> <p>Suppose we have a program that models various shapes, such as circles, rectangles, and triangles. One way to implement this program is to define a base <code>Shape</code> class, and then create specific classes for each type of shape that inherit from the <code>Shape</code> class, like this:</p> <pre><code>class Shape {\npublic:\n  virtual double getArea() = 0;\n};\n\nclass Circle : public Shape {\npublic:\n  double getArea() override {\n    return pi * radius * radius;\n  }\n};\n\nclass Rectangle : public Shape {\npublic:\n  double getArea() override {\n    return width * height;\n  }\n};\n\nclass Triangle : public Shape {\npublic:\n  double getArea() override {\n    return 0.5 * base * height;\n  }\n};\n</code></pre> <p>While this approach might work, it can lead to a complex inheritance hierarchy as more types of shapes are added. Additionally, it might be difficult to add new behavior to a specific shape without affecting the behavior of all other shapes.</p> <p>Adherence of CoI:</p> <p>A better approach might be to use composition, and define separate classes for each aspect of a shape, such as <code>AreaCalculator</code> and <code>ShapeRenderer</code>, like this:</p> <pre><code>class AreaCalculator {\npublic:\n  virtual double getArea() = 0;\n};\n\nclass CircleAreaCalculator : public AreaCalculator {\npublic:\n  double getArea() override {\n    return pi * radius * radius;\n  }\n};\n\nclass RectangleAreaCalculator : public AreaCalculator {\npublic:\n  double getArea() override {\n    return width * height;\n  }\n};\n\nclass TriangleAreaCalculator : public AreaCalculator {\npublic:\n  double getArea() override {\n    return 0.5 * base * height;\n  }\n};\n\nclass ShapeRenderer {\npublic:\n  virtual void render() = 0;\n};\n\nclass CircleRenderer : public ShapeRenderer {\npublic:\n  void render() override {\n    // draw circle\n  }\n};\n\nclass RectangleRenderer : public ShapeRenderer {\npublic:\n  void render() override {\n    // draw rectangle\n  }\n};\n\nclass TriangleRenderer : public ShapeRenderer {\npublic:\n  void render() override {\n    // draw triangle\n  }\n};\n</code></pre> <p>In the example, we have defined separate classes for calculating the area of a shape (<code>AreaCalculator</code>) and rendering a shape (<code>ShapeRenderer</code>). Each specific type of shape has its own implementation of <code>AreaCalculator</code> and <code>ShapeRenderer</code>, which can be combined to create a composite object that has the desired behavior.</p> <p>By using composition, we can create objects that are composed of smaller, reusable components, rather than relying on large and complex inheritance hierarchies. This makes the code more flexible and maintainable, and allows us to add new behavior to specific shapes without affecting the behavior of all other shapes.</p> </li> </ol>"},{"location":"articles/software-design-principles/#116-separation-of-interface-and-implementation","title":"1.1.6. Separation of Interface and Implementation","text":"<p>Separation of Interface and Implementation is a design principle that emphasizes the importance of separating the public interface of a module from its internal implementation. The principle suggests that the public interface of a module should be defined independently of its implementation, so that changes to the implementation do not affect the interface, and changes to the interface do not affect the implementation.</p> <p>The primary goal of separating the interface and implementation is to promote modularity, maintainability, and flexibility. By separating the interface and implementation, developers can modify and improve the internal implementation of a module without affecting other modules that depend on it. Similarly, changes to the interface can be made without affecting the implementation, allowing for better integration with other modules.</p> <p>One common approach to achieving separation of interface and implementation is through the use of abstract classes or interfaces. An abstract class or interface defines a set of public methods that represent the module's interface, but does not provide an implementation for those methods. Instead, concrete classes provide the implementation for the methods defined by the interface.</p> <p>Examples of Separation of Interface and Implementation in C++:</p> <ol> <li> <p>Abstract Class</p> <p>Suppose we have a module that provides a database abstraction layer, which allows other modules to interact with the database without having to deal with the details of the underlying implementation. The module consists of a set of classes that provide the implementation for various database operations, such as querying, inserting, and updating data.</p> <p>To separate the interface and implementation, we can define an abstract class or interface that represents the public interface of the database abstraction layer. For example:</p> <pre><code>class Database {\npublic:\n  virtual bool connect() = 0;\n  virtual bool disconnect() = 0;\n  virtual bool executeQuery(const std::string&amp; query) = 0;\n  virtual bool executeUpdate(const std::string&amp; query) = 0;\n};\n</code></pre> <p>In the example, the <code>Database</code> class defines a set of methods that represent the public interface of the database abstraction layer. These methods include <code>connect</code>, <code>disconnect</code>, <code>executeQuery</code>, and <code>executeUpdate</code>, which are used to establish a connection to the database, disconnect from the database, execute a query, and execute an update, respectively.</p> <p>With the interface defined, we can now provide concrete implementations of the <code>Database</code> class that provide the actual functionality for the database operations. For example:</p> <pre><code>class MySqlDatabase : public Database {\npublic:\n  virtual bool connect() override {\n    // connect to MySQL database\n  }\n  virtual bool disconnect() override {\n    // disconnect from MySQL database\n  }\n  virtual bool executeQuery(const std::string&amp; query) override {\n    // execute query against MySQL database\n  }\n  virtual bool executeUpdate(const std::string&amp; query) override {\n    // execute update against MySQL database\n  }\n};\n\nclass PostgresDatabase : public Database {\npublic:\n  virtual bool connect() override {\n    // connect to Postgres database\n  }\n  virtual bool disconnect() override {\n    // disconnect from Postgres database\n  }\n  virtual bool executeQuery(const std::string&amp; query) override {\n    // execute query against Postgres database\n  }\n  virtual bool executeUpdate(const std::string&amp; query) override {\n    // execute update against Postgres database\n  }\n};\n</code></pre> <p>In the example, we have provided concrete implementations of the <code>Database</code> class for MySQL and Postgres databases. These classes provide the actual functionality for the database operations defined by the <code>Database</code> interface, but the interface is independent of the implementation, allowing us to modify the implementation without affecting other modules that depend on the <code>Database</code> abstraction layer.</p> </li> </ol>"},{"location":"articles/software-design-principles/#117-convention-over-configuration","title":"1.1.7. Convention over Configuration","text":"<p>Convention over Configuration (CoC) is a software design principle that suggests that a framework or tool should provide sensible default configurations based on conventions, rather than requiring explicit configuration for every aspect of the system. This means that the developer doesn't have to write any configuration files, and the framework will automatically assume certain conventions and defaults to simplify the development process.</p> <p>Benefits of CoC:</p> <ol> <li> <p>Increased Productivity</p> <p>By reducing the amount of configuration that developers need to write, Convention over Configuration increases productivity. Developers can focus on writing code and building features rather than configuring the system.</p> </li> <li> <p>Reduced Complexity</p> <p>With sensible defaults, developers don't need to worry about every detail of the configuration. They can rely on the framework to do the right thing, which reduces complexity and makes the system easier to maintain.</p> </li> <li> <p>Better Consistency</p> <p>By following conventions, different parts of the system will work together seamlessly, reducing the risk of errors and inconsistencies.</p> </li> <li> <p>Easier Maintenance</p> <p>Because the system follows established conventions, it is easier for new developers to understand and maintain the code. They don't need to learn all the configuration options, only the conventions.</p> </li> </ol> <p>Examples of CoC in Go:</p> <ol> <li> <p>Conventions</p> <p>A Go web application using the popular Gin web framework:</p> <pre><code>package main\n\nimport \"github.com/gin-gonic/gin\"\n\nfunc main() {\n    router := gin.Default()\n    router.GET(\"/\", func(c *gin.Context) {\n        c.JSON(200, gin.H{\n            \"message\": \"Hello, World!\",\n        })\n    })\n    router.Run() // automatically uses the default configuration of \"localhost:8080\"\n}\n</code></pre> <p>In the example, we're creating a new Gin router and defining a simple <code>GET</code> route for the root path that returns a JSON response. We don't have to specify any configuration options for the router because Gin follows the convention of using <code>localhost:8080</code> as the default address and port.</p> <p>This allows to focus on writing the actual application logic and not worry about boilerplate code or configuration details. Additionally, since Gin provides a set of standard conventions for routing, middleware, and error handling, we can easily reuse and share our code with other developers who are also using the framework.</p> </li> </ol>"},{"location":"articles/software-design-principles/#118-coupling","title":"1.1.8. Coupling","text":"<p>Coupling in software engineering refers to the degree of interdependence between two software components. In other words, it measures how much one component depends on another component.</p> <p>Coupling can be classified into different types based on the nature of the dependency. In general, loose coupling is preferred over tight coupling because it makes the system more modular and easier to maintain. Developers can achieve loose coupling by using design patterns such as Dependency Injection, Observer pattern, and Event-driven architecture.</p> <p>Types of Coupling:</p> <ol> <li> <p>Loose Coupling</p> <p>Loose coupling occurs when two or more components are relatively independent of each other. In a loosely coupled system, changes to one component do not require changes to other components, which can make the system more modular and easier to maintain.</p> </li> <li> <p>Tight Coupling</p> <p>Tight coupling occurs when two or more components are highly dependent on each other. In a tightly coupled system, changes to one component require changes to other components, which can make the system difficult to maintain and modify.</p> </li> <li> <p>Content Coupling</p> <p>Content coupling occurs when one component directly accesses or modifies the data of another component. Content coupling can lead to tight coupling and can make the system difficult to maintain and modify.</p> </li> <li> <p>Control Coupling</p> <p>Control coupling occurs when one component passes control information to another component, such as a flag or a signal. Control coupling can be either tight or loose depending on the nature of the control information.</p> </li> <li> <p>Data Coupling</p> <p>Data coupling occurs when two components share data but do not have direct access to each other's code. Data coupling can be either tight or loose depending on the nature of the data sharing.</p> </li> <li> <p>Common Coupling</p> <p>Common coupling occurs when two or more components share a global data area. Common coupling can lead to tight coupling and can make the system difficult to maintain and modify.</p> </li> </ol> <p>Examples of Coupling in C#:</p> <ol> <li> <p>Loose Coupling</p> <pre><code>public interface IEngine {\n    void Start();\n}\n\npublic class Car {\n    private readonly IEngine engine;\n\n    public Car(IEngine engine) {\n        this.engine = engine;\n    }\n\n    public void Move() {\n        // code to move the car forward\n    }\n}\n</code></pre> <p>In the example, the <code>Car</code> class is loosely coupled with the <code>IEngine</code> interface. The <code>Car</code> class does not depend on any specific implementation of the <code>IEngine</code> interface, which means that it is easier to change the implementation without affecting the <code>Car</code> class.</p> </li> <li> <p>Tight Coupling</p> <pre><code>public class Car {\n    public void StartEngine() {\n        // code to start the engine\n    }\n    public void Move() {\n        // code to move the car forward\n    }\n}\n</code></pre> <p>In the example, the <code>Move</code> method depends on the <code>StartEngine</code> method, which means that the two methods are tightly coupled. Any change to the <code>StartEngine</code> method may affect the <code>Move</code> method as well.</p> </li> <li> <p>Content Coupling</p> <pre><code>public class Employee {\n    public string Name { get; set; }\n    public void UpdateSalary(double amount) {\n        // code to update the salary\n    }\n}\n\npublic class PayrollSystem {\n    private readonly Employee employee;\n\n    public PayrollSystem(Employee employee) {\n        this.employee = employee;\n    }\n\n    public void CalculateSalary() {\n        // code to calculate the salary based on the employee data\n        employee.UpdateSalary(amount);\n    }\n}\n</code></pre> <p>In the example, the <code>PayrollSystem</code> class directly modifies the data of the <code>Employee</code> class, which means that it is content-coupled with the <code>Employee</code> class.</p> </li> <li> <p>Control Coupling</p> <pre><code>public class Button {\n    public event EventHandler Click;\n\n    public void OnClick() {\n        Click?.Invoke(this, EventArgs.Empty);\n    }\n}\n\npublic class Window {\n    private readonly Button button;\n\n    public Window(Button button) {\n        this.button = button;\n        this.button.Click += ButtonClicked;\n    }\n\n    private void ButtonClicked(object sender, EventArgs e) {\n        // code to handle the button click event\n    }\n}\n</code></pre> <p>In the example, the <code>Button</code> class signals the <code>Window</code> class using the <code>Click</code> event. This is an example of control coupling, where one component passes control information to another component.</p> </li> <li> <p>Data Coupling</p> <pre><code>public class Calculator {\n    public int Add(int a, int b) {\n        return a + b;\n    }\n}\n\npublic class Display {\n    public void ShowResult(int result) {\n        // code to display the result\n    }\n}\n\npublic class CalculatorController {\n    private readonly Calculator calculator;\n    private readonly Display display;\n\n    public CalculatorController(Calculator calculator, Display display) {\n        this.calculator = calculator;\n        this.display = display;\n    }\n\n    public void Calculate(int a, int b) {\n        int result = calculator.Add(a, b);\n        display.ShowResult(result);\n    }\n}\n</code></pre> <p>In the example, the <code>CalculatorController</code> class shares data between the <code>Calculator</code> and <code>Display</code> classes but does not have direct access to their code. This is an example of data coupling, where two components share data but do not have direct access to each other's code.</p> </li> <li> <p>Common Coupling</p> <pre><code>public static class GlobalData\n{\n    public static int Counter;\n}\n\npublic class Module1\n{\n    public void IncrementCounter()\n    {\n        GlobalData.Counter++;\n    }\n}\n\npublic class Module2\n{\n    public void DecrementCounter()\n    {\n        GlobalData.Counter--;\n    }\n}\n</code></pre> <p>In the example, the <code>Module1</code> and <code>Module2</code> classes both have access to the global <code>Counter</code> variable through the <code>GlobalData</code> class. If either module modifies the <code>Counter</code> variable, it will affect the other module's behavior, which can lead to unexpected bugs and errors.</p> <p>To avoid common coupling, it is best to encapsulate data within classes and avoid global data entities. This allows each module to have its own state and behavior without affecting the behavior of other modules.</p> </li> </ol>"},{"location":"articles/software-design-principles/#119-cohesion","title":"1.1.9. Cohesion","text":"<p>Cohesion refers to the degree to which the elements within a module or class are related to each other and work together to achieve a single, well-defined purpose. High cohesion indicates that the elements within a module or class are closely related and work together effectively, while low cohesion indicates that the elements may not be well-organized and may not work together effectively.</p> <p>NOTE High cohesion is generally desirable because it results in modules or classes that are easier to understand, maintain, and modify. However, achieving high cohesion often requires a careful design process and can involve trade-offs with other design principles such as coupling.</p> <p>Types of Cohesion:</p> <ol> <li> <p>Functional Cohesion</p> <p>Functional cohesion is a type of cohesion in which the functions within a module are related and perform a single, well-defined task or a closely related set of tasks. This type of cohesion is desirable as it promotes reusability and modularity.</p> </li> <li> <p>Sequential Cohesion</p> <p>Sequential cohesion refers to a situation where elements or functions within a module are organized in a sequence where the output of one function becomes the input of the next function. This type of cohesion is also known as temporal cohesion. The purpose of sequential cohesion is to process a sequence of tasks in a specific order.</p> </li> <li> <p>Communicational Cohesion</p> <p>Communicational cohesion is one of the types of cohesion, in which elements of a module are grouped together because they operate on the same data or input and output of a task. This type of cohesion focuses on the communication between module elements.</p> </li> <li> <p>Procedural Cohesion</p> <p>Procedural cohesion is a type of cohesion that groups related functionality of a module based on the procedure or method being performed. The code within a procedure is highly related to each other and performs a single task.</p> </li> <li> <p>Temporal Cohesion</p> <p>Temporal cohesion is when the elements within a module or function are related and must be executed in a specific order over time. In other words, temporal cohesion is when elements of a module or function must be executed in a specific order for the module or function to work properly.</p> <p>NOTE Temporal cohesion is generally not desirable because it makes the code harder to read and understand, and it can also make the code more error-prone if the order of execution is not followed correctly.</p> </li> <li> <p>Logical Cohesion</p> <p>Logical cohesion is a type of cohesion where the elements of a module are logically related and perform a single well-defined task. The focus is on grouping similar responsibilities together in a way that they are performed by a single function or module. This helps in creating a codebase that is more maintainable, testable, and reusable.</p> </li> </ol> <p>Examples of Cohesion in Go:</p> <ol> <li> <p>Functional Cohesion</p> <pre><code>package math\n\n// Add returns the sum of two integers\nfunc Add(a, b int) int {\n    return a + b\n}\n\n// Subtract returns the difference between two integers\nfunc Subtract(a, b int) int {\n    return a - b\n}\n\n// Multiply returns the product of two integers\nfunc Multiply(a, b int) int {\n    return a * b\n}\n\n// Divide returns the quotient of two integers\nfunc Divide(a, b int) (int, error) {\n    if b == 0 {\n        nil, error(\"division by zero\")\n    }\n    return a / b, nil\n}\n</code></pre> <p>In the example, the functions in the <code>math</code> package are all related to performing arithmetic operations. They have a clear and focused purpose, and each function performs a single task.</p> </li> <li> <p>Sequential Cohesion</p> <pre><code>func FetchData() ([]byte, error) {\n    // ...\n}\n\nfunc ParseData(data []byte) (Data, error) {\n    // ...\n}\n\nfunc ProcessData(data Data) (Result, error) {\n    // ...\n}\n\nfunc OutputResult(result Result) error {\n    // ...\n}\n\nfunc RunPipeline() error {\n    data, err := FetchData()\n    if err != nil {\n        return err\n    }\n\n    parsedData, err := ParseData(data)\n    if err != nil {\n        return err\n    }\n\n    processedData, err := ProcessData(parsedData)\n    if err != nil {\n        return err\n    }\n\n    err = OutputResult(processedData)\n    if err != nil {\n        return err\n    }\n\n    return nil\n}\n</code></pre> <p>In the example, the output of one module is the input of another in a pipeline of functions that transform data from one form to another.</p> </li> <li> <p>Communicational Cohesion</p> <pre><code>type User struct {\n    ID        int\n    FirstName string\n    LastName  string\n    Email     string\n}\n\nfunc saveUser(user *User) error {\n    // Insert the user into the database\n    return nil\n}\n\nfunc getUser(id int) (*User, error) {\n    // Get the user from the database\n    return &amp;User{}, nil\n}\n</code></pre> <p>In the example, the functions <code>saveUser</code> and <code>getUser</code> perform different tasks, but they are both related to the <code>User</code> struct, which represents a user in the system. They communicate with the same data structure and perform operations related to it.</p> </li> <li> <p>Procedural Cohesion</p> <pre><code>func processRequest(req Request) Response {\n    logRequest(req)\n    authenticateUser(req)\n    validateRequest(req)\n    handleRequest(req)\n    logResponse(res)\n\n    return res\n}\n</code></pre> <p>In the example, the function processes a request by logging it, authenticating the user, validating the request, handling the request, and logging the response. The tasks are not necessarily related but are required to process the request.</p> </li> <li> <p>Temporal Cohesion</p> <pre><code>func main() {\n    scheduleTask1()\n    time.Sleep(time.Second * 5) // Wait for 5 seconds\n    scheduleTask2()\n}\n\nfunc scheduleTask1() {\n    fmt.Println(\"Task 1 scheduled.\")\n}\n\nfunc scheduleTask2() {\n    fmt.Println(\"Task 2 scheduled.\")\n}\n</code></pre> <p>In the example, all the scheduleTask() functions are related to each other and should be executed in a specific order with a specific time gap between them. They are executed in a sequence such that Task 1 is scheduled, then Task 2 is scheduled after 5 seconds.</p> <p>This demonstrates the concept of temporal cohesion, where all the tasks are related to each other and should be executed at specific times to achieve the desired result.</p> </li> <li> <p>Logical Cohesion</p> <pre><code>package logger\n\ntype Logger struct {\n    // fields related to the logger\n}\n\nfunc (l *Logger) LogInfo(message string) {\n    // code to log info messages\n}\n\nfunc (l *Logger) LogError(message string) {\n    // code to log error messages\n}\n</code></pre> <p>In the example, we have a <code>Logger</code> struct that has fields related to the logger. The <code>LogInfo()</code> and <code>LogError()</code> methods are related to logging different types of messages and hence are logically cohesive.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1110-modularity","title":"1.1.10. Modularity","text":"<p>Modularity is a design principle that involves breaking down a large system into smaller, more manageable and independent modules, each with its own well-defined functionality. The main objective of modularity is to simplify the complexity of a system, improve maintainability, and promote reusability.</p> <p>In software development, modularity is achieved by dividing the codebase into smaller, self-contained modules that can be developed, tested, and deployed independently. Each module should have a clear interface that defines the inputs, outputs, and responsibilities of the module. The interface should be well-defined and easy to use, which promotes ease of integration and promotes reusability.</p> <p>Examples of Modularity in Go:</p> <ol> <li> <p>Independent Modules</p> <pre><code>// greetings.go\n\npackage greetings\n\nimport \"fmt\"\n\n// Returns a greeting message for the given name\nfunc Greet(name string) string {\n    return fmt.Sprintf(\"Hello, %s!\", name)\n}\n</code></pre> <pre><code>// main.go\n\npackage main\n\nimport (\n    \"fmt\"\n    \"example.com/greetings\"\n)\n\nfunc main() {\n    message := greetings.Greet(\"John\")\n    fmt.Println(message)\n}\n</code></pre> <p>In the example, the <code>greetings</code> package contains a single function <code>Greet</code> that returns a greeting message for a given name. This function can be reused in other parts of the codebase, promoting reusability. The <code>main</code> package uses the <code>greetings</code> package to generate a greeting message for the name \"John\".</p> <p>By dividing the code into self-contained and independent modules, we promote modularity, which makes the codebase easier to understand, maintain, and extend. Additionally, each module can be tested independently, promoting testability and making the codebase more robust.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1111-encapsulation","title":"1.1.11. Encapsulation","text":"<p>Encapsulation is a fundamental concept in object-oriented programming (OOP) that involves bundling data and related functionality (e.g., methods) together into a single unit called a class. The idea behind encapsulation is to hide the internal details of an object from the outside world and provide a public interface through which the object can be accessed and manipulated.</p> <p>In encapsulation, the data of an object is stored in private variables, which can only be accessed and modified by the methods of the same class. The public methods of the class are used to access and manipulate the private data in a controlled way. This ensures that the internal state of the object is not corrupted or manipulated in an unintended way.</p> <p>Benefits of Encapsulation:</p> <ol> <li> <p>Modularity</p> <p>Encapsulation promotes modularity by allowing the codebase to be divided into smaller, self-contained units. The implementation details of each unit are hidden, which makes the codebase easier to understand, maintain, and extend.</p> </li> <li> <p>Security</p> <p>Encapsulation provides a mechanism for protecting data from unauthorized access or modification. By keeping the implementation details hidden, only authorized parts of the codebase can access the data, which promotes security.</p> </li> <li> <p>Abstraction</p> <p>Encapsulation promotes abstraction by providing a simplified interface for interacting with complex data structures. The interface hides the implementation details of the data structure, which makes it easier to use and reduces complexity.</p> </li> <li> <p>Code Reuse</p> <p>Encapsulation promotes code reuse by allowing the same implementation to be used in multiple parts of the codebase. The implementation details are hidden, which makes it easier to integrate the implementation into other parts of the codebase.</p> </li> <li> <p>Maintenance</p> <p>Encapsulation makes it easier to maintain the codebase by reducing the impact of changes to the implementation details. Because the implementation details are hidden, changes can be made without affecting other parts of the codebase.</p> </li> <li> <p>Testing</p> <p>Encapsulation promotes testing by providing a well-defined interface for testing the behavior of the data structure. Tests can be written against the interface, which promotes testability and makes the codebase more robust.</p> </li> </ol> <p>Examples of Encapsulation in C#:</p> <ol> <li> <p>Encapsulation</p> <pre><code>public class BankAccount\n{\n    private decimal balance;\n\n    public void Deposit(decimal amount)\n    {\n        balance += amount;\n    }\n\n    public void Withdraw(decimal amount)\n    {\n        balance -= amount;\n    }\n\n    public decimal GetBalance()\n    {\n        return balance;\n    }\n}\n</code></pre> <p>In the example, the <code>BankAccount</code> class encapsulates the balance data and methods that operate on that data. The implementation details of the balance data are hidden from other parts of the codebase. The class provides a public interface (<code>Deposit</code>, <code>Withdraw</code>, <code>GetBalance</code>) for other parts of the codebase to interact with the balance data. This promotes modularity, security, abstraction, code reuse, maintenance, and testing.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1112-principle-of-least-astonishment","title":"1.1.12. Principle of Least Astonishment","text":"<p>The Principle of Least Astonishment (POLA) or the Principle of Least Surprise, is a software design principle that primarily focuses on user experience and design considerations. POLA suggests designing systems and interfaces in a way that minimizes user confusion, surprises, and unexpected behaviors. The goal is to make the system behave in a way that is intuitive and aligns with users' expectations, reducing the likelihood of errors and improving user satisfaction.</p> <p>The principle is based on the assumption that users will make assumptions and predictions about how a system or interface should work based on their prior experiences with similar systems. Therefore, the design should align with these assumptions to minimize confusion and cognitive load.</p> <p>By applying the Principle of Least Astonishment, developers can create systems and interfaces that are more intuitive, predictable, and user-friendly. This reduces the learning curve for users, minimizes errors and frustration, and ultimately improves the user experience.</p> <p>Types of POLA:</p> <ol> <li> <p>Consistency</p> <p>The system should follow consistent and predictable patterns across different features and interactions. Users should not encounter unexpected changes or variations in behavior.</p> </li> <li> <p>Conventions</p> <p>Utilize established conventions and standards in the design to leverage users' existing knowledge and expectations. This includes following platform-specific guidelines, industry best practices, and familiar interaction patterns.</p> </li> <li> <p>Feedback</p> <p>Provide clear and timely feedback to users about the outcome of their actions. Inform them about any changes in the system's state, errors, or potential consequences to prevent confusion or surprises.</p> </li> <li> <p>Minimize Complexity</p> <p>Keep the system's complexity at a manageable level by simplifying interfaces, reducing the number of options, and avoiding unnecessary complexity. Complexity can lead to confusion and increase the chances of surprising behavior.</p> </li> <li> <p>Clear and Descriptive Documentation</p> <p>Provide comprehensive and easily accessible documentation that explains the system's behavior, features, and any potential pitfalls or exceptions. This helps users understand and anticipate the system's behavior.</p> </li> <li> <p>User Testing and Feedback</p> <p>Regularly gather user feedback and conduct usability testing to identify any instances where the system's behavior surprises or confuses users. Incorporate this feedback into the design to align with users' mental models and expectations.</p> </li> </ol> <p>Examples of POLA IN Go:</p> <ol> <li> <p>Consistency:</p> <p>Bad Example:</p> <pre><code>// Inconsistent naming and code style\nfunc calc(r float64) float64 {\n    return 3.14 * r * r\n}\n</code></pre> <p>The bad example, on the other hand, uses unclear naming and abbreviations, which can be confusing and surprising to other developers.</p> <p>Good Example:</p> <pre><code>// Consistent naming and code style\nfunc calculateArea(radius float64) float64 {\n    return math.Pi * radius * radius\n}\n</code></pre> <p>In the good example, the function <code>calculateArea</code> follows a consistent naming convention and uses descriptive variable names, making the code more readable and easier to understand.</p> </li> <li> <p>Conversations</p> <p>Naming Conventions:</p> <pre><code>// Struct names in CamelCase\ntype UserProfile struct {\n    // Field names in CamelCase\n    FirstName string\n    LastName  string\n}\n</code></pre> <p>Error Handling Conventions:</p> <pre><code>// Use named return values to indicate errors\nfunc GetUserByID(userID string) (User, error) {\n    // ...\n    if err != nil {\n        return User{}, fmt.Errorf(\"failed to retrieve user: %w\", err)\n    }\n    // ...\n}\n</code></pre> <p>Comment Conventions:</p> <pre><code>// User represents a user in the system\ntype User struct {\n    ID       int\n    Username string\n}\n</code></pre> <p>Package and File Structure Conventions:</p> <pre><code>// Package name matches the directory name\npackage mypackage\n\n// Import statements grouped and sorted\nimport (\n    \"fmt\"\n    \"net/http\"\n)\n\n// File names follow the snake_case convention\nfunc myFunction() {\n    // Function body\n}\n</code></pre> <p>Code Formatting Conventions:</p> <pre><code>// Indentation with tabs or spaces\nfunc main() {\n    for i := 0; i &lt; 10; i++ {\n        if i%2 == 0 {\n            fmt.Println(i)\n        }\n    }\n}\n</code></pre> <p>Function and Method Naming Conventions:</p> <pre><code>// Function name in camelCase\nfunc calculateTotalPrice(prices []float64) float64 {\n    // ...\n}\n\n// Method name in CamelCase\nfunc (c *Calculator) Add(a, b int) int {\n    // ...\n}\n</code></pre> <p>These examples illustrate some common conventions in Go programming, such as following naming conventions, structuring packages and files, handling errors, formatting code, and naming functions and methods. By adhering to these conventions, your code becomes more readable, maintainable, and consistent with established Go programming practices. This promotes code understandability and helps other developers easily work with and contribute to the codebase.</p> </li> <li> <p>Feedback</p> <p>Bad Example:</p> <pre><code>// Lack of feedback\nfunc divide(a int, b int) int {\n    // Division without handling the zero case\n    return a / b\n}\n</code></pre> <p>Good Example:</p> <pre><code>// Clear feedback through error messages\nfunc divide(a int, b int) (int, error) {\n    if b == 0 {\n        return 0, errors.New(\"Cannot divide by zero\")\n    }\n    return a / b, nil\n}\n</code></pre> <p>In the good example, the <code>divide</code> function provides clear feedback by returning an error when attempting to divide by zero. This feedback informs users about the exceptional case and prevents unexpected results or surprises.</p> </li> <li> <p>Minimize Complexity</p> <p>Bad Example:</p> <pre><code>// Complex and convoluted code\nfor i := 0; i &lt; len(items); i++ {\n    if items[i].IsValid() &amp;&amp; items[i].Status == \"Active\" {\n        // Process item\n    }\n}\n</code></pre> <p>The bad example introduces unnecessary complexity with additional conditions and checks, which can surprise developers and make the code harder to understand and maintain.</p> <p>Good Example:</p> <pre><code>// Simple and readable code\nif len(items) &gt; 0 {\n    for _, item := range items {\n        // Process item\n    }\n}\n</code></pre> <p>In the good example, the code follows a straightforward and intuitive approach to iterate over a collection of items.</p> </li> <li> <p>Clear and Descriptive Documentation</p> <p>Bad Example:</p> <pre><code>// Tax calculates the tax.\nfunc Tax(p float64, r float64) float64 {\n    return p * r\n}\n</code></pre> <p>The bad example lacks clarity and context, making it difficult for others to understand the intended behavior of the function.</p> <p>Good Example:</p> <pre><code>// CalculateTax calculates the tax amount based on the given price and tax rate.\nfunc CalculateTax(price float64, taxRate float64) float64 {\n    return price * taxRate\n}\n</code></pre> <p>In the good example, the documentation provides clear and descriptive information about the function's purpose and parameters, reducing any potential surprises or confusion for developers who use the function.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1113-principle-of-least-privilege","title":"1.1.13. Principle of Least Privilege","text":"<p>The Principle of Least Privilege (POLP) or the Principle of Least Authority, is a security principle in software design and access control. It states that a user, program, or process should be given only the minimum privileges or permissions necessary to perform its required tasks, and no more.</p> <p>The principle aims to reduce the potential impact of security breaches or vulnerabilities by limiting the access and capabilities of entities within a system. By granting minimal privileges, the risk of accidental or intentional misuse, data breaches, and unauthorized actions can be significantly reduced.</p> <p>NOTE Implementing the POLP requires careful consideration of user roles, permissions, and access controls. It may involve defining fine-grained access policies, enforcing strong authentication mechanisms, and regularly reviewing and updating access privileges based on changing requirements or personnel changes.</p> <p>Types of POLP:</p> <ol> <li> <p>User Roles and Permissions</p> <p>Define roles or user groups based on job responsibilities or system requirements. Grant each role the necessary permissions to perform their designated tasks and restrict access to sensitive or privileged operations.</p> </li> <li> <p>Access Controls</p> <p>Implement access control mechanisms, such as authentication and authorization, to enforce the Principle of Least Privilege. Only authenticated and authorized entities should be granted access to specific resources or functionalities.</p> </li> <li> <p>Privilege Separation</p> <p>Separate privileges and separate functionalities based on their security requirements. For example, separate administrative functions from regular user functions, and limit access to administrative features to authorized personnel only.</p> </li> <li> <p>Principle of Minimal Authority</p> <p>Grant the minimum level of privilege required for a task to be executed successfully. Avoid granting unnecessary or excessive permissions that can potentially be misused.</p> </li> <li> <p>Regular Auditing and Reviews</p> <p>Conduct periodic audits and reviews of user privileges and access permissions to ensure they align with the Principle of Least Privilege. Remove or modify privileges that are no longer needed or are deemed excessive.</p> </li> </ol> <p>Benefits of POLP:</p> <ol> <li> <p>Reduced Attack Surface</p> <p>Limiting privileges reduces the potential impact of an attacker gaining unauthorized access to critical resources or performing malicious actions.</p> </li> <li> <p>Minimized Damage</p> <p>In the event of a security breach or vulnerability exploitation, the potential damage or impact is limited to the privileges assigned to the compromised entity.</p> </li> <li> <p>Improved System Integrity</p> <p>By separating privileges and limiting access, the system integrity is enhanced, preventing unintended or unauthorized modifications.</p> </li> <li> <p>Compliance with Regulations</p> <p>Security and privacy regulations, such as GDPR or HIPAA, emphasize the Principle of Least Privilege as a best practice. Adhering to POLP helps organizations meet compliance requirements.</p> </li> </ol> <p>Examples of POLP in Go:</p> <ul> <li>Implementing the POLP <p>Within a software system it involves managing user roles, permissions, and access controls.</p> </li> </ul> <pre><code>type User struct {\n    ID       int\n    Username string\n    // Additional user properties\n}\n\ntype Role struct {\n    ID          int\n    Name        string\n    Permissions []string\n    // Additional role properties\n}\n\ntype UserRepository struct {\n    // Database or storage for user data\n    users []User\n}\n\nfunc (ur *UserRepository) GetByID(userID int) (User, error) {\n    // Retrieve user from the repository\n    // Implement the necessary logic to fetch the user by ID\n    // Return the user and an error if not found\n}\n\ntype AuthorizationService struct {\n    userRepository *UserRepository\n    // Additional dependencies\n}\n\nfunc (as *AuthorizationService) HasPermission(userID int, permission string) bool {\n    // Check if the user with the given ID has the specified permission\n    user, err := as.userRepository.GetByID(userID)\n    if err != nil {\n        // Handle error\n        return false\n    }\n\n    // Retrieve user's roles and check for the permission\n    for _, role := range user.Roles {\n        if as.hasPermissionInRole(role, permission) {\n            return true\n        }\n    }\n\n    return false\n}\n\nfunc (as *AuthorizationService) hasPermissionInRole(role Role, permission string) bool {\n    // Check if the role has the specified permission\n    for _, perm := range role.Permissions {\n        if perm == permission {\n            return true\n        }\n    }\n    return false\n}\n</code></pre> <p>In the example, we have a <code>User</code> struct representing a user with an ID, username, and potentially other properties. We also have a <code>Role</code> struct representing a role with an ID, name, and a list of permissions associated with that role.</p> <p>The <code>UserRepository</code> struct represents the storage or database for user data. In the <code>AuthorizationService</code>, we have a <code>HasPermission</code> method that takes a user ID and a permission string and checks if the user has the specified permission. It does so by retrieving the user from the repository, iterating over the user's roles, and checking if any of the roles have the desired permission.</p> <p>This example showcases how the Principle of Least Privilege can be implemented by associating roles with specific permissions and checking those permissions when needed. The code focuses on granting only the necessary privileges to perform specific actions and preventing unauthorized access to sensitive operations or resources.</p> <p>NOTE The actual implementation of access controls and permissions may vary depending on the specific requirements of your application and the underlying authentication and authorization mechanisms used.</p>"},{"location":"articles/software-design-principles/#1114-inversion-of-control","title":"1.1.14. Inversion of Control","text":"<p>Inversion of Control (IoC) is a software design principle that promotes the inversion of the traditional flow of control in a program. Instead of the developer being responsible for managing the flow and dependencies of components, IoC shifts the control to a framework or container that manages the lifecycle and dependencies of components. This allows for more flexible, decoupled, and reusable code.</p> <p>The IoC principle is often implemented using a technique called Dependency Injection (DI), where the dependencies of a component are injected or provided from an external source rather than being created or managed by the component itself.</p> <p>Benefits of IoC:</p> <ol> <li> <p>Decoupling of Components</p> <p>With IoC, components are decoupled from their dependencies, allowing for easier maintenance, testing, and reusability. Components only depend on abstractions or interfaces, rather than concrete implementations.</p> </li> <li> <p>Inversion of Control Containers</p> <p>IoC containers are used to manage the lifecycle and dependencies of components. They create, configure, and inject the necessary dependencies into the components, relieving developers from explicitly managing these dependencies.</p> </li> <li> <p>Dependency Injection</p> <p>Dependency injection is a popular implementation technique for IoC. Dependencies are injected into a component either through constructor injection, method injection, or property injection. This enables loose coupling, as components only need to know about their dependencies through interfaces or abstractions.</p> </li> <li> <p>Testability</p> <p>IoC facilitates unit testing by allowing components to be easily replaced with mock or stub implementations of their dependencies. This isolation enables more focused and reliable testing of individual components.</p> </li> <li> <p>Flexibility and Extensibility</p> <p>IoC makes it easier to modify or extend the behavior of a system by simply configuring or replacing components within the container. This promotes a modular and pluggable architecture, where components can be added or modified without impacting the entire system.</p> </li> </ol> <p>Examples of IoC in Go:</p> <ul> <li>IoC using Dependency Injection (DI)</li> </ul> <pre><code>package main\n\nimport (\n\"fmt\"\n\"log\"\n)\n\n// Logger interface defines the log method\ntype Logger interface {\nLog(message string)\n}\n\n// ConsoleLogger is an implementation of the Logger interface\ntype ConsoleLogger struct{}\n\n// Log prints the message to the console\nfunc (c ConsoleLogger) Log(message string) {\nfmt.Println(message)\n}\n\n// OrderProcessor represents a component that processes orders\ntype OrderProcessor struct {\nLogger Logger\n}\n\n// ProcessOrder processes an order and logs a message\nfunc (o OrderProcessor) ProcessOrder() {\n// Order processing logic\no.Logger.Log(\"Order processed successfully.\")\n}\n\nfunc main() {\n// Create an instance of the ConsoleLogger\nlogger := ConsoleLogger{}\n\n// Create an instance of the OrderProcessor with the logger injected\norderProcessor := OrderProcessor{Logger: logger}\n\n// Process the order\norderProcessor.ProcessOrder()\n}\n</code></pre> <p>In the example, we have an <code>Logger</code> interface that defines a <code>Log</code> method, and a <code>ConsoleLogger</code> struct that implements the <code>Logger</code> interface.</p> <p>The <code>OrderProcessor</code> struct has a dependency on the <code>Logger</code> interface, which is injected into its <code>Logger</code> field. The <code>ProcessOrder</code> method of <code>OrderProcessor</code> uses the logger to log a message during order processing.</p> <p>In the <code>main</code> function, an instance of <code>ConsoleLogger</code> is created and assigned to the <code>Logger</code> field of <code>OrderProcessor</code> during initialization. This demonstrates the concept of dependency injection, where the control over the creation and management of the logger is inverted to the calling code.</p> <p>By using dependency injection and IoC, the <code>OrderProcessor</code> is decoupled from the specific logger implementation (<code>ConsoleLogger</code>). This allows for easier testing, flexibility in swapping out different logger implementations, and better separation of concerns in the codebase.</p>"},{"location":"articles/software-design-principles/#1115-law-of-demeter","title":"1.1.15. Law of Demeter","text":"<p>The Law of Demeter or the Principle of Least Knowledge, is a design guideline that promotes loose coupling and information hiding between objects. It states that an object should only communicate with its immediate dependencies and should not have knowledge of the internal details of other objects. The Law of Demeter helps to reduce the complexity and dependencies in a system, making the code more maintainable and less prone to errors.</p> <p>The main idea behind the Law of Demeter can be summarized as \"only talk to your friends, not to strangers.\" In other words, an object should only interact with its own members, its parameters, objects it creates, or objects it holds as instance variables. It should avoid accessing the properties or methods of objects that are obtained through intermediate objects.</p> <p>Benefits of LoD:</p> <ol> <li> <p>Loose Coupling</p> <p>The objects in your system become less dependent on each other, which makes it easier to modify and replace individual components without affecting the entire system.</p> </li> <li> <p>Modularity</p> <p>The code becomes more modular, with each object encapsulating its own behavior and having limited knowledge of other objects. This improves the organization and maintainability of the codebase.</p> </li> <li> <p>Code Readability</p> <p>By limiting the interactions between objects, the code becomes more readable and easier to understand. It reduces the cognitive load and makes it easier to reason about the behavior of individual objects.</p> </li> <li> <p>Testing</p> <p>Objects with limited dependencies are easier to test in isolation, as you can mock or stub the necessary dependencies without having to traverse a complex object graph.</p> </li> </ol> <p>Adherence of LoD:</p> <ul> <li> <p>Avoid chaining method calls on objects to access nested properties or invoke methods of other objects.</p> </li> <li> <p>Use parameters to communicate with other objects, rather than directly accessing their properties or methods.</p> </li> <li> <p>Limit the exposure of object internals by providing only necessary interfaces and methods to interact with the object.</p> </li> <li> <p>Delegate complex operations to specialized objects or services, rather than having an object orchestrate the entire process.</p> </li> </ul> <p>Examples of LoD in C++:</p> <ol> <li> <p>Tight Coupling</p> <p>Violation of LoD:</p> <p>Suppose we have a <code>Customer</code> class that has a method for placing an order:</p> <pre><code>class Customer {\npublic:\n  void placeOrder(Item item) {\n    Inventory inventory;\n    inventory.update(item); // access to neighbor object\n    PaymentGateway gateway;\n    gateway.processPayment(); // access to neighbor object\n    // other order processing logic\n  }\n};\n</code></pre> <p>In the example, the <code>Customer</code> class has direct knowledge of two other classes, <code>Inventory</code> and <code>PaymentGateway</code>, and is tightly coupled to them. This violates the LoD, as the <code>Customer</code> class should only communicate with a limited number of related objects.</p> <p>Adherence of LoD:</p> <p>A better approach would be to modify the <code>placeOrder</code> method to only interact with objects that are directly related to the <code>Customer</code> class, like this:</p> <pre><code>class Customer {\npublic:\n  void placeOrder(Item item, Inventory&amp; inventory, PaymentGateway&amp; gateway) {\n    inventory.update(item);\n    gateway.processPayment();\n    // other order processing logic\n  }\n};\n</code></pre> <p>In this revised example, the <code>Customer</code> class only communicates with two objects that are passed in as parameters, and does not have direct knowledge of them. This reduces the coupling between objects and promotes loose coupling, which can improve maintainability, flexibility, and modularity.</p> <p>Overall, the LoD is a useful guideline for promoting good design practices and reducing coupling between objects. By limiting the interactions between objects, the LoD can help improve the design of a system and make it easier to maintain and modify.</p> </li> </ol>"},{"location":"articles/software-design-principles/#1116-law-of-conservation-of-complexity","title":"1.1.16. Law of Conservation of Complexity","text":"<p>The Law of Conservation of Complexity is a principle in software development that states that the complexity of a system is inherent and cannot be eliminated but can only be shifted or redistributed. It suggests that complexity cannot be completely eliminated from a system; it can only be moved from one part to another.</p> <p>In other words, the Law of Conservation of Complexity recognizes that complexity is an inherent attribute of software systems, and efforts to simplify one aspect of the system often result in increased complexity in another aspect.</p> <p>NOTE The Law of Conservation of Complexity does not mean that complexity should be embraced without question. Instead, it highlights the need for thoughtful consideration of complexity trade-offs and effective management of complexity throughout the development process. The Law of Conservation of Complexity provides a high-level understanding of complexity and its redistribution within a software system, guiding developers to make informed decisions to manage complexity effectively.</p> <p>Features of Law of Conservation of Complexity:</p> <ol> <li> <p>Complexity Redistribution</p> <p>When you simplify or reduce complexity in one part of a system, it often leads to an increase in complexity in another part. For example, introducing abstractions or design patterns to simplify one component may require additional layers of code or configuration, increasing the complexity of the system.</p> </li> <li> <p>Trade-offs</p> <p>Simplifying one aspect of a system may require making trade-offs or accepting increased complexity in other areas. It's important to consider the impact of complexity redistribution and make informed decisions based on the specific needs and requirements of the system.</p> </li> <li> <p>Managing Complexity</p> <p>Instead of aiming to eliminate complexity, the focus should be on effectively managing and controlling complexity. This involves identifying critical areas where complexity is necessary and keeping other areas as simple as possible.</p> </li> <li> <p>System Understanding</p> <p>Understanding the underlying complexity of a system is crucial for making informed decisions. It helps in identifying areas where complexity is essential and where it can be minimized.</p> </li> <li> <p>Documentation and Communication</p> <p>Clear documentation and effective communication are vital for managing complexity. Documenting design decisions, system dependencies, and other relevant information helps in understanding and maintaining the complexity of the system.</p> </li> </ol> <p>Examples of Law of Conservation of Complexity in C#:</p> <ul> <li>Conceptual idea of Complexity Redistribution</li> </ul> <p>Let's consider a simple example where we have a system that performs some calculations. Initially, we have a straightforward implementation that calculates the sum of two numbers:</p> <pre><code>public class Calculator\n{\n    public int Add(int a, int b)\n    {\n        return a + b;\n    }\n}\n</code></pre> <p>In the example, the code is simple and has low complexity. However, as the requirements evolve, we may need to introduce additional features, such as support for logging and error handling. This can lead to complexity redistribution.</p> <pre><code>public class Calculator\n{\n    private ILogger logger;\n\n    public Calculator(ILogger logger)\n    {\n        this.logger = logger;\n    }\n\n    public int Add(int a, int b)\n    {\n        try\n        {\n            int sum = a + b;\n            logger.Log(\"Calculation successful.\");\n            return sum;\n        }\n        catch (Exception ex)\n        {\n            logger.Log(\"Error occurred: \" + ex.Message);\n            throw;\n        }\n    }\n}\n</code></pre> <p>In the modified version, we introduced a logger dependency and added error handling logic. While the original calculation logic remains relatively simple, we have increased complexity by introducing logging and error handling capabilities. We redistributed the complexity from the calculation logic to the error handling and logging aspects of the system.</p> <p>This example demonstrates how complexity can be redistributed within a system as new requirements or features are introduced. It emphasizes the need to manage and control complexity by making conscious decisions about where complexity is essential and where it can be minimized.</p>"},{"location":"articles/software-design-principles/#1117-law-of-simplicity","title":"1.1.17. Law of Simplicity","text":"<p>The Law of Simplicity is a principle in software development that advocates for simplicity as a key factor in designing and building software systems. It suggests that simple solutions are often more effective, efficient, and easier to understand and maintain than complex ones.</p> <p>The Law of Simplicity highlights the importance of simplicity in software development. It emphasizes the benefits of simplicity in terms of understanding, maintainability, performance, and user experience, guiding developers to prioritize simplicity in their design and implementation decisions.</p> <p>NOTE Simplicity should not be pursued at the expense of essential functionality or necessary complexity. The goal is to find the right balance between simplicity and meeting the requirements of the system.</p> <p>Benefits of Law of Simplicity:</p> <ol> <li> <p>Minimalism</p> <p>The Law of Simplicity promotes minimalism in design and implementation. It encourages developers to eliminate unnecessary complexity, code, and features, focusing on delivering the essential functionality.</p> </li> <li> <p>Ease of Understanding</p> <p>Simple code and design are easier to understand, even for developers who are not familiar with the system. By minimizing complexity, the intent and behavior of the code become more apparent, reducing the cognitive load on developers.</p> </li> <li> <p>Improved Maintainability</p> <p>Simple code is easier to maintain and troubleshoot. When the codebase is straightforward, it is simpler to identify and fix bugs, make changes, and add new features. It reduces the chances of introducing unintended side effects or breaking existing functionality.</p> </li> <li> <p>Enhanced Testability</p> <p>Simple code is more testable. By isolating and decoupling components, it becomes easier to write unit tests that cover specific functionalities. Simple code allows for targeted testing, leading to more reliable and efficient test suites.</p> </li> <li> <p>Increased Performance</p> <p>Simple designs often result in more efficient and performant systems. By minimizing unnecessary complexity and overhead, the system can focus on delivering the required functionality without unnecessary bottlenecks or resource usage.</p> </li> <li> <p>User Experience</p> <p>Simple and intuitive user interfaces provide a better user experience. By focusing on essential features and streamlining user interactions, the system becomes more user-friendly and easier to navigate.</p> </li> </ol> <p>Examples of Law of Simplicity in C#:</p> <ul> <li>Illustration of Law of Simplicity</li> </ul> <p>Bad Example:</p> <pre><code>public class Customer\n{\n    public string Name { get; set; }\n    public string Address { get; set; }\n    public string PhoneNumber { get; set; }\n\n    public string GetFormattedCustomerInfo()\n    {\n        // Complex logic to format customer information with additional validations and transformations\n        // ...\n        return \"Formatted customer info\";\n    }\n}\n</code></pre> <p>In the example, the <code>Customer</code> class has properties for the name, address, and phone number, along with a method <code>GetFormattedCustomerInfo</code> that performs complex logic to format the customer information. The implementation mixes concerns by combining data storage with formatting logic, violating the principle of simplicity.</p> <p>Good Example:</p> <pre><code>public class Customer\n{\n    public string Name { get; set; }\n    public string Address { get; set; }\n    public string PhoneNumber { get; set; }\n}\n\npublic class CustomerFormatter\n{\n    public string FormatCustomerInfo(Customer customer)\n    {\n        // Simple logic to format customer information\n        // ...\n        return \"Formatted customer info\";\n    }\n}\n</code></pre> <p>In the improved implementation, we separate concerns by having a <code>Customer</code> class that only represents the customer data without any formatting logic. We introduce a separate <code>CustomerFormatter</code> class responsible for formatting customer information. This adheres to the principle of simplicity by keeping each class focused on a single responsibility.</p> <p>By splitting the responsibilities, we achieve several benefits like Separation of Concerns, Improved Testability and Clearer Intent and Simplicity.</p>"},{"location":"articles/software-design-principles/#1118-law-of-readability","title":"1.1.18. Law of Readability","text":"<p>The Law of Readability is a principle in software development that emphasizes the importance of writing code that is easy to read, understand, and maintain. It states that code should be written with the primary audience in mind, which is typically other developers who will read, modify, and extend the codebase.</p> <p>By adhering to the Law of Readability, the code is easier to comprehend, modify, and maintain. Other developers can quickly understand the purpose and flow of the code without needing extensive comments or struggling with unclear or overly complex code constructs.</p> <p>Remember, readability is subjective to some extent, and it's important to consider the conventions and best practices of the programming language and development team. The goal is to prioritize code clarity and understandability to foster effective collaboration and long-term maintainability.</p> <p>NOTE It's important to prioritize readability over writing code solely for machine optimization. While performance is important, readable code enables better collaboration, reduces bugs, and allows for easier maintenance and extensibility.</p> <p>Benefits of Law of Readability:</p> <ol> <li> <p>Clear and Expressive Code</p> <p>Readable code is written in a clear and expressive manner. It uses meaningful names for variables, functions, and classes, making it easier to understand the purpose and functionality of each component.</p> </li> <li> <p>Consistent Formatting and Style</p> <p>Consistent formatting and style conventions contribute to readability. Following a standardized coding style, such as indentation, spacing, and naming conventions, helps maintain a cohesive and uniform codebase.</p> </li> <li> <p>Modularity and Organization</p> <p>Well-organized code is easier to read and navigate. Breaking down complex logic into smaller, self-contained functions or modules improves readability by allowing developers to focus on specific parts of the codebase without being overwhelmed by unnecessary details.</p> </li> <li> <p>Proper Use of Comments and Documentation</p> <p>Adding clear and concise comments and documentation helps in understanding the code's intention and behavior. It provides context, explains complex sections, and documents any assumptions or edge cases.</p> </li> <li> <p>Avoidance of Clever Code Tricks</p> <p>Readable code favors clarity over cleverness. It avoids unnecessarily complex or convoluted solutions that may confuse other developers. Simple, straightforward code is often easier to understand and maintain in the long run.</p> </li> <li> <p>Self-Documenting Code</p> <p>Readable code reduces the need for excessive comments by using meaningful names, intuitive function signatures, and self-explanatory code structures. The code itself serves as documentation, making it easier for developers to grasp the purpose and flow of the code.</p> </li> </ol> <p>Examples of Law of Readability in Go:</p> <ul> <li>Readability</li> </ul> <p>Bad Example:</p> <pre><code>func CalculateTotal(items []Item) float64 {\n    t := 0.0\n    for _, i := range items {\n        if i.Quantity &gt; 0 {\n            p := i.Price * float64(i.Quantity)\n            if i.Quantity &gt; 10 {\n                p *= 0.9\n            }\n            t += p\n        }\n    }\n    return t\n}\n</code></pre> <p>In the above example, the <code>CalculateTotal</code> function calculates the total price of a list of items. However, the code lacks readability due to several factors:</p> <ul> <li> <p>Poor variable naming     &gt; The variable names <code>t</code>, <code>i</code>, and <code>p</code> are not descriptive, making it difficult to understand their purpose.</p> </li> <li> <p>Lack of modularity     &gt; The logic for calculating the total price, including the quantity-based discount, is nested within the loop, making the code harder to follow.</p> </li> <li> <p>Absence of whitespace and indentation     &gt; Proper indentation and spacing can significantly enhance code readability, but they are missing in this implementation.</p> </li> </ul> <p>Good Example:</p> <pre><code>func CalculateTotal(items []Item) float64 {\n    var totalPrice float64\n\n    for _, item := range items {\n        if item.Quantity &gt; 0 {\n            itemPrice := item.Price * float64(item.Quantity)\n\n            if item.Quantity &gt; 10 {\n                itemPrice *= 0.9 // 10% discount for bulk orders\n            }\n\n            totalPrice += itemPrice\n        }\n    }\n\n    return totalPrice\n}\n</code></pre> <p>In the improved implementation, the code is structured and named in a way that enhances readability:</p> <ul> <li> <p>Descriptive variable naming     &gt; The variable names <code>totalPrice</code>, <code>item</code>, and <code>itemPrice</code> clearly indicate their purpose, making the code self-explanatory.</p> </li> <li> <p>Modularity     &gt; The logic for calculating the total price is extracted into a separate variable, <code>itemPrice</code>, improving code organization and reducing nested complexity.</p> </li> <li> <p>Consistent indentation and whitespace     &gt; Proper indentation and spacing are used, making the code visually clearer and easier to follow.</p> </li> </ul>"},{"location":"articles/software-design-principles/#1119-law-of-clarity","title":"1.1.19. Law of Clarity","text":"<p>The Law of Clarity is a principle in software development that emphasizes the importance of writing code that is clear, straightforward, and easy to understand. It states that code should be written with the intention of being easily comprehensible to other developers, both present and future.</p> <p>By following the Law of Clarity, the code becomes easier to read, understand, and maintain. The use of clear and descriptive names, separation of responsibilities, and proper error handling contribute to code that is more self-explanatory and less prone to misunderstandings. Other developers can quickly grasp the intent and logic of the code, leading to improved collaboration and maintainability.</p> <p>Benefits of Law of Clarity:</p> <ol> <li> <p>Clear and Expressive Naming</p> <p>Clarity starts with using meaningful and descriptive names for variables, functions, classes, and other code elements. Clear naming helps other developers quickly understand the purpose and functionality of each component.</p> </li> <li> <p>Simplified and Self-Documenting Code</p> <p>Clarity is achieved by writing code that is self-explanatory and minimizes the need for excessive comments or documentation. The code itself should be expressive enough to convey its intent, making it easier for others to understand and maintain.</p> </li> <li> <p>Consistent and Intuitive Structure</p> <p>Clarity is enhanced by maintaining a consistent structure throughout the codebase. Following established patterns and conventions makes it easier for developers to navigate and understand the code, reducing cognitive load.</p> </li> <li> <p>Avoidance of Ambiguity and Complexity</p> <p>Clarity requires avoiding overly complex or convoluted code constructs. It's important to keep the code simple, straightforward, and free from unnecessary complexity that can confuse other developers.</p> </li> <li> <p>Clear Documentation and Comments</p> <p>While self-explanatory code is desirable, there are cases where additional documentation or comments may be necessary. When used, clear and concise documentation should provide relevant context, explanations, and details that aid in understanding the code's functionality.</p> </li> <li> <p>Prioritization of Readability over Optimization</p> <p>Clarity emphasizes writing code that is readable and understandable, even if it means sacrificing some optimizations. While performance is important, it should not come at the expense of code clarity and maintainability.</p> </li> </ol> <p>Examples of Law of Clarity in Go:</p> <ul> <li>Clarity</li> </ul> <p>Bad Example:</p> <pre><code>func processOrder(order *Order) error {\n    if order == nil {\n        return errors.New(\"Order cannot be nil\")\n    }\n\n    if len(order.Items) == 0 {\n        return errors.New(\"Order must contain at least one item\")\n    }\n\n    totalPrice := 0.0\n    for _, item := range order.Items {\n        totalPrice += item.Price * float64(item.Quantity)\n    }\n\n    order.TotalPrice = totalPrice\n\n    // Logic to save the order to a database or perform other necessary operations\n\n    return nil\n}\n</code></pre> <p>In the example, the code lacks clarity due to the following reasons:</p> <ul> <li> <p>Lack of meaningful variable names     &gt; The variable names like <code>order</code>, <code>totalPrice</code>, and <code>item</code> are not descriptive enough to convey their purpose.</p> </li> <li> <p>Mixing of responsibilities     &gt; The <code>processOrder</code> function handles multiple responsibilities, including order validation, total price calculation, and saving the order. This lack of separation makes the code harder to understand and maintain.</p> </li> </ul> <p>Good Example:</p> <pre><code>func ProcessOrder(order *Order) error {\n    if order == nil {\n        return errors.New(\"Order cannot be nil\")\n    }\n\n    if len(order.Items) == 0 {\n        return errors.New(\"Order must contain at least one item\")\n    }\n\n    calculateTotalPrice(order)\n\n    saveOrder(order)\n\n    return nil\n}\n\nfunc calculateTotalPrice(order *Order) {\n    totalPrice := 0.0\n    for _, item := range order.Items {\n        totalPrice += item.Price * float64(item.Quantity)\n    }\n    order.TotalPrice = totalPrice\n}\n\nfunc saveOrder(order *Order) {\n    // Logic to save the order to a database or perform other necessary operations\n}\n</code></pre> <p>In the improved implementation, the code exhibits clarity through the following improvements:</p> <ul> <li> <p>Clear function names     &gt; The functions <code>ProcessOrder</code>, <code>calculateTotalPrice</code>, and <code>saveOrder</code> have clear and descriptive names that reflect their purpose and functionality.</p> </li> <li> <p>Separation of responsibilities     &gt; The code separates different responsibilities into separate functions. The <code>ProcessOrder</code> function focuses on coordinating the order processing, while the <code>calculateTotalPrice</code> and <code>saveOrder</code> functions handle specific tasks.</p> </li> <li> <p>Error handling     &gt; The code returns meaningful error messages when encountering invalid or unexpected scenarios, improving the clarity of error handling.</p> </li> </ul>"},{"location":"articles/software-design-principles/#12-coding-principles","title":"1.2. Coding Principles","text":"<p>Coding principles are a set of guidelines that deal with the implementation details of a software application, including the structure, syntax, and organization of code.  By following these coding principles, software developers can create high-quality code that is easy to maintain, scalable, and efficient. These principles help to reduce complexity and make the code more flexible, reusable, and efficient.</p>"},{"location":"articles/software-design-principles/#121-kiss","title":"1.2.1. KISS","text":"<p>The Keep It Simple and Stupid (KISS) principle emphasizes simplicity and clarity in software development. It encourages developers to favor simple, straightforward solutions over complex and convoluted ones. The KISS principle aims to reduce unnecessary complexity, improve readability, and enhance maintainability of the codebase.</p> <p>NOTE While the KISS principle advocates for simplicity, it is important to strike a balance. It does not mean sacrificing necessary complexity or disregarding design considerations. The aim is to simplify where possible without compromising functionality, performance, or scalability.</p> <p>Benefits of KISS:</p> <ol> <li> <p>Simplicity</p> <p>KISS suggests avoiding unnecessary complexities, excessive abstractions, and over-engineering. By adopting simpler solutions, the code becomes easier to understand, debug, and modify.</p> </li> <li> <p>Clarity</p> <p>Simple code is more readable and understandable. It is easier for other developers to comprehend and follow the logic. The KISS principle encourages using clear and intuitive naming conventions, avoiding overly clever or cryptic code constructs, and minimizing code duplication.</p> </li> <li> <p>Minimalism</p> </li> </ol> <p>Strive for minimalism by removing anything that is not essential. Simplify interfaces, eliminate redundant features, and reduce clutter to create a cleaner and more efficient design.</p> <ol> <li> <p>Maintainability</p> <p>Simple code is easier to maintain and troubleshoot. When the codebase is straightforward, it is simpler to identify and fix bugs, make changes, and add new features. It reduces the chances of introducing unintended side effects or breaking existing functionality.</p> </li> <li> <p>Cognitive Load</p> <p>Complex code can be mentally taxing for developers to comprehend. By adhering to the KISS principle, the cognitive load on developers is reduced, allowing them to focus on the core functionality and make informed decisions.</p> </li> </ol> <p>Examples of KISS in Go:</p> <ol> <li> <p>Simplicity</p> <p>Bad Example:</p> <pre><code>func calculateFormula(a float64, b float64, c float64) float64 {\n    // ...\n    return result\n}\n</code></pre> <p>In the bad example, the function declaration unnecessarily specifies the parameter types separately.</p> <p>Good Example:</p> <pre><code>func calculateFormula(a, b, c float64) float64 {\n    // ...\n    return result\n}\n</code></pre> <p>The good example simplifies the code by using a shorter, more concise syntax.</p> </li> <li> <p>Clarity</p> <p>Bad Example:</p> <pre><code>func sqr(x float64) float64 {\n    return x * x\n}\n</code></pre> <p>In the bad example, the function name <code>sqr</code> is unclear and may require additional mental effort to understand its purpose.</p> <p>Good Example:</p> <pre><code>func square(x float64) float64 {\n    return x * x\n}\n</code></pre> <p>The good example renames the function to <code>square</code>, which provides a clear and intuitive understanding of what the function does.</p> </li> <li> <p>Minimalism</p> <p>Bad Example:</p> <pre><code>func processComplexData(data []string) {\n    // ...\n}\n</code></pre> <p>In the bad example, the function <code>processComplexData</code> implies that it handles complex data processing, but the actual operations performed are not clear.</p> <p>Good Example:</p> <pre><code>func processData(data []string) {\n    // ...\n}\n</code></pre> <p>The good example focuses on the essential data processing tasks, making the function more concise and purpose-driven.</p> </li> <li> <p>Maintainability</p> <p>Bad Example:</p> <pre><code>func getUserData(userID int) (string, string, string) {\n    // Fetch user data from database\n    // ...\n    return name, email, address\n}\n</code></pre> <p>In the bad example, the function <code>getUserData</code> returns multiple values as separate strings, which can be inconvenient to work with and prone to errors.</p> <p>Good Example:</p> <pre><code>type User struct {\n    Name    string\n    Email   string\n    Address string\n}\n\nfunc getUserData(userID int) (*User, error) {\n    // Fetch user data from database\n    // ...\n    return user, nil\n}\n</code></pre> <p>The good example introduces a <code>User</code> struct to encapsulate user data, making it more user-centered and providing a cleaner interface to access user information and more maintainable.</p> </li> <li> <p>Cognitive Load</p> <p>Bad Example:</p> <pre><code>// Bad Example: Complex code with nested conditionals and convoluted logic\nfunc processUserData(userData map[string]interface{}) {\n    if userData != nil {\n        if value, ok := userData[\"name\"]; ok {\n            if name, ok := value.(string); ok {\n                if len(name) &gt; 0 {\n                    fmt.Println(\"User name:\", name)\n                } else {\n                    fmt.Println(\"Invalid name\")\n                }\n            } else {\n                fmt.Println(\"Invalid data type for name\")\n            }\n        } else {\n            fmt.Println(\"Missing name field\")\n        }\n    } else {\n        fmt.Println(\"Empty user data\")\n    }\n}\n</code></pre> <p>In the bad example, the code is nested with multiple conditionals, making it difficult to follow and understand the logic. This increases the cognitive load on developers.</p> <p>Good Example:</p> <pre><code>// Good Example: Simplified code with early returns and clear conditions\nfunc processUserData(userData map[string]interface{}) {\n    if userData == nil {\n        fmt.Println(\"Empty user data\")\n        return\n    }\n\n    name, ok := userData[\"name\"].(string)\n    if !ok {\n        fmt.Println(\"Invalid data type for name\")\n        return\n    }\n\n    if len(name) == 0 {\n        fmt.Println(\"Invalid name\")\n        return\n    }\n\n    fmt.Println(\"User name:\", name)\n}\n</code></pre> <p>In contrast, the good example simplifies the code by using early returns and clear conditions. Each condition is checked separately, reducing the cognitive load and allowing developers to focus on the core functionality.</p> </li> </ol>"},{"location":"articles/software-design-principles/#122-dry","title":"1.2.2. DRY","text":"<p>DRY (Don't Repeat Yourself) is a coding principle that promotes the avoidance of duplicating code in software development. The principle emphasizes that code duplication can lead to various issues, such as maintenance difficulties, inconsistency, and bugs, and should be avoided whenever possible.</p> <p>The DRY principle suggests that every piece of knowledge or logic in a system should have a single, unambiguous, and authoritative representation within the codebase. This means that when a piece of functionality or a piece of information needs to be modified or updated, it should be done in a single place, and the changes should propagate throughout the system.</p> <p>DRY principle help in reducing code duplication, improving code organization and maintainability, and reducing the likelihood of bugs caused by inconsistencies in the code.</p> <p>Types of DRY:</p> <ol> <li> <p>DRY Code</p> <p>Don't Repeat Code focuses on avoiding the repetition of the same code in multiple places in the program. Instead, try to encapsulate the common code into reusable functions, classes, or modules. This makes it easier to maintain and update the code because changes only need to be made in one place.</p> </li> <li> <p>DRY Knowledge</p> <p>Don't Repeat Knowledge focuses on avoiding the duplication of information or knowledge in different parts of the program. This includes avoiding hard-coding constants, configuration settings, or other data that may change over time. Instead, use variables or configuration files to store this information in one place.</p> </li> <li> <p>DRY Process</p> <p>Don't Repeat Process focuses on avoiding the duplication of steps or processes in the program. This includes avoiding redundant validation or error-handling logic, as well as avoiding unnecessary complexity or repetition in the program's workflow. Instead, try to streamline the processes and workflows to make them as simple and efficient as possible.</p> </li> </ol> <p>Examples of DRY in Go:</p> <ol> <li> <p>DRY Code - Duplicated Code</p> <p>Without DRY:</p> <pre><code>// Repeated code\nfunc calculateAreaOfSquare(side float64) float64 {\n    return side * side\n}\n\nfunc calculateAreaOfRectangle(length float64, width float64) float64 {\n    return length * width\n}\n</code></pre> <p>In the example, there are two separate functions that calculate the area of a geometric shape, but they are essentially doing the same thing. This violates the <code>Don't Repeat Code</code> principle because the same logic is being duplicated in two separate functions.</p> <p>With DRY:</p> <pre><code>// Reusable function\nfunc calculateArea(shape Shape) float64 {\n    return shape.Area()\n}\n\ntype Shape interface {\n    Area() float64\n}\n\ntype Square struct {\n    Side float64\n}\n\nfunc (s Square) Area() float64 {\n    return s.Side * s.Side\n}\n\ntype Rectangle struct {\n    Length float64\n    Width  float64\n}\n\nfunc (r Rectangle) Area() float64 {\n    return r.Length * r.Width\n}\n</code></pre> <p>In the example, a single <code>calculateArea</code> function is used to calculate the area of various shapes, including squares and rectangles. This is a good example of DRY because the <code>calculateArea</code> function is reusable and can be used with different shapes. The <code>Shape</code> interface defines a common <code>Area()</code> method, which allows the <code>calculateArea</code> function to work with any shape that implements the interface.</p> </li> <li> <p>DRY Knowledge - Redundant Variables</p> <p>Without DRY:</p> <pre><code>// Hard-coded value\nfunc getMaximumAllowedFileSize() int64 {\n    return 1048576 // 1 MB\n}\n</code></pre> <p>In the example, the maximum allowed file size is hard-coded into the function. This violates the <code>Don't Repeat Knowledge</code> principle because the value is duplicated in the code and could potentially change in the future.</p> <p>With DRY:</p> <pre><code>// Using configuration file\nfunc getMaximumAllowedFileSize() int64 {\n    config, err := LoadConfig(\"config.toml\")\n    if err != nil {\n        return 0\n    }\n    return config.Application.MaximumFileSize\n}\n\ntype Config struct {\n    Application struct {\n        MaximumFileSize int64 `toml:\"maximum_file_size\"`\n    } `toml:\"application\"`\n}\n</code></pre> <p>In the example, the maximum allowed file size is read from a configuration file. This is a good example of DRY because the value is only specified in one place (the configuration file) and can be easily changed if necessary. The <code>Config</code> struct defines the structure of the configuration file and uses the <code>toml</code> tag to specify the name of the field in the file.</p> </li> <li> <p>DRY Process - Repeated Logic</p> <p>Without DRY:</p> <pre><code>// Repetitive error handling\nfunc doSomething(arg1 string, arg2 int) error {\n    if err := validateArg1(arg1); err != nil {\n        return err\n    }\n\n    if err := validateArg2(arg2); err != nil {\n        return err\n    }\n\n    if err := performTask(arg1, arg2); err != nil {\n        return err\n    }\n\n    return nil\n}\n\nfunc validateArg1(arg1 string) error {\n    // validation logic\n    return nil\n}\n\nfunc validateArg2(arg2 int) error {\n    // validation logic\n    return nil\n}\n\nfunc performTask(arg1 string, arg2 int) error {\n    // task logic\n    return nil\n}\n</code></pre> <p>In the example, there are multiple validation functions that are called before performing a task. Each validation function returns an error if the argument is invalid, and the errors are checked in each function call. This violates the <code>Don't Repeat Process</code> principle because the same validation logic is repeated in multiple places.</p> <p>With DRY:</p> <pre><code>// Single error handling function\nfunc doSomething(arg1 string, arg2 int) error {\n    err := validateAndPerformTask(arg1, arg2)\n    if err != nil {\n        return err\n    }\n\n    return nil\n}\n\nfunc validateAndPerformTask(arg1 string, arg2 int) error {\n    if err := validateArg1(arg1); err != nil {\n        return err\n    }\n\n    if err := validateArg2(arg2); err != nil {\n        return err\n    }\n\n    if err := performTask(arg1, arg2); err != nil {\n        return err\n    }\n\n    return nil\n}\n\nfunc validateArg1(arg1 string) error {\n    // validation logic\n    return nil\n}\n\nfunc validateArg2(arg2 int) error {\n    // validation logic\n    return nil\n}\n\nfunc performTask(arg1 string, arg2 int) error {\n    // task logic\n    return nil\n}\n</code></pre> <p>In the example, a single function <code>validateAndPerformTask</code> is used to perform all the validations and the task. The <code>doSomething</code> function then calls this function and handles any errors returned. This code follows the <code>Don't Repeat Process</code> principle by consolidating all the steps of the process into a single function. This improves readability, reduces code duplication, and makes it easier to maintain.</p> </li> </ol>"},{"location":"articles/software-design-principles/#123-yagni","title":"1.2.3. YAGNI","text":"<p>YAGNI (You Aren't Gonna Need It) is a principle that suggest only to implement features that are necessary for the current requirements, and not add features that may be needed in the future but aren't required now.</p> <p>Applying YAGNI can help teams avoid over-engineering, reduce development time and cost, and improve software quality.</p> <p>NOTE It's important to note that YAGNI doesn't mean that potential future requirements should completely ignored. Instead, it suggests to prioritize what is needed now and keep the code flexible and adaptable to future changes.</p> <p>Types of YAGNI:</p> <ol> <li> <p>Speculative YAGNI</p> <p>Speculative YAGNI refers to adding features that are not currently needed but are expected to be needed in the future. This violates the YAGNI principle because the future requirements may not materialize, and the features may become unnecessary. By implementing only what is currently needed, teams can avoid wasting time and resources on features that may never be used.</p> </li> <li> <p>Optimistic YAGNI</p> <p>Optimistic YAGNI refers to adding features that are not currently needed, but are assumed to be necessary based on incomplete or insufficient information. Teams may assume that a feature is needed based on incomplete knowledge of the problem or the customer's requirements. By waiting until the feature is clearly needed, teams can avoid building features that are not required or that do not work as expected.</p> </li> <li> <p>Fear-Driven YAGNI</p> <p>Fear-Driven YAGNI refers to adding features that are not currently needed, but are added out of fear that they may be needed in the future. This fear can be driven by concerns about future requirements, customer needs, or competition. By focusing on delivering only what is needed today, teams can avoid building features that may never be used, and they can deliver working software faster.</p> </li> </ol> <p>Examples of YAGNI in Go:</p> <ol> <li> <p>Over-Engineering</p> <p>Without YAGNI:</p> <pre><code>// Over-Engineering\nfunc add(a, b interface{}) interface{} {\n    switch a.(type) {\n    case int:\n        switch b.(type) {\n        case int:\n            return a.(int) + b.(int)\n        case float64:\n            return float64(a.(int)) + b.(float64)\n        case string:\n            return strconv.Itoa(a.(int)) + b.(string)\n        }\n    case float64:\n        switch b.(type) {\n        case int:\n            return a.(float64) + float64(b.(int))\n        case float64:\n            return a.(float64) + b.(float64)\n        case string:\n            return strconv.FormatFloat(a.(float64), 'f', -1, 64) + b.(string)\n        }\n    case string:\n        switch b.(type) {\n        case int:\n            return a.(string) + strconv.Itoa(b.(int))\n        case float64:\n            return a.(string) + strconv.FormatFloat(b.(float64), 'f', -1, 64)\n        case string:\n            return a.(string) + b.(string)\n        }\n    }\n    return nil\n}\n</code></pre> <p>In the example, the <code>add</code> function is designed to handle multiple input types, including integers, floats, and strings. However, it's unlikely that the function will be called with anything other than integers. This code violates the YAGNI principle because it is over-engineered. The function handles many different input types, but it's unlikely that it will ever be called with anything other than integers. This adds unnecessary complexity to the function, making it harder to read and maintain.</p> <p>With YAGNI:</p> <pre><code>// Simplicity\nfunc add(a, b int) int {\n    return a + b\n}\n</code></pre> <p>In the example, the <code>add</code> function is designed to handle only integers. This code follows the YAGNI principle by keeping the function simple and focused on the specific use case. This makes the code easier to read, reduces complexity, and makes it easier to maintain. If the function needs to handle other input types in the future, it can be updated at that time.</p> </li> </ol>"},{"location":"articles/software-design-principles/#124-defensive-programming","title":"1.2.4. Defensive Programming","text":"<p>Defensive programming is a coding technique that involves anticipating and guarding against potential errors and exceptions in a program. It's a way of thinking that focuses on writing code that is more resilient and less likely to break, even when unexpected or unusual situations occur.</p> <p>Using defensive programming techniques create more robust and reliable software that is less prone to errors and exceptions.</p> <p>Types of Defensive Programming:</p> <ol> <li> <p>Input Validation</p> <p>Check and sanitize all user input to ensure that it meets expected format and range criteria. This can help prevent unexpected behavior due to invalid input.</p> </li> <li> <p>Error Handling</p> <p>Implement try-catch blocks and error handling routines to gracefully handle errors and exceptions. This can prevent unexpected crashes and provide a better user experience.</p> </li> <li> <p>Assertions</p> <p>Use assertions to test for conditions that should always be true. This can help identify bugs early in the development process and prevent them from causing problems later on.</p> </li> <li> <p>Defensive Copying</p> <p>Create copies of objects and data to ensure that they are not modified unintentionally. This can help prevent data corruption and security vulnerabilities.</p> </li> <li> <p>Logging</p> <p>Implement logging to record program events and error messages. This can help with debugging and analysis of issues that occur during runtime.</p> </li> <li> <p>Code Reviews</p> <p>Have code reviewed by other developers to catch potential issues that may have been missed. This can improve the quality of the code and reduce the likelihood of bugs.</p> <p>Code reviews are not implemented in code directly, but rather as a process. It involves having other developers review the code and provide feedback to catch potential issues that may have been missed.</p> </li> </ol> <p>Examples of Defensive Programming in Go:</p> <ol> <li> <p>Input Validation</p> <pre><code>func calculateBMI(weight float64, height float64) float64 {\n    if weight &lt;= 0 || height &lt;= 0 {\n        // Handle invalid input\n        return 0\n    }\n    // Calculate BMI\n    bmi := weight / (height * height)\n    return bmi\n}\n</code></pre> <p>In the example, we validate the weight and height input to ensure they are positive numbers before calculating the BMI.</p> </li> <li> <p>Error Handling</p> <pre><code>func readFile(filename string) ([]byte, error) {\n    data, err := ioutil.ReadFile(filename)\n    if err != nil {\n        // Handle error\n        return nil, err\n    }\n\n    return data, nil\n}\n</code></pre> <p>In the example, we use the <code>ioutil.ReadFile()</code> function to read the contents of a file, and then check for errors using the <code>err</code> variable. If an error occurs, we handle it and return an error value.</p> </li> <li> <p>Assertions</p> <pre><code>func divide(x float64, y float64) float64 {\n    assert(y != 0, \"Divisor cannot be zero\")\n    return x / y\n}\n\nfunc assert(condition bool, message string) {\n    if !condition {\n        panic(message)\n    }\n}\n</code></pre> <p>In the example, we use the <code>assert()</code> function to check if the divisor <code>y</code> is not zero. If it is, we panic and display an error message.</p> </li> <li> <p>Defensive Copying</p> <pre><code>func addToList(list []int, num int) []int {\n    // Make a copy of the list to avoid modifying the original\n    newList := make([]int, len(list))\n    copy(newList, list)\n    newList = append(newList, num)\n    return newList\n}\n</code></pre> <p>In the example, we make a copy of the <code>list</code> slice using the <code>make()</code> and <code>copy()</code> functions to avoid modifying the original <code>list</code> slice.</p> </li> <li> <p>Logging</p> <pre><code>func main() {\n    // Create a log file\n    logFile, err := os.Create(\"log.txt\")\n    if err != nil {\n        log.Fatal(\"Cannot create log file\")\n    }\n    defer logFile.Close()\n\n    // Create a logger object\n    logger := log.New(logFile, \"\", log.LstdFlags)\n\n    // Log a message\n    logger.Println(\"Program started\")\n}\n</code></pre> <p>In the example, we create a log file and use the <code>log</code> package to log a message to the file.</p> </li> <li> <p>Code Reviews</p> <pre><code>// Example code\n// TODO: Implement error handling and input validation\nfunc divide(x float64, y float64) float64 {\n    return x / y\n}\n</code></pre> <p>In the example, we use a <code>TODO</code> comment to indicate that error handling and input validation need to be implemented. A code review would help catch these issues and ensure they are addressed before the code is released.</p> </li> </ol>"},{"location":"articles/software-design-principles/#125-single-point-of-responsibility","title":"1.2.5. Single Point of Responsibility","text":"<p>Single Point of Responsibility (SPoR) is a software design principle that states that each module, class, or method in a system should have only one reason to change. In other words, a module or component should have only one responsibility or job to perform, and it should do it well.</p> <p>By limiting the responsibility of a module, class, or method, it becomes easier to maintain, test, and modify the code. This is because changes to one responsibility will not affect other responsibilities, which reduces the risk of introducing bugs or unintended behavior.</p> <p>The Single Point of Responsibility principle create code that is easier to maintain, test, and modify, which can lead to a more robust and reliable software system.</p> <p>Types of SPoR:</p> <ol> <li> <p>Separation of Concerns</p> <p>Divide the functionality of a system into separate components, each responsible for a specific task.</p> </li> <li> <p>Modular Design</p> <p>Break down complex systems into smaller, more manageable modules, each with a single responsibility. This makes it easier to test and modify individual components without affecting the rest of the system.</p> </li> <li> <p>Class Design</p> <p>Create classes with a single responsibility. This makes the code easier to understand and maintain.</p> </li> <li> <p>Method Design</p> <p>Create methods that do only one thing and do it well. This makes the code more reusable and easier to test.</p> </li> </ol> <p>Examples of SPoR in Go:</p> <ol> <li> <p>Separation of Concerns</p> <p>In the example, the user interface code is separated from the business logic code.</p> <pre><code>// UI package responsible for handling user interface\npackage ui\n\nfunc renderUI() {\n    // code for rendering the user interface\n}\n</code></pre> <pre><code>// Business package responsible for handling business logic\npackage business\n\nfunc performCalculations() {\n    // code for performing calculations\n}\n</code></pre> </li> <li> <p>Modular Design</p> <p>In the example, a package is responsible for file input/output and another package is responsible to performs calculations.</p> <pre><code>// Package responsible for handling file input/output\npackage fileio\n\nfunc readFile(filename string) ([]byte, error) {\n    // code for reading a file\n}\n\nfunc writeFile(filename string, data []byte) error {\n    // code for writing data to a file\n}\n</code></pre> <pre><code>// Package responsible for handling calculations\npackage calculations\n\nfunc performCalculations(data []byte) {\n    // code for performing calculations on data\n}\n</code></pre> </li> <li> <p>Class Design</p> <pre><code>// FileIO class responsible for handling file input/output\ntype FileIO struct {\n    // fields\n}\n\nfunc (f *FileIO) ReadFile(filename string) ([]byte, error) {\n    // code for reading a file\n}\n\nfunc (f *FileIO) WriteFile(filename string, data []byte) error {\n    // code for writing data to a file\n}\n\n// Calculation class responsible for performing calculations\ntype Calculation struct {\n    // fields\n}\n\nfunc (c *Calculation) PerformCalculations(data []byte) {\n    // code for performing calculations on data\n}\n</code></pre> </li> <li> <p>Method Design</p> <pre><code>// Calculation class responsible for performing calculations\ntype Calculation struct {\n    // fields\n}\n\nfunc (c *Calculation) Add(a, b int) int {\n    return a + b\n}\n\nfunc (c *Calculation) Subtract(a, b int) int {\n    return a - b\n}\n\nfunc (c *Calculation) Multiply(a, b int) int {\n    return a * b\n}\n\nfunc (c *Calculation) Divide(a, b int) (int, error) {\n    if b == 0 {\n        return 0, errors.New(\"division by zero\")\n    }\n    return a / b, nil\n}\n</code></pre> </li> </ol>"},{"location":"articles/software-design-principles/#126-design-by-contract","title":"1.2.6. Design by Contract","text":"<p>Design by Contract (DbC) is a software design principle that focuses on defining a contract between software components or modules. The contract defines the expected behavior of the component or module, including its inputs, outputs, and any error conditions. DbC is a programming paradigm that helps to ensure the correctness of code by defining and enforcing a set of preconditions, postconditions, and invariants.</p> <p>By defining contracts for each module or component, the software system can be designed and tested in a modular fashion. Each module can be tested independently of the others, which reduces the risk of introducing bugs or unintended behavior. The Design by Contract principle create more reliable and robust software systems by clearly defining the behavior of each module or component and enforcing that behavior through contracts.</p> <p>Types of DbC:</p> <ol> <li> <p>Preconditions</p> <p>Preconditions specify the conditions that must be satisfied before a function is called. They define the valid inputs and state of the system.</p> </li> <li> <p>Postconditions</p> <p>Postconditions specify the conditions that must be satisfied after a function is called. They define the expected outputs and state of the system.</p> </li> <li> <p>Invariants</p> <p>Invariants specify the conditions that must always be true during the execution of a program. They define the rules that the system must follow to ensure correctness.</p> </li> </ol> <p>Examples of DbC in Kotlin:</p> <ol> <li> <p>Preconditions</p> <pre><code>fun divide(a: Int, b: Int): Int {\n    require(b != 0) { \"The divisor must not be zero\" }\n    return a / b\n}\n</code></pre> <p>In the example, the <code>require</code> function checks that the divisor is not zero before the function is executed. If the divisor is zero, an exception is thrown with a specified error message.</p> </li> <li> <p>Postconditions</p> <pre><code>fun divide(a: Int, b: Int): Int {\n    val result = a / b\n    require(result * b == a) { \"The result must satisfy a * b == a\" }\n    return result\n}\n</code></pre> <p>In the example, the <code>require</code> function checks that the result satisfies the postcondition, which is that <code>result * b == a</code>. If the result does not satisfy the postcondition, an exception is thrown with a specified error message.</p> </li> <li> <p>Invariants</p> <pre><code>class Stack&lt;T&gt; {\n    private val items = mutableListOf&lt;T&gt;()\n\n    fun push(item: T) {\n        items.add(item)\n        assert(items.size &gt; 0) { \"The stack must not be empty\" }\n    }\n\n    fun pop(): T {\n        assert(items.size &gt; 0) { \"The stack must not be empty\" }\n        return items.removeAt(items.size - 1)\n    }\n\n    fun size() = items.size\n}\n</code></pre> <p>In the example, the <code>assert</code> function is used to check that the stack is not empty before a <code>pop</code> operation is executed, and after a <code>push</code> operation is executed. If the stack is empty, an exception is thrown with a specified error message.</p> </li> </ol>"},{"location":"articles/software-design-principles/#127-command-query-separation","title":"1.2.7. Command-Query Separation","text":"<p>Command-Query Separation (CQS) is a design principle that separates methods into two categories: commands that modify the state of the system and queries that return a result without modifying the state of the system. The principle was first introduced by Bertrand Meyer, the creator of the Eiffel programming language.</p> <p>In CQS, a method is either a command or a query, but not both. Commands modify the state of the system and have a void return type, while queries return a result and do not modify the state of the system. This separation can help make the code easier to understand, maintain, and test.</p> <p>The Command-Query Separation principle make code easier to understand and maintain by clearly separating methods that modify the state of the system from those that do not. This can also make it easier to test the code since commands and queries can be tested separately.</p> <p>Examples of CQS in JavaScript:</p> <ol> <li> <p>Separating a method into a command and a query:</p> <pre><code>class ShoppingCart {\n  constructor() {\n    this.items = [];\n  }\n\n  // Command that modifies the state of the system\n  addItem(item) {\n    this.items.push(item);\n  }\n\n  // Query that returns a result without modifying the state of the system\n  getItemCount() {\n    return this.items.length;\n  }\n}\n</code></pre> </li> <li> <p>Using different method names to indicate whether it is a command or a query:</p> <pre><code>class UserService {\n  constructor() {\n    this.users = [];\n  }\n\n  // Command that modifies the state of the system\n  createUser(user) {\n    this.users.push(user);\n  }\n\n  // Query that returns a result without modifying the state of the system\n  getUserById(id) {\n    return this.users.find(user =&gt; user.id === id);\n  }\n}\n</code></pre> </li> </ol>"},{"location":"articles/software-design-principles/#13-process-principles","title":"1.3. Process Principles","text":"<p>Process principles deal with the software development process and provide guidelines for managing the software development life cycle.</p> <p>Process principles refer to a set of guidelines that govern how software is developed, tested, and deployed. By following these process principles, software development teams can improve the efficiency and effectiveness of their development processes, while also improving the quality and reliability of the software they produce. These principles help to reduce waste, increase collaboration, and deliver value to customers.</p>"},{"location":"articles/software-design-principles/#131-waterfall-model","title":"1.3.1. Waterfall Model","text":"<p>The Waterfall Model is a sequential software development process model that follows a linear and phased approach. It consists of distinct, well-defined phases, each of which must be completed before progressing to the next phase.</p> <p>In the Waterfall Model, each phase must be completed before moving on to the next, and there is little room for iteration or changes once a phase is finished. The model assumes that the requirements are well-defined and stable, and any changes or updates are handled through a formal change control process.</p> <p>NOTE The Waterfall Model has some limitations. It can be inflexible in accommodating changes, and any errors or misunderstandings in the earlier phases can have significant consequences later on. Additionally, it may not be suitable for complex or large-scale projects where requirements are subject to frequent changes.</p> <p>Roles of Waterfall:</p> <ol> <li> <p>Project Manager</p> <p>The Project Manager is responsible for overall project planning, coordination, and execution. They define project milestones, allocate resources, and ensure that the project progresses according to the defined schedule and requirements.</p> </li> <li> <p>Business Analyst</p> <p>The Business Analyst gathers and analyzes requirements from stakeholders, translates them into documentation, and ensures that the requirements are accurately captured and communicated to the development team.</p> </li> <li> <p>Development Team</p> <p>The Development Team consists of individuals responsible for implementing and coding the software based on the predefined requirements. They follow a sequential approach, completing one phase before moving on to the next.</p> </li> <li> <p>Quality Assurance (QA) Team</p> <p>The QA Team is responsible for testing and validating the software. They ensure that the developed software meets the specified requirements and adhere to quality standards.</p> </li> <li> <p>Technical Writers</p> <p>Technical Writers create documentation, user manuals, and other instructional materials to support the software developed in the Waterfall Model.</p> </li> </ol> <p>Features of Waterfall:</p> <ol> <li>Requirements Analysis</li> </ol> <p>In this phase, the project requirements are gathered, analyzed, and documented. This includes identifying user needs, defining system features, and creating a detailed requirements specification.</p> <ol> <li>System Design</li> </ol> <p>In this phase, the system architecture and high-level design are developed. It involves defining the structure of the system, subsystems, modules, and their relationships. Design decisions related to hardware, software, network, and user interface are made.</p> <ol> <li>Implementation</li> </ol> <p>The implementation phase involves translating the system design into actual code. Developers write and integrate the code according to the design specifications. It includes coding, unit testing, and debugging.</p> <ol> <li>Testing</li> </ol> <p>Once the implementation is complete, the system is tested to ensure that it functions correctly and meets the specified requirements. Different types of testing, such as unit testing, integration testing, system testing, and acceptance testing, are conducted.</p> <ol> <li>Deployment</li> </ol> <p>After successful testing, the system is deployed or released to the end-users. This phase involves installation, configuration, and training.</p> <ol> <li>Maintenance</li> </ol> <p>The maintenance phase focuses on the ongoing support and maintenance of the system. It includes fixing bugs, addressing user issues, and making updates or enhancements as needed.</p> <p>Benefits of Waterfall:</p> <ol> <li>Clear Project Scope</li> </ol> <p>The Waterfall Model requires a comprehensive analysis and documentation of project requirements upfront. This helps in defining the scope of the project early on and minimizing scope creep. With a well-defined scope, the project team and stakeholders have a clear understanding of what needs to be delivered, reducing the chances of misunderstandings and change requests during development.</p> <ol> <li>Resource Allocation</li> </ol> <p>The Waterfall Model allows for better resource allocation and planning. Since each phase has distinct deliverables and requirements, resources can be allocated based on the specific needs of each phase. This helps in optimizing resource utilization and ensuring that the right people with the necessary skills are assigned to the appropriate tasks.</p> <ol> <li>Predictability</li> </ol> <p>The Waterfall Model follows a linear and predetermined path, which makes it highly predictable in terms of timeframes and outcomes. This can be advantageous for projects with strict deadlines or fixed budgets.</p> <ol> <li>Emphasis on Documentation</li> </ol> <p>The Waterfall Model emphasizes documentation at each phase of the project. This includes detailed requirements specifications, design documents, and test plans. The comprehensive documentation ensures that the project's progress, requirements, and deliverables are well-documented, facilitating future maintenance, support, and knowledge transfer.</p> <ol> <li>Well-Suited for Stable Requirements</li> </ol> <p>The Waterfall Model is effective when the project requirements are stable and unlikely to change significantly. It works well in situations where the scope is well-defined and the client's expectations are clear.</p> <ol> <li> <p>Formality and Control</p> <p>The Waterfall Model offers a structured and controlled approach to software development. The sequential nature of the model ensures that each phase is completed before moving on to the next, providing a clear order of execution. This formality and control can be beneficial in situations where strict adherence to processes, standards, and regulations is required.</p> </li> <li> <p>Simplicity</p> </li> </ol> <p>The Waterfall Model is straightforward and easy to understand. It follows a linear progression of phases, starting from requirements gathering and ending with product deployment. This simplicity makes it easier to plan and manage projects, especially for smaller or less complex software development efforts.</p> <ol> <li>Well-Defined Milestones</li> </ol> <p>The Waterfall Model has well-defined milestones for each phase of the project. These milestones serve as checkpoints for project progress and provide clear targets for evaluation and decision-making. They help track the project's advancement and ensure that the necessary activities and deliverables are completed at each stage.</p> <ol> <li> <p>Client Engagement</p> <p>The Waterfall Model often involves a significant level of client engagement during the requirements gathering and initial planning phases. This allows clients to provide input and review the project's direction before development begins. The early involvement of clients can result in better alignment between expectations and the final product.</p> </li> </ol> <p>Example of Waterfall:</p> <ol> <li> <p>Requirements Phase</p> <ul> <li> <p>Activities</p> <p>Gathering and documenting all the software requirements from stakeholders and clients.</p> </li> <li> <p>Deliverable</p> <p>Detailed Requirement Specification Document.</p> </li> </ul> </li> <li> <p>Design Phase</p> <ul> <li> <p>Activities</p> <p>Translating the requirements into a design document that outlines the system architecture, database design, user interface layout, and other design aspects.</p> </li> <li> <p>Deliverable</p> <p>System Design Document.</p> </li> </ul> </li> <li> <p>Implementation Phase</p> <ul> <li> <p>Activities</p> <p>Coding and development of the software based on the design specifications.</p> </li> <li> <p>Deliverable</p> <p>Executable software code.</p> </li> </ul> </li> <li> <p>Testing Phase</p> <ul> <li> <p>Activities</p> <p>Performing various types of testing, including unit testing, integration testing, and system testing, to ensure that the software meets the specified requirements.</p> </li> <li> <p>Deliverable</p> <p>Test reports and defect logs.</p> </li> </ul> </li> <li> <p>Deployment Phase</p> <ul> <li> <p>Activities</p> <p>Installing and configuring the software in the production environment, preparing the system for end-user access.</p> </li> <li> <p>Deliverable</p> <p>Deployed software system.</p> </li> </ul> </li> <li> <p>Maintenance Phase</p> <ul> <li> <p>Activities</p> <p>Providing ongoing support, bug fixing, and updates to the software as needed.</p> </li> <li> <p>Deliverable</p> <p>Maintenance and support documentation, updated software versions.</p> </li> </ul> </li> </ol>"},{"location":"articles/software-design-principles/#132-v-model","title":"1.3.2. V Model","text":"<p>The V Model is a software development model that is an extension of the Waterfall Model. It emphasizes the relationship between each phase of the development life cycle and its associated testing activities.</p> <p>The V Model emphasizes the importance of testing throughout the development process. Each phase has a corresponding testing phase, and the testing activities mirror the development activities. This approach ensures that defects are identified and fixed at an early stage, reducing the cost and effort required for rework.</p> <p>One of the key advantages of the V Model is its strong emphasis on verification and validation. The testing activities are clearly defined and aligned with the corresponding development phases, ensuring that the system meets the specified requirements. It also provides a systematic and structured approach to software development, making it easier to track progress and manage project risks.</p> <p>NOTE The V Model can be inflexible in accommodating changes and may not be suitable for projects with evolving or uncertain requirements. It is best suited for projects with well-defined and stable requirements, where a systematic approach to testing is crucial.</p> <p>Roles of V:</p> <ol> <li> <p>Project Manager</p> <p>The Project Manager oversees the project's planning, execution, and delivery. They coordinate with stakeholders, allocate resources, and ensure that the project progresses according to the defined schedule and requirements.</p> </li> <li> <p>Business Analyst</p> <p>The Business Analyst gathers and documents requirements from stakeholders, ensuring that they align with the desired functionality of the software.</p> </li> <li> <p>System Architect</p> <p>The System Architect designs the system architecture based on the requirements. They define the technical structure and components of the software.</p> </li> <li> <p>Development Team</p> <p>The Development Team consists of individuals responsible for implementing the software based on the defined system architecture. They follow a sequential approach, completing one phase before moving on to the next.</p> </li> <li> <p>Testers</p> <p>Testers are responsible for creating and executing test cases to validate the software against the specified requirements. They ensure that the software meets the expected functionality and quality standards.</p> </li> <li> <p>Technical Writers</p> <p>Technical Writers create documentation, user manuals, and other instructional materials to support the software developed in the V Model.</p> </li> </ol> <p>Features of V:</p> <ol> <li> <p>Requirements Analysis</p> <p>Similar to the Waterfall Model, the V Model starts with requirements analysis, where the project requirements are gathered, analyzed, and documented.</p> </li> <li> <p>System Design</p> <p>In this phase, the system architecture and high-level design are developed, just like in the Waterfall Model.</p> </li> <li> <p>Subsystem Design</p> <p>In the V Model, the subsystem design phase follows the system design phase. It involves translating the high-level design into more detailed designs for each subsystem or component.</p> </li> <li> <p>Unit Testing</p> <p>Once the subsystem design is complete, the corresponding unit tests are created and executed. Unit testing focuses on testing individual units or components in isolation to ensure their proper functionality.</p> </li> <li> <p>Integration Testing</p> <p>After the unit testing phase, integration testing takes place. Integration testing verifies the proper integration and interaction between the subsystems or components.</p> </li> <li> <p>System Testing</p> <p>Once the integration testing is complete, system testing is performed to ensure that the entire system functions correctly and meets the specified requirements.</p> </li> <li> <p>Acceptance Testing</p> <p>After system testing, the system is handed over to the end-users or clients for acceptance testing. Acceptance testing validates that the system meets the user's requirements and is ready for deployment.</p> </li> </ol> <p>Benefits of V:</p> <ol> <li> <p>Clear Verification and Validation</p> <p>The V Model emphasizes the relationship between development and testing activities. It provides a clear framework for verifying and validating each phase of the development life cycle, ensuring that the software meets the specified requirements. This structured approach reduces the risk of overlooking critical quality assurance activities.</p> </li> <li> <p>Early Defect Detection</p> <p>By incorporating testing activities at each phase, the V Model promotes early defect detection. Unit testing, integration testing, system testing, and acceptance testing help identify and rectify issues early in the development process, reducing the cost and effort required for bug fixing.</p> </li> <li> <p>Thorough Test Coverage</p> <p>The V Model ensures comprehensive test coverage by defining specific testing activities for each phase. This approach helps address functional, integration, and system-level requirements, ensuring that the software is thoroughly tested and meets the desired quality standards.</p> </li> <li> <p>Traceability and Documentation</p> <p>The V Model encourages the creation of detailed documentation at each stage of development and testing. This documentation facilitates traceability between requirements, design, implementation, and testing artifacts. It helps stakeholders understand the progress of the project, facilitates knowledge transfer, and supports future maintenance and enhancement activities.</p> </li> <li> <p>Structured Development Process</p> <p>The V Model provides a well-defined and structured development process. It outlines the sequential execution of activities, making it easier to plan, track, and manage the project. The clear dependencies and milestones ensure a systematic and controlled approach to software development.</p> </li> <li> <p>Reduced Rework and Costs</p> <p>With its emphasis on early defect detection and comprehensive testing, the V Model helps reduce rework and associated costs. By addressing issues at the appropriate stages, it minimizes the chances of major defects slipping through to later stages, where fixing them becomes more time-consuming and expensive.</p> </li> <li> <p>Improved Stakeholder Communication</p> <p>The V Model facilitates effective communication and collaboration among project stakeholders. The structured approach and clear milestones provide a common understanding of project progress and expectations. This promotes transparency, reduces misunderstandings, and enables timely decision-making.</p> </li> <li> <p>Compliance and Auditability</p> <p>The V Model's documentation-centric approach supports compliance requirements and auditability. The well-documented artifacts and traceability enable organizations to demonstrate adherence to regulatory standards and best practices.</p> </li> </ol> <p>Example of V:</p> <p>The V Model is a software development model that emphasizes a sequential and systematic approach to project execution. It is named after the shape of the V, which represents the relationship between each phase of development and its corresponding testing phase. Here's an example of how the V Model can be applied to a software development project:</p> <ol> <li> <p>Requirements Analysis Phase</p> <ul> <li> <p>Activities</p> <p>Gathering and documenting the software requirements, including functional and non-functional specifications.</p> </li> <li> <p>Deliverable</p> <p>Software Requirements Specification (SRS) document.</p> </li> </ul> </li> <li> <p>System Design Phase</p> <ul> <li> <p>Activities</p> <p>Translating the requirements into a detailed system design, including architecture, database design, and module interfaces.</p> </li> <li> <p>Deliverable</p> <p>System Design Document.</p> </li> </ul> </li> <li> <p>Module Design Phase</p> <ul> <li> <p>Activities</p> <p>Breaking down the system design into individual modules and defining their specifications, interfaces, and interactions.</p> </li> <li> <p>Deliverable</p> <p>Module Design Documents for each module.</p> </li> </ul> </li> <li> <p>Implementation Phase</p> <ul> <li> <p>Activities</p> <p>Writing code based on the module design specifications, implementing the functionality, and performing unit testing.</p> </li> <li> <p>Deliverable</p> <p>Executable code for each module.</p> </li> </ul> </li> <li> <p>Integration Phase</p> <ul> <li> <p>Activities</p> <p>Integrating the individual modules together to build the complete system, performing integration testing to ensure proper functionality and compatibility.</p> </li> <li> <p>Deliverable</p> <p>Integrated system.</p> </li> </ul> </li> <li> <p>System Testing Phase</p> <ul> <li> <p>Activities</p> <p>Conducting thorough testing of the integrated system to verify its compliance with the requirements, including functional, performance, and security testing.</p> </li> <li> <p>Deliverable</p> <p>Test Reports and Defect Logs.</p> </li> </ul> </li> <li> <p>User Acceptance Testing (UAT) Phase</p> <ul> <li> <p>Activities</p> <p>Involving end-users or clients to test the system in a real-world environment and provide feedback on its usability and conformance to their needs.</p> </li> <li> <p>Deliverable</p> <p>UAT Test Reports.</p> </li> </ul> </li> <li> <p>Deployment Phase</p> <ul> <li> <p>Activities</p> <p>Preparing the system for deployment, including installation, configuration, and data migration.</p> </li> <li> <p>Deliverable</p> <p>Deployed and operational system.</p> </li> </ul> </li> <li> <p>Maintenance Phase</p> <ul> <li> <p>Activities</p> <p>Providing ongoing support, bug fixing, and system enhancements based on user feedback and changing requirements.</p> </li> <li> <p>Deliverable</p> <p>Maintenance and Support documentation, updated system versions.</p> </li> </ul> </li> </ol>"},{"location":"articles/software-design-principles/#133-agile","title":"1.3.3. Agile","text":"<p>The Agile methodology is an iterative and collaborative approach to software development that prioritizes flexibility, adaptability, and customer satisfaction. It emphasizes delivering working software in frequent iterations and incorporating feedback to continuously improve the product.</p> <p>By adopting Agile, organizations can increase collaboration, improve customer satisfaction, respond effectively to changes, and deliver high-quality software in a more efficient and iterative manner. Agile provides a flexible framework that allows teams to adapt to evolving requirements and deliver value to customers in a timely and incremental manner.</p> <p>Types of Agile frameworks:</p> <ol> <li> <p>Scrum</p> <p>Scrum is one of the most widely used Agile frameworks. It emphasizes iterative development, regular feedback, and continuous improvement. It uses time-boxed iterations called Sprints and includes specific roles (such as Product Owner, Scrum Master, and Development Team) and ceremonies (such as Sprint Planning, Daily Stand-up, Sprint Review, and Sprint Retrospective) to structure the development process.</p> </li> <li> <p>Kanban</p> <p>Kanban is a visual Agile framework that focuses on visualizing work, limiting work in progress, and optimizing flow. It uses a Kanban board to represent tasks and their states, allowing teams to track progress and identify bottlenecks. Kanban promotes continuous delivery and encourages the team to pull work from the backlog as capacity allows.</p> </li> <li> <p>Lean Software Development</p> <p>While not strictly an Agile framework, Lean principles heavily influence Agile methodologies. Lean Software Development emphasizes reducing waste, maximizing value, and optimizing flow. It incorporates concepts such as value stream mapping, eliminating waste, continuous improvement, and respecting people.</p> </li> <li> <p>Extreme Programming (XP)</p> <p>Extreme Programming is an Agile framework known for its engineering practices and focus on quality. It emphasizes short iterations, <code>Continuous Integration</code>, <code>Test-Driven Development (TDD)</code>, <code>Pair Programming</code>, and frequent customer interaction. XP aims to deliver high-quality software through a disciplined and collaborative development approach.</p> </li> <li> <p>Crystal</p> <p>Crystal is a family of Agile methodologies that vary in size, complexity, and team structure. Crystal methodologies focus on adapting to the specific characteristics and needs of the project. They emphasize active communication, reflection, and simplicity.</p> </li> <li> <p>Dynamic Systems Development Method (DSDM)</p> <p>DSDM is an Agile framework that places strong emphasis on the business value and maintaining a focus on the end-users. It provides a comprehensive framework for iterative and incremental development, covering areas such as requirements gathering, prototyping, timeboxing, and frequent feedback.</p> </li> <li> <p>Feature-Driven Development (FDD)</p> <p>FDD is an Agile framework that emphasizes feature-driven development and domain modeling. It involves breaking down development into small, manageable features and focuses on iterative development, regular inspections, and progress tracking.</p> </li> </ol> <p>Features of Agile:</p> <ol> <li> <p>Customer Satisfaction</p> <p>The highest priority in Agile is to satisfy the customer through continuous delivery of valuable software. Collaboration with customers and stakeholders is essential to understand their needs, gather feedback, and ensure the software meets their expectations.</p> </li> <li> <p>Embrace Change</p> <p>Agile recognizes that requirements and priorities can change throughout the project. It encourages flexibility and embraces changes, even late in the development process. Agile teams are responsive to change, accommodating new requirements and incorporating feedback to deliver a better end product.</p> </li> <li> <p>Deliver Working Software Frequently</p> <p>Agile focuses on delivering working software frequently, with short and regular iterations. This allows for early validation, gathering feedback, and incorporating changes. Continuous delivery of increments of the software ensures value is delivered to the customer consistently.</p> </li> <li> <p>Collaboration and Communication</p> <p>Agile values collaboration and communication among team members and with stakeholders. Cross-functional teams work together closely, sharing knowledge, ideas, and responsibilities. Frequent communication helps in understanding requirements, resolving issues, and ensuring a common understanding of the project goals.</p> </li> <li> <p>Self-Organizing Teams</p> <p>Agile promotes self-organizing teams that have the autonomy to make decisions and manage their own work. Team members collaborate and take collective ownership of the project, leading to increased motivation, creativity, and accountability.</p> </li> <li> <p>Sustainable Pace</p> <p>Agile recognizes the importance of maintaining a sustainable pace of work. It emphasizes the well-being and long-term productivity of team members. Avoiding overwork and burnout leads to a more productive and motivated team.</p> </li> <li> <p>Continuous Improvement</p> <p>Agile encourages a culture of learning and continuous improvement. Agile emphasizes continuous improvement through regular reflection and adaptation. Teams conduct retrospectives to review their work, identify areas for improvement, and make adjustments to enhance their processes, practices, and outcomes.</p> </li> <li> <p>Iterative and Incremental Development</p> <p>Agile promotes an iterative and incremental approach to development. Instead of trying to deliver the entire software at once, the project is divided into small iterations or sprints. Each iteration delivers a working increment of the software, allowing for continuous improvement and adaptation.</p> </li> </ol> <p>Benefits of Agile:</p> <ol> <li> <p>Flexibility and Adaptability</p> <p>Agile methodologies provide flexibility to accommodate changes and respond to evolving requirements throughout the development process. This enables teams to quickly adapt to new information, customer feedback, and market conditions, resulting in a more responsive and successful project.</p> </li> <li> <p>Faster Time-to-Market</p> <p>Agile methodologies, with their iterative and incremental approach, enable faster delivery of working software. By breaking the project into smaller iterations, teams can release functional increments of the software more frequently. This allows organizations to respond to market demands, gain a competitive edge, and deliver value to customers sooner.</p> </li> <li> <p>Improved Quality</p> <p>Agile methodologies prioritize quality throughout the development process. Practices such as continuous integration, automated testing, and frequent customer feedback help identify and address issues early on. This results in higher software quality, reduced defects, and a better user experience.</p> </li> <li> <p>Enhanced Team Collaboration</p> <p>Agile fosters collaborative teamwork and communication among team members. Cross-functional teams work closely together, sharing knowledge and responsibilities. This promotes better collaboration, creativity, and problem-solving, leading to higher productivity and team satisfaction.</p> </li> <li> <p>Transparency and Visibility</p> <p>Agile methodologies provide transparency into the development process. Through practices like daily stand-up meetings, backlog management, and visual task boards, stakeholders have visibility into the progress, priorities, and challenges. This improves communication, trust, and alignment among team members and stakeholders.</p> </li> <li> <p>Risk Mitigation</p> </li> </ol> <p>Agile methodologies promote early and frequent delivery of working software. This allows teams to identify and address risks and issues in a timely manner. By obtaining continuous feedback and validating assumptions, risks can be mitigated early, reducing the chances of costly project failures.</p>"},{"location":"articles/software-design-principles/#134-lean-software-development","title":"1.3.4. Lean Software Development","text":"<p>Lean Software Development is an iterative and incremental approach to software development that adopts the principles and practices of Lean thinking. It focuses on maximizing value, minimizing waste, and fostering continuous improvement throughout the software development process.</p> <p>By embracing Lean principles, organizations can optimize their software development processes, deliver value to customers more effectively, and foster a culture of continuous improvement and learning. Lean provides a systematic approach to streamlining workflows, reducing waste, and delivering high-quality software in a more efficient and customer-centric manner.</p> <p>Types of Lean Software Development:</p> <ol> <li> <p>Value Stream Mapping</p> <p>Value Stream Mapping (VSM) is a technique used to identify and visualize the steps involved in the software development process. It helps identify waste, bottlenecks, and opportunities for improvement. By analyzing the value stream, teams can streamline their processes and optimize the flow of work.</p> </li> <li> <p>Kanban</p> <p>Kanban is a visual management tool used to visualize and control the flow of work. It involves the use of a Kanban board, which represents different stages of work (e.g., to-do, in progress, done) as columns. Tasks are represented as cards that move across the board as they progress. Kanban promotes a pull-based system, limits work in progress, and helps teams focus on completing one task before starting the next.</p> </li> <li> <p>Continuous Flow</p> <p>Continuous Flow is an approach that emphasizes a steady and uninterrupted flow of work. It aims to eliminate bottlenecks and delays by reducing batch sizes, minimizing handoffs, and optimizing the flow of tasks. Continuous Flow helps ensure that work moves smoothly through the development process, enabling faster and more predictable delivery.</p> </li> <li> <p>Just-in-Time (JIT)</p> <p>Just-in-Time is a principle borrowed from Lean manufacturing that emphasizes delivering work or value at the right time, avoiding unnecessary inventory or overproduction. In Lean Software Development, JIT focuses on optimizing the delivery of features, enhancements, or fixes, ensuring they are delivered when they are needed by the customers or stakeholders.</p> </li> <li> <p>Kaizen (Continuous Improvement)</p> <p>Kaizen is a philosophy of continuous improvement that is integral to Lean Software Development. It encourages teams to constantly reflect on their processes, identify areas for improvement, and experiment with small changes. Kaizen promotes a culture of learning, adaptability, and incremental enhancements to optimize the software development process over time.</p> </li> <li> <p>Elimination of Waste</p> <p>Lean Software Development aims to minimize or eliminate different types of waste that do not add value to the final product. These wastes can include unnecessary features, overproduction, waiting times, defects, and unused talent. By identifying and eliminating waste, teams can optimize their processes and resources, leading to increased efficiency and value delivery.</p> </li> <li> <p>Lean Six Sigma</p> <p>Lean Six Sigma combines the Lean principles with Six Sigma methodology for process improvement. It aims to reduce defects and waste while improving process efficiency. It involves data-driven analysis, root cause identification, and process optimization to deliver high-quality software.</p> </li> <li> <p>Lean Startup</p> <p>The Lean Startup methodology applies Lean principles to startup environments, emphasizing the importance of validated learning and iterative development. It focuses on creating a minimum viable product (MVP) to gather customer feedback, measure key metrics, and make data-driven decisions to pivot or persevere.</p> </li> <li> <p>Theory of Constraints (ToC)</p> <p>The Theory of Constraints is a management philosophy that focuses on identifying and eliminating bottlenecks in the system to improve efficiency. It can be applied in software development to identify constraints or limiting factors that hinder productivity and take actions to alleviate them.</p> </li> </ol> <p>NOTE Lean Software Development is a flexible and adaptable approach, and organizations may adopt different practices or techniques based on their specific needs and context. The overarching goal is to create a lean and efficient software development process that maximizes value for the customer and minimizes waste.</p> <p>Features of Lean Software Development:</p> <ol> <li> <p>Eliminate Waste</p> <p>Identify and eliminate activities, processes, or artifacts that do not add value to the customer or the development process. This includes reducing unnecessary documentation, waiting times, rework, and inefficient practices.</p> </li> <li> <p>Amplify Learning</p> <p>Encourage a learning mindset and foster a culture of experimentation and feedback. Continuously seek customer feedback, conduct experiments, and gather data to validate assumptions and make informed decisions.</p> </li> <li> <p>Decide as Late as Possible</p> <p>Delay decisions until the last responsible moment when the most information is available. Avoid premature decisions that may be based on assumptions or incomplete understanding. Instead, gather data, validate assumptions, and make decisions when the time is right.</p> </li> <li> <p>Deliver Fast</p> <p>Strive for short lead times and frequent delivery of valuable increments. Delivering working software quickly allows for faster feedback, adaptation, and validation of assumptions. It helps identify issues early and enables faster value realization.</p> </li> <li> <p>Empower the Team</p> <p>Trust and empower the development team to make decisions and take ownership of their work. Foster a culture of self-organization, collaboration, and shared responsibility. Provide the necessary resources and support for the team to succeed.</p> </li> <li> <p>Build Quality In</p> <p>Place a strong emphasis on delivering high-quality software from the start. Ensure that quality is built into every step of the development process, including requirements gathering, design, coding, testing, and deployment. Use automated testing, continuous integration, and other quality assurance practices.</p> </li> <li> <p>Optimize the Whole</p> <p>Optimize the entire development process, rather than focusing on individual parts in isolation. Consider the end-to-end value stream, from idea to delivery, and identify opportunities to streamline and improve the flow. This includes removing bottlenecks, optimizing handoffs, and eliminating non-value-adding activities.</p> </li> <li> <p>Empathize with Customers</p> <p>Understand the needs and perspectives of customers and users. Involve them throughout the development process to gather feedback, validate assumptions, and ensure that the software meets their requirements and expectations. Use techniques like user research, user testing, and usability studies.</p> </li> <li> <p>Continuous Improvement</p> <p>Foster a culture of continuous improvement and learning. Regularly reflect on the development process, gather metrics, and identify areas for improvement. Encourage experimentation, feedback loops, and the adoption of new practices and technologies.</p> </li> </ol> <p>Benefits of Lean Software Development:</p> <ol> <li> <p>Waste Reduction</p> <p>Lean Software Development focuses on eliminating waste, such as unnecessary features, delays, and defects. By identifying and eliminating non-value-added activities, teams can streamline their processes and optimize efficiency, resulting in reduced time, effort, and resources wasted.</p> </li> <li> <p>Improved Quality</p> <p>Lean emphasizes the importance of delivering high-quality software. Through practices like continuous integration, automated testing, and frequent feedback loops, teams can detect and address defects early in the development process. This leads to improved software quality, fewer bugs, and higher customer satisfaction.</p> </li> <li> <p>Faster Time-to-Market</p> <p>By reducing waste, improving efficiency, and focusing on delivering value, Lean Software Development enables faster time-to-market. Teams can prioritize and deliver essential features quickly, gather customer feedback early, and make necessary adjustments to meet market demands more effectively.</p> </li> <li> <p>Increased Customer Satisfaction</p> <p>Lean Software Development emphasizes customer-centricity and the delivery of value. By involving customers throughout the development process, gathering feedback, and adapting to their needs, teams can ensure that the software meets customer expectations. This leads to higher customer satisfaction and loyalty.</p> </li> <li> <p>Agile and Adaptive Approach</p> <p>Lean Software Development promotes an agile and adaptive mindset. Teams are encouraged to embrace change, respond to customer feedback, and continuously improve their processes. This flexibility allows teams to be more responsive to changing requirements, market conditions, and customer needs.</p> </li> <li> <p>Collaborative Teamwork</p> <p>Lean Software Development encourages cross-functional and collaborative teamwork. It emphasizes effective communication, knowledge sharing, and empowered teams. This fosters a culture of collaboration, innovation, and continuous learning, resulting in higher team morale and productivity.</p> </li> <li> <p>Focus on Value</p> <p>Lean Software Development puts a strong emphasis on delivering value to the customer. By prioritizing features based on customer needs and eliminating unnecessary work, teams can maximize the value delivered by the software. This aligns development efforts with business goals and ensures a more impactful outcome.</p> </li> </ol> <p>Example of Lean Software Development:</p> <ol> <li> <p>Value Stream Mapping</p> <p>The team begins by mapping out the entire value stream, identifying the steps involved in developing and delivering the software. They analyze each step and look for opportunities to eliminate waste and improve efficiency.</p> </li> <li> <p>Pull System</p> <p>The team establishes a pull-based system to manage their work. They use a Kanban board to visualize their tasks and limit work in progress (WIP) to ensure a smooth flow. Each team member pulls new tasks when they have capacity, preventing overloading and bottlenecks. This helps maintain a steady and sustainable pace of work.</p> </li> <li> <p>Continuous Delivery</p> <p>The team focuses on delivering small, frequent increments of the application to gather feedback and provide value to users. They automate the build, testing, and deployment processes to enable continuous integration and continuous delivery. This allows them to quickly respond to changes, address issues, and release new features to the users.</p> </li> <li> <p>Kaizen (Continuous Improvement)</p> <p>The team embraces a culture of continuous improvement. They regularly gather feedback from users, measure key metrics, and conduct retrospectives to identify areas for improvement. They experiment with new ideas, technologies, and processes to enhance their productivity and customer satisfaction continuously.</p> </li> <li> <p>Just-in-Time (JIT)</p> <p>The team applies the JIT principle by optimizing their work to minimize waste and reduce unnecessary inventory. They prioritize the most valuable features and tasks, focusing on delivering what is needed at the right time. They avoid overproduction by not building excessive functionality that may not be immediately required by the users.</p> </li> <li> <p>Empowered and Cross-functional Teams</p> <p>The team is self-organizing and cross-functional, with members having different skills and expertise. They have the autonomy to make decisions and are empowered to solve problems collaboratively. This enables them to take ownership of their work, collaborate effectively, and deliver high-quality software.</p> </li> <li> <p>Customer Collaboration</p> </li> </ol> <p>The team actively involves the customers throughout the development process. They conduct user research, usability testing, and gather feedback to ensure that the application meets customer needs and expectations. They prioritize features based on customer feedback and work closely with them to iterate and improve the product.</p>"},{"location":"articles/software-design-principles/#135-kanban","title":"1.3.5. Kanban","text":"<p>Kanban is a Lean software development methodology that emphasizes visualizing the workflow and limiting work in progress. It is a pull-based system that focuses on continuous delivery and continuous improvement.</p> <p>The Kanban methodology provides a flexible and adaptable approach to software development that allows teams to focus on delivering value quickly while improving the process over time.</p> <p>Features of Kanban:</p> <ol> <li>Kanban Board</li> </ol> <p>A physical or digital board divided into columns representing the stages of work. Each column contains cards or sticky notes representing individual work items or tasks.</p> <ol> <li> <p>Work Items (Cards)</p> <p>Each work item or task is represented by a card or sticky note on the Kanban board. These cards typically include information such as task description, assignee, priority, and due dates.</p> </li> <li> <p>Columns</p> <p>The columns on the Kanban board represent different stages or statuses of work. Common columns include <code>To Do</code>, <code>In Progress</code>, <code>Testing</code>, and <code>Done</code>. The number of columns can vary depending on the specific workflow.</p> </li> <li> <p>WIP (Work in Progress) Limits</p> <p>WIP limits are predefined limits set for each column to control the number of work items that can be in progress at any given time. WIP limits prevent work overload, bottlenecks, and help maintain a smooth workflow.</p> </li> <li> <p>Visual Signals</p> <p>Kanban utilizes visual signals, such as color coding or icons, to provide additional information about work items. This can include indicating priority levels, identifying blockers or issues, or highlighting specific work item types.</p> </li> <li> <p>Pull System</p> <p>Kanban follows a pull-based approach, where new work items are pulled into the workflow only when there is available capacity. This helps prevent overloading the team and ensures that work items are completed before new ones are started.</p> </li> <li> <p>Continuous Improvement</p> <p>Kanban encourages continuous improvement by regularly analyzing and optimizing the workflow. Teams reflect on their processes, identify bottlenecks or inefficiencies, and make adjustments to enhance productivity and flow.</p> </li> <li> <p>Metrics and Analytics</p> <p>Kanban relies on metrics and analytics to measure and monitor the performance of the team and workflow. Key metrics may include lead time, cycle time, throughput, and work item aging, providing insights into efficiency and identifying areas for improvement.</p> </li> </ol> <p>Benefits of Kanban:</p> <ol> <li> <p>Visualize Workflow</p> <p>Kanban provides a visual representation of the workflow, allowing teams to see the status of each task or work item at a glance. This promotes transparency and shared understanding among team members, making it easier to identify bottlenecks, prioritize work, and allocate resources effectively.</p> </li> <li> <p>Improved Flow and Efficiency</p> <p>By limiting the work in progress (WIP) and managing the flow of tasks through the workflow, Kanban helps teams maintain a steady and balanced workload. This leads to improved efficiency, reduced lead times, and faster delivery of value to customers.</p> </li> <li> <p>Flexibility and Adaptability</p> <p>Kanban is highly flexible and adaptable to different types of projects and work environments. It doesn't require extensive upfront planning or a rigid project structure, making it suitable for both predictable and unpredictable work scenarios. Teams can easily adjust their processes and priorities based on changing requirements or market conditions.</p> </li> <li> <p>Continuous Improvement</p> <p>Kanban encourages a culture of continuous improvement. By regularly analyzing workflow metrics and soliciting feedback from team members, Kanban teams can identify areas for optimization and make incremental changes to their processes. This iterative approach to improvement leads to a constant evolution of the workflow and increased efficiency over time.</p> </li> <li> <p>Enhanced Collaboration and Communication</p> <p>Kanban promotes collaboration and communication among team members. The visual nature of the Kanban board fosters shared understanding, encourages conversations around work items, and facilitates coordination between team members. This leads to better coordination, reduced dependencies, and improved teamwork.</p> </li> <li> <p>Reduced Waste and Overhead</p> <p>Kanban helps teams identify and eliminate waste in their processes. By visualizing the workflow and focusing on the timely completion of tasks, teams can identify and address bottlenecks, minimize waiting times, and reduce unnecessary handoffs. This results in improved productivity and a reduction in overhead.</p> </li> <li> <p>Improved Customer Satisfaction</p> </li> </ol> <p>Kanban's focus on delivering value in a timely manner and continuous improvement ultimately leads to improved customer satisfaction. By continuously monitoring and adapting to customer needs, teams can ensure that the right features and work items are prioritized and delivered in a timely manner, increasing customer satisfaction and loyalty.</p> <p>Example of Kanban:</p> <ol> <li> <p>Visualizing the Workflow</p> <ul> <li> <p>Create a Kanban board with columns representing different stages of the workflow, such as <code>To Do</code>, <code>In Progress</code>, and <code>Done</code>.</p> </li> <li> <p>Each user story or task is represented by a card or sticky note on the board.</p> </li> </ul> </li> <li> <p>Setting Work-in-Progress (WIP) Limits</p> <ul> <li> <p>Determine the maximum number of user stories or tasks that can be in progress at any given time for each column.</p> </li> <li> <p>WIP limits prevent work overload and encourage focus on completing tasks before starting new ones.</p> </li> </ul> </li> <li> <p>Pull System</p> <ul> <li> <p>Work is pulled into the \"In Progress\" column based on team capacity and WIP limits.</p> </li> <li> <p>Only when a team member completes a task, they pull the next task from the \"To Do\" column into the \"In Progress\" column.</p> </li> </ul> </li> <li> <p>Continuous Flow</p> <ul> <li> <p>Team members work on tasks in a continuous flow, ensuring that each task is completed before starting a new one.</p> </li> <li> <p>Focus on completing and delivering tasks rather than starting new ones.</p> </li> </ul> </li> <li> <p>Visualizing Bottlenecks</p> <ul> <li> <p>By tracking the movement of tasks on the Kanban board, bottlenecks and areas of inefficiency become visible.</p> </li> <li> <p>Bottlenecks can be identified and addressed to improve the flow and productivity.</p> </li> </ul> </li> <li> <p>Continuous Improvement</p> <ul> <li> <p>Regularly review the Kanban board and the team's performance to identify areas for improvement.</p> </li> <li> <p>Collaboratively discuss and implement changes to optimize the workflow and increase efficiency.</p> </li> </ul> </li> <li> <p>Cycle Time and Lead Time Analysis</p> <ul> <li> <p>Measure the cycle time (time taken to complete a task) and lead time (time taken from request to completion) for tasks.</p> </li> <li> <p>Analyze the data to identify trends, bottlenecks, and areas for improvement in the workflow.</p> </li> </ul> </li> <li> <p>Feedback and Collaboration</p> <ul> <li> <p>Foster a culture of collaboration and feedback among team members.</p> </li> <li> <p>Encourage open communication, problem-solving, and knowledge sharing to improve the performance of the team.</p> </li> </ul> </li> <li> <p>Continuous Delivery</p> <ul> <li> <p>Aim to deliver completed tasks or user stories as soon as they are ready, rather than waiting for a specific release date.</p> </li> <li> <p>This allows for faster feedback and value delivery to the customers.</p> </li> </ul> </li> </ol>"},{"location":"articles/software-design-principles/#136-scrum","title":"1.3.6. Scrum","text":"<p>Scrum is an Agile framework for managing and delivering complex projects. It provides a flexible and iterative approach to software development that focuses on delivering value to customers through regular product increments. Scrum promotes collaboration, transparency, and adaptability, allowing teams to respond quickly to changing requirements and market dynamics.</p> <p>Scrum is widely used in various industries and has proven effective in managing complex projects and teams. It promotes a collaborative and iterative approach, empowering teams to deliver high-quality products that meet customer expectations.</p> <p>Roles of Scrum:</p> <ol> <li> <p>Scrum Master</p> <p>The Scrum Master is responsible for ensuring that the Scrum framework is understood and followed. They facilitate Scrum events, remove obstacles that hinder the team's progress, and protect the team from external disruptions. The Scrum Master promotes collaboration, self-organization, and continuous improvement within the team.</p> </li> <li> <p>Product Owner</p> <p>The Product Owner represents the stakeholders and is responsible for maximizing the value of the product. They define and prioritize the product backlog, ensuring that it reflects the needs and vision of the stakeholders. The Product Owner collaborates with the team to refine requirements, make trade-off decisions, and accept or reject work results.</p> </li> <li> <p>Development Team</p> <p>The Development Team consists of cross-functional members who collaborate to deliver a potentially shippable increment of the product at the end of each sprint. They self-organize and determine the best way to accomplish the work. The Development Team is responsible for estimating, planning, and delivering the committed work, as well as ensuring the quality of the product.</p> </li> </ol> <p>Features of Scrum:</p> <ol> <li> <p>Product Backlog</p> <p>The <code>Product Owner</code> maintains a prioritized list of product requirements, known as the <code>Product Backlog</code>. It represents all the work that needs to be done on the project and serves as the team's guide for development.</p> </li> <li> <p>Sprint Planning</p> <p>At the beginning of each <code>Sprint</code>, the Scrum team holds a <code>Sprint Planning</code> meeting. They discuss and define the <code>Sprint</code> goal, select the items from the <code>Product Backlog</code> to work on, and create a <code>Sprint Backlog</code> with the specific tasks to be completed during the <code>Sprint</code>.</p> </li> <li> <p>Sprint</p> <p>A time-boxed iteration of development, typically lasting 1-4 weeks, during which the team works to deliver a potentially shippable product increment.</p> </li> <li> <p>Daily Scrum/Stand-up</p> <p>The <code>Daily Scrum/Stand-up</code> is a short daily meeting where team members provide updates on their progress, discuss any obstacles or challenges, and coordinate their work for the day. It promotes collaboration, transparency, and alignment within the team.</p> </li> <li> <p>Sprint Review</p> <p>At the end of each Sprint, the team holds a <code>Sprint Review</code> meeting to demonstrate the completed work to stakeholders and gather feedback. The <code>Product Owner</code> reviews the <code>Product Backlog</code> and adjusts priorities based on the feedback received.</p> </li> <li> <p>Sprint Retrospective</p> <p>Following the <code>Sprint Review</code>, the team holds a <code>Sprint Retrospective</code> meeting to reflect on the <code>Sprint</code> and identify areas for improvement. They discuss what went well, what could be improved, and take actions to enhance their processes and performance in the next Sprint.</p> </li> </ol> <p>Benefits of Scrum:</p> <ol> <li> <p>Flexibility and Adaptability</p> <p>Scrum embraces change and provides a flexible framework that allows teams to respond quickly to evolving requirements, market dynamics, and customer feedback. The iterative and incremental nature of Scrum enables continuous learning and adaptation throughout the project.</p> </li> <li> <p>Increased Collaboration</p> <p>Scrum promotes collaboration and cross-functional teamwork. It encourages open communication, regular interactions, and shared accountability among team members. Collaboration within a self-organizing Scrum team leads to better problem-solving, knowledge sharing, and a sense of collective ownership of the project.</p> </li> <li> <p>Faster Time to Market</p> <p>Scrum emphasizes delivering valuable product increments at the end of each Sprint. By breaking down the work into small, manageable units and focusing on frequent releases, Scrum enables faster delivery of working software. This helps organizations seize market opportunities, gather customer feedback early, and iterate on the product accordingly.</p> </li> <li> <p>Transparency and Visibility</p> <p>Scrum provides transparency into the project's progress, work completed, and upcoming priorities. Through artifacts like the Product Backlog, Sprint Backlog, and Sprint Burndown Chart, stakeholders have clear visibility into the team's activities and can track the progress towards project goals. This transparency fosters trust, collaboration, and effective decision-making.</p> </li> <li> <p>Continuous Improvement</p> <p>Scrum encourages regular reflection and adaptation through ceremonies like the Sprint Retrospective. This dedicated time for introspection and process evaluation enables the team to identify areas for improvement, address bottlenecks, and refine their working practices. Continuous improvement becomes an integral part of the team's workflow, leading to increased productivity and quality over time.</p> </li> <li> <p>Customer Satisfaction</p> <p>Scrum places a strong emphasis on delivering value to customers. The involvement of the Product Owner in prioritizing features and incorporating customer feedback ensures that the team is building what the customers truly need. This customer-centric approach leads to higher satisfaction levels and enhances the chances of delivering a product that meets or exceeds customer expectations.</p> </li> <li> <p>Empowered and Motivated Teams</p> <p>Scrum empowers teams to make decisions, take ownership of their work, and collaborate effectively. By providing autonomy and a supportive environment, Scrum boosts team morale and motivation. Teams are more likely to be engaged, creative, and committed to delivering high-quality results.</p> </li> </ol> <p>Example of Scrum:</p> <p>Scrum is a iterative and incremental approach that allows the team to adapt to changing requirements, gather feedback regularly, and deliver working software at the end of each Sprint, ensuring a high degree of customer satisfaction and continuous improvement.</p> <ol> <li> <p>Scrum Team Formation</p> <ul> <li> <p>Identify and form a cross-functional Scrum team consisting of a Product Owner, Scrum Master, and Development Team members.</p> </li> <li> <p>Determine the team's size and composition based on project requirements and available resources.</p> </li> </ul> </li> <li> <p>Product Backlog</p> <ul> <li> <p>The Product Owner collaborates with stakeholders to gather requirements.</p> </li> <li> <p>The Product Owner creates and maintains a prioritized list of user stories and requirements called the Product Backlog.</p> </li> <li> <p>User stories represent specific features or functionalities desired by the end-users or stakeholders.</p> </li> <li> <p>The Product Backlog is continuously refined and updated throughout the project.</p> </li> </ul> </li> <li> <p>Sprint Planning</p> <ul> <li> <p>At the beginning of each Sprint, the Scrum Team, including the Product Owner and Development Team, conducts a Sprint Planning meeting.</p> </li> <li> <p>The Product Owner presents the top-priority items from the Product Backlog for the upcoming Sprint.</p> </li> <li> <p>The Development Team estimates the effort required for each item and determines which items they commit to completing during the Sprint.</p> </li> </ul> </li> <li> <p>Daily Scrum</p> <ul> <li> <p>The Development Team holds a Daily Scrum meeting, usually lasting 15 minutes, to synchronize their work.</p> </li> <li> <p>Each team member shares what they accomplished since the last meeting, what they plan to do next, and any obstacles or issues they are facing.</p> </li> <li> <p>The Daily Scrum promotes collaboration, transparency, and quick decision-making within the team.</p> </li> </ul> </li> <li> <p>Sprint</p> <ul> <li> <p>The Development Team works on the committed items during the Sprint.</p> </li> <li> <p>They collaborate, design, develop, and test the features, following best practices and coding standards.</p> </li> <li> <p>The Development Team self-organizes and manages their work to deliver the Sprint goals.</p> </li> </ul> </li> <li> <p>Sprint Review</p> <ul> <li> <p>At the end of each Sprint, the Scrum Team conducts a Sprint Review meeting.</p> </li> <li> <p>The Development Team presents the completed work to the stakeholders and receives feedback.</p> </li> <li> <p>The Product Owner reviews and updates the Product Backlog based on the feedback and new requirements that emerge.</p> </li> </ul> </li> <li> <p>Sprint Retrospective</p> <ul> <li> <p>After the Sprint Review, the Scrum Team holds a Sprint Retrospective meeting.</p> </li> <li> <p>They reflect on the previous Sprint, discussing what went well, what could be improved, and actions to enhance the team's performance.</p> </li> <li> <p>The team identifies opportunities for process improvement and defines action items to implement in the next Sprint.</p> </li> </ul> </li> <li> <p>Increment and Release</p> <ul> <li> <p>At the end of each Sprint, the Development Team delivers an increment of the product.</p> </li> <li> <p>The increment is a potentially releasable product version that incorporates the completed user stories.</p> </li> <li> <p>The Product Owner decides when to release the product, considering the stakeholders' requirements and market conditions.</p> </li> </ul> </li> <li> <p>Repeat Sprint Cycle</p> <ul> <li> <p>The Scrum Team continues with subsequent Sprints, repeating the process of Sprint Planning, Daily Scrum, Sprint Development, Sprint Review, and Sprint Retrospective.</p> </li> <li> <p>The product evolves incrementally with each Sprint, responding to changing requirements and delivering value to the users.</p> </li> </ul> </li> <li> <p>Monitoring and Observability</p> <p>Throughout the project, the Scrum Master ensures that the Scrum framework is followed, facilitates collaboration and communication, and helps the team overcome any obstacles. The Product Owner represents the interests of the stakeholders, maintains the Product Backlog, and ensures that the team is delivering value.</p> </li> </ol>"},{"location":"articles/software-design-principles/#137-extreme-programming","title":"1.3.7. Extreme Programming","text":"<p>Extreme Programming (XP) is an agile methodology that focuses on producing high-quality software through iterative and incremental development. It emphasizes collaboration, customer involvement, and continuous feedback.</p> <p>By adopting Extreme Programming, teams can deliver high-quality software through regular iterations, continuous feedback, and collaboration. XP's practices aim to improve communication, code quality, and customer satisfaction, making it a popular choice for teams seeking agility and adaptability in software development.</p> <p>NOTE Adapting Extreme Programming may vary depending on the project, team, and organization. Successful adoption of XP requires commitment, discipline, and a supportive environment that values collaboration, feedback, and continuous learning.</p> <p>Roles of XP:</p> <ol> <li> <p>Programmer</p> <p>The Programmer is responsible for writing code and implementing the software features. They collaborate with others through <code>Pair Programming</code>, where two programmers work together on the same task, actively reviewing and improving each other's code.</p> </li> <li> <p>Customer/On-Site Customer</p> <p>The Customer represents the end-users or stakeholders and provides guidance on the software's requirements and features. They work closely with the team, participating in planning, providing feedback, and making timely decisions to ensure the software meets their needs.</p> </li> <li> <p>Coach</p> <p>The Coach, also known as an XP Coach or Agile Coach, provides guidance and expertise on XP practices and principles. They help the team understand and adopt XP practices effectively, facilitate continuous improvement, and address any challenges or issues that arise during the development process.</p> </li> </ol> <p>Features of XP:</p> <ol> <li> <p>User Stories</p> <p>Small, actionable descriptions of features or requirements from a user's perspective.</p> </li> <li> <p>Pair Programming</p> <p>XP encourages <code>Pair Programming</code>, where two developers work together on the same code. This practice promotes knowledge sharing, improves code quality, and helps catch errors early.</p> </li> <li> <p>Test-Driven Development (TDD)</p> <p>TDD is a core practice in XP. Developers write automated tests before writing the code. These tests drive the development process, ensure code correctness, and act as a safety net for refactoring and future changes.</p> </li> <li> <p>Continuous Integration</p> <p>Frequently integrating code changes into a shared repository, enabling early detection of integration issues and ensuring a working software baseline. Automated builds and tests ensure that the software remains in a releasable state at all times.</p> </li> <li> <p>Refactoring</p> <p>Restructuring and improving the codebase without changing its external behavior, design, maintainability, readability and extensibility of the codebase. Refactoring is an ongoing process that eliminates code smells and improves the quality of the software.</p> </li> <li> <p>Collective Code Ownership</p> <p>All team members have equal responsibility and authority to modify any part of the codebase. There is no individual ownership of code, which ensure shared code quality, collaboration, encourages code reviews, and knowledge sharing.</p> </li> <li> <p>On-Site Customer</p> <p>Involvement of a representative customer or end user to provide real-time feedback and clarify requirements. This close collaboration ensures that the software meets the customer's expectations.</p> </li> <li> <p>Sustainable Pace</p> <p>Maintaining a balanced and sustainable workload to prevent burnout and promote long-term productivity and quality.</p> </li> <li> <p>Planning Game</p> <p>XP uses the <code>Planning Game</code> technique to involve customers and development teams in the planning process. Customers define user stories or requirements, and the team estimates the effort required for each story. Prioritization is done collaboratively, ensuring the most valuable features are developed first.</p> </li> <li> <p>Iterative and Incremental Development</p> <p>XP follows a series of short development cycles called iterations. Each iteration involves coding, testing, and delivering a working increment of the software. The software evolves through these iterations, with continuous feedback and learning.</p> </li> </ol> <p>Benefits of XP:</p> <ol> <li> <p>Improved Quality</p> <p>XP emphasizes practices such as <code>Test-Driven Development (TDD)</code>, <code>Pair Programming</code>, and <code>Continuous Integration</code>. These practices promote code quality, early defect detection, and faster bug fixing, resulting in a higher-quality product.</p> </li> <li> <p>Rapid Feedback</p> <p>XP encourages frequent customer involvement and feedback. Through practices like short iterations, <code>Continuous Integration</code>, and regular customer reviews, teams can quickly incorporate feedback, address concerns, and ensure that the delivered software meets customer expectations.</p> </li> <li> <p>Flexibility and Adaptability</p> <p>XP embraces changing requirements and encourages teams to respond to changes quickly. The iterative nature of XP allows for regular reprioritization of features and adaptation to evolving customer needs and market conditions.</p> </li> <li> <p>Collaborative Environment</p> <p>XP promotes collaboration and effective communication among team members. Practices like <code>Pair Programming</code> and on-site customer involvement facilitate knowledge sharing, collective code ownership, and cross-functional collaboration, leading to a cohesive and high-performing team.</p> </li> <li> <p>Increased Productivity</p> <p>XP focuses on eliminating waste and optimizing the development process. Practices like small releases, <code>Continuous Integration</code>, and automation reduce unnecessary overhead, streamline development activities, and improve productivity.</p> </li> <li> <p>Reduced Risk</p> <p>The iterative and incremental approach of XP helps manage risks effectively. By delivering working software at regular intervals, teams can identify potential issues earlier and make necessary adjustments. Frequent customer involvement and feedback also minimize the risk of building the wrong product.</p> </li> <li> <p>Customer Satisfaction</p> <p>XP places a strong emphasis on customer collaboration and satisfaction. By involving customers in the development process, addressing their feedback, and delivering value early and frequently, XP helps ensure that the final product aligns with customer needs and provides a high level of customer satisfaction.</p> </li> <li> <p>Continuous Improvement</p> <p>XP promotes a culture of continuous improvement. Regular retrospectives allow teams to reflect on their processes, identify areas for improvement, and implement changes to enhance productivity, quality, and team dynamics.</p> </li> </ol> <p>Example of XP:</p> <ol> <li>User Stories and Planning:</li> </ol> <p>The development team and stakeholders collaborate to identify user stories and define their acceptance criteria. Conduct release planning to determine which user stories will be included in each iteration.</p> <ol> <li> <p>Small Releases and Iterations</p> <p>The team focuses on delivering working software in small, frequent releases. Each release contains a set of user stories that are implemented, tested, and ready for deployment.</p> </li> <li> <p>Pair Programming</p> <p>Developers work in pairs, with one person actively coding (the driver) and the other observing and providing feedback (the navigator). They switch roles frequently to share knowledge and maintain code quality.</p> </li> <li> <p>Test-Driven Development (TDD)</p> <p>Developers practice TDD by writing automated tests before writing the corresponding code. Then, they write the code to make the test pass, iteratively refining and expanding the code while maintaining a suite of automated tests.</p> </li> <li> <p>Continuous Integration</p> <p>The team sets up a CI server that automatically builds and tests the application whenever changes are committed to the source code repository. This ensures that the codebase is always in a working state and catches integration issues early. The CI server runs the automated tests, providing immediate feedback to the team.</p> </li> <li> <p>Continuous Refactoring</p> <p>As the project progresses, the team continuously refactors the codebase to improve its design, maintainability, and performance. They identify areas of the code that could be enhanced, and without changing the external behavior. They refactor the code to eliminate duplication, improve readability, and enhance maintainability.</p> </li> <li> <p>Continuous Delivery</p> <p>Aim to deliver working software at the end of each iteration or even more frequently. Deploy the software to a staging environment for further testing and feedback.</p> </li> <li> <p>On-site Customer</p> <p>The team maintains regular communication and collaboration with a representative from the customer side. The customer provides feedback on the delivered features, suggests improvements, and prioritizes the upcoming work. They might conduct weekly meetings to review progress, discuss requirements, and adjust priorities.</p> </li> <li> <p>Continuous Improvement</p> </li> </ol> <p>The team holds regular retrospectives, where they reflect on the previous iteration, discuss what went well and what could be improved, and identify actionable items for the next iteration. They focus on enhancing their processes, teamwork, and technical practices.</p> <ol> <li> <p>Sustainable Pace</p> <p>The team maintains a sustainable and healthy working pace, avoiding long overtime hours or burnout. They focus on maintaining a consistent and productive work rhythm.</p> </li> </ol>"},{"location":"articles/software-design-principles/#138-feature-driven-development","title":"1.3.8. Feature-Driven Development","text":"<p>Feature-Driven Development (FDD) is an iterative and incremental software development methodology that focuses on delivering features in a timely and organized manner. It provides a structured approach to software development by breaking down the development process into specific, manageable features.</p> <p>Each feature is developed incrementally, following the feature-centric approach of FDD. The development team collaborates, completes each feature within a time-boxed iteration, and delivers it for testing and review.</p> <p>Feature-Driven Development promotes an organized and feature-centric approach to software development, enabling teams to deliver valuable features in a timely manner while maintaining a focus on quality and collaboration.</p> <p>Roles of FDD:</p> <ol> <li> <p>Chief Architect</p> <p>The Chief Architect is responsible for the technical direction and design of the system. They define the high-level architecture, ensure that it aligns with the project's goals, and provide guidance to the development teams.</p> </li> <li> <p>Feature Owner</p> <p>The Feature Owner is responsible for a specific feature or set of features. They work with the Chief Architect to create a detailed design for the assigned feature(s), coordinate with the development teams, and ensure the timely and successful delivery of the features.</p> </li> <li> <p>Development Manager</p> <p>The Development Manager oversees the development process, manages the resources, and ensures that the project progresses smoothly. They coordinate with the Chief Architect, Feature Owners, and development teams to monitor progress, identify and resolve issues, and maintain the project schedule.</p> </li> <li> <p>Chief Programmer</p> <p>The Chief Programmer is responsible for the technical integrity of the codebase. They ensure that the coding standards are followed, provide technical guidance to the development teams, and oversee the integration and testing of features.</p> </li> <li> <p>Feature Teams</p> <p>Feature Teams are responsible for implementing individual features. They collaborate with the Chief Architect, Feature Owners, and Chief Programmer to design, build, and test the assigned features.</p> </li> </ol> <p>Features of FDD:</p> <ol> <li> <p>Domain Object Modeling</p> <p>A high-level representation of the system's structure and entities, forming the basis for feature development. The development team collaborates with domain experts and stakeholders to create an object model that forms the basis for feature development.</p> </li> <li> <p>Feature List</p> <p>FDD utilizes a feature-centric approach. A comprehensive list of features required for the system, prioritized based on business value. Each feature is identified, described, and prioritized based on its importance and value to the users and stakeholders.</p> </li> <li> <p>Feature Design</p> <p>Once the feature list is established, the team focuses on designing individual features. Design sessions are conducted to determine the technical approach, user interfaces, and interactions required to implement each feature. The design work is typically done collaboratively, involving developers, designers, and other relevant stakeholders.</p> </li> <li> <p>Feature Iterations</p> <p>Iterative development cycles focused on delivering specific features. Iteration includes analysis, design, coding, and testing activities specific to the feature being implemented.</p> </li> <li> <p>Inspections</p> <p>Formal code and design reviews conducted to ensure adherence to standards and identify potential issues. Inspections are conducted at various stages of development, including design inspections, code inspections, and feature inspections.</p> </li> <li> <p>Reporting</p> <p>Regular reporting on the progress of feature development, allowing stakeholders to track the overall status of the system. FDD emphasizes accurate and transparent reporting to provide visibility into the project's status and progress. The team generates regular reports that highlight feature completion, project metrics, and any outstanding issues. These reports facilitate effective communication with stakeholders and support informed decision-making.</p> </li> <li> <p>Refactoring</p> <p>FDD recognizes the need for continuous improvement and refactoring. The development team performs iterative refactoring to improve the design, code quality, and maintainability of the software.</p> </li> <li> <p>Release</p> <p>FDD promotes regular releases to deliver value to users and stakeholders. As features are completed, they are integrated, tested, and released in incremental versions. This allows for frequent user feedback and ensures that working software is delivered at regular intervals.</p> </li> </ol> <p>Benefits of FDD:</p> <ol> <li> <p>Emphasizes Business Value</p> <p>FDD focuses on delivering business value by prioritizing features based on their importance to stakeholders and end users. This approach ensures that the most critical and valuable features are developed first, maximizing the return on investment.</p> </li> <li> <p>Clear Feature Ownership</p> <p>FDD promotes clear feature ownership, where each feature is assigned to a specific developer or development team. This ownership fosters accountability and encourages developers to take responsibility for the end-to-end delivery of their assigned features.</p> </li> <li> <p>Iterative and Incremental Development</p> <p>FDD follows an iterative and incremental development approach, allowing for the delivery of working software at regular intervals. This approach provides early and frequent feedback, enabling stakeholders to validate the software's functionality and make necessary adjustments throughout the development process.</p> </li> <li> <p>Effective Planning and Prioritization</p> <p>FDD incorporates a detailed planning and prioritization process. The feature breakdown and task estimation allow for better planning and resource allocation, ensuring that the development efforts are focused on delivering the most important features within the available time and resources.</p> </li> <li> <p>Scalability and Flexibility</p> <p>FDD is well-suited for large-scale development projects. The clear feature breakdown and ownership facilitate parallel development by enabling multiple teams to work on different features concurrently. This scalability and flexibility help manage complex projects more efficiently.</p> </li> <li> <p>Quality Focus</p> <p>FDD places a strong emphasis on quality throughout the development process. The verification phase ensures thorough testing of each feature, promoting the delivery of high-quality software. The focus on individual feature development also allows for easier bug tracking and isolation.</p> </li> <li> <p>Collaboration and Communication</p> <p>FDD fosters collaboration and effective communication among team members and stakeholders. The emphasis on feature breakdown, planning, and ownership promotes regular interactions and knowledge sharing, leading to better coordination and alignment across the team.</p> </li> <li> <p>Continuous Improvement</p> <p>FDD encourages a continuous improvement mindset. The iterative nature of development, combined with feedback loops, retrospectives, and lessons learned, allows teams to identify areas for improvement and make necessary adjustments in subsequent iterations.</p> </li> <li> <p>Predictability and Transparency</p> <p>FDD provides a structured and transparent approach to software development. The clear feature breakdown, progress tracking, and regular deliverables enhance predictability, allowing stakeholders to have a clear view of project status, timelines, and expected outcomes.</p> </li> </ol> <p>Example of FDD:</p> <p>NOTE FDD is a flexible methodology, and the specific implementation may vary depending on the project and team dynamics. The key principles of FDD, such as domain object modeling, feature-driven development, and regular inspections, help ensure a systematic and efficient development process that delivers high-quality software.</p> <ol> <li> <p>Develop Model</p> <p>Identify the key features or functionalities required for the software. Create a high-level domain object model that represents the major entities and their relationships within the software system. This model serves as a visual representation of the system's structure and functionality.</p> </li> <li> <p>Build Feature List</p> <p>The team collaborates with stakeholders to identify the key features required for the software system. Each feature is described in terms of its scope, acceptance criteria, and estimated effort. The features are then prioritized and added to the feature list.</p> </li> <li> <p>Regular Progress Reporting</p> </li> </ol> <p>Hold regular progress meetings or stand-ups to update the team on the status of feature development. Each team member shares their progress, any challenges or issues faced, and plans for the upcoming work.</p> <ol> <li> <p>Plan by Feature</p> <ul> <li> <p>Break down features into tasks</p> <p>For each feature, define the specific tasks required for its implementation.</p> </li> <li> <p>Estimate task effort</p> <p>Assign effort estimates to each task, considering factors like complexity and dependencies.</p> </li> <li> <p>Schedule and allocate resources</p> <p>Plan the development timeline and assign tasks to developers based on their expertise and availability.</p> </li> </ul> </li> <li> <p>Design by Feature</p> <ul> <li> <p>Detail the design specifications</p> <p>Create detailed design specifications for each feature, defining the required classes, interfaces, and data structures.</p> </li> <li> <p>Collaborate on design</p> <p>Foster collaboration among developers to ensure a cohesive and consistent design across features.</p> </li> <li> <p>Review and refine the designs</p> <p>Conduct design reviews and make necessary refinements to ensure the designs align with the system architecture.</p> </li> </ul> </li> <li> <p>Build by Feature</p> <ul> <li> <p>Implement features iteratively</p> <p>Developers start working on the features in parallel, focusing on one feature at a time. They follow coding standards and best practices to write clean and maintainable code.</p> </li> <li> <p>Regular integration and testing</p> <p>As each feature is completed, it is integrated into the main codebase and undergoes testing to ensure its functionality.</p> </li> </ul> </li> <li> <p>Verify by Feature</p> <ul> <li> <p>Conduct feature-specific testing</p> <p>Perform thorough testing of each feature to identify and address any defects or issues. This includes unit testing, integration testing, and functional testing.</p> </li> <li> <p>Validate against requirements</p> <p>Verify that each feature meets the specified requirements and functions as intended.</p> </li> </ul> </li> <li> <p>Inspect and Adapt</p> <p>Review the implemented feature to identify any issues or areas for improvement. Make necessary adjustments, refactor the code if needed, and ensure the feature is of high quality.</p> </li> <li> <p>Integrate Features</p> <ul> <li> <p>Regular integration and testing</p> <p>Continuously integrate and test the completed features to ensure their seamless integration and proper functioning as part of the larger system.</p> </li> <li> <p>Address integration issues</p> <p>Resolve any conflicts or issues that arise during the integration process.</p> </li> </ul> </li> <li> <p>Deploy by Features</p> <ul> <li> <p>Prepare for release</p> <p>Conduct a final round of testing, including user acceptance testing, to validate the functionality and usability of the system.</p> </li> <li> <p>Deploy the software</p> <p>Once the system is deemed ready, deploy it to the production environment, making it available to end-users.</p> </li> </ul> </li> <li> <p>Iterate and Enhance</p> <ul> <li> <p>Gather feedback</p> <p>Collect feedback from end-users and stakeholders to identify areas for improvement or additional features.</p> </li> <li> <p>Plan subsequent iterations</p> <p>Based on feedback and changing requirements, plan subsequent iterations to enhance the application further.</p> </li> </ul> </li> </ol>"},{"location":"articles/software-design-principles/#2-principles","title":"2. Principles","text":"<p>These principles are not mutually exclusive and often overlap with one another. A well-designed system should strive to adhere to all these principles to the best of its ability.</p> <ul> <li> <p>Understandability</p> <p>A good design should be easy to understand and maintain by other developers who may have to work on the codebase in the future.</p> </li> <li> <p>Modularity</p> <p>A good design should be modular, with each module having a clear, single responsibility. This makes the code easier to read, understand, and modify.</p> </li> <li> <p>Reusability</p> <p>A good design should be reusable, with each module being independent and able to be used in other parts of the system or in other projects.</p> </li> <li> <p>Testability</p> <p>A good design should be testable, with each module being able to be tested independently of other modules. This allows for easier debugging and reduces the risk of introducing bugs into the system.</p> </li> <li> <p>Maintainability</p> <p>A good design should be maintainable, with each module being easy to modify and extend without introducing new bugs or breaking existing functionality.</p> </li> <li> <p>Scalability</p> <p>A good design should be scalable, able to handle increasing amounts of data, traffic, or users without sacrificing performance or reliability.</p> </li> <li> <p>Extensibility</p> <p>A good design should be extensible, allowing for the addition of new features or functionality without breaking existing code.</p> </li> <li> <p>Performance</p> <p>A good design should be designed with performance in mind, using appropriate algorithms and data structures to minimize processing time and memory usage.</p> </li> <li> <p>Security</p> <p>A good design should be designed with security in mind, using appropriate security protocols and practices to protect sensitive data and prevent unauthorized access.</p> </li> <li> <p>Usability</p> <p>A good design should be usable, with the user interface being intuitive and easy to navigate, and the system being responsive and reliable.</p> </li> </ul>"},{"location":"articles/software-design-principles/#3-best-practice","title":"3. Best Practice","text":"<ul> <li> <p>Start with the user</p> <p>Always keep the user and their needs in mind when designing software. This will help to create a product that is intuitive, user-friendly, and meets the user's requirements.</p> </li> <li> <p>Use multiple principles</p> <p>No single principle can solve all  problems. Instead, try to use multiple principles in conjunction to create a software design that is flexible, maintainable, and scalable.</p> </li> <li> <p>Follow a design process</p> <p>Don't jump straight into coding. Follow a structured design process that involves identifying requirements, creating a design, and testing and iterating on that design.</p> </li> <li> <p>Emphasize simplicity</p> <p>Keep the design as simple as possible. A simple design is easier to understand, maintain, and extend than a complex one.</p> </li> <li> <p>Prioritize flexibility</p> <p>The design should be flexible enough to accommodate future changes and enhancements. This will avoid costly rework in the future.</p> </li> <li> <p>Strive for modularity</p> <p>Divide the software into smaller, more manageable modules. This will achieve greater flexibility and maintainability.</p> </li> <li> <p>Use design patterns</p> <p>Design patterns are time-tested solutions to common software design problems. Familiarize with common patterns and use them where appropriate.</p> </li> <li> <p>Continuously refine the design</p> <p>Don't consider the design to be set in stone. Continuously refine and improve it based on feedback from users and stakeholders.</p> </li> <li> <p>Document the design</p> <p>Create documentation that describes the design and how it works. This will help to understand and maintain the software over time.</p> </li> <li> <p>Test the design</p> <p>Test the software design to ensure it meets the requirements and performs as expected. This will catch issues early on and avoid costly rework down the line.</p> </li> </ul>"},{"location":"articles/software-design-principles/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Abstraction</p> <p>The process of hiding implementation details and exposing only the necessary features or functionalities.</p> </li> <li> <p>Coupling</p> <p>The degree to which one component or module of a system is dependent on another component or module.</p> </li> <li> <p>Cohesion</p> <p>The degree to which the elements within a module or component are related to each other and contribute to a single purpose or responsibility.</p> </li> <li> <p>Inheritance</p> <p>A mechanism that allows a new class to be based on an existing class, inheriting its properties and methods.</p> </li> <li> <p>Polymorphism</p> <p>The ability of an object or method to take on multiple forms or behaviors depending on the context in which it is used.</p> </li> <li> <p>Interface</p> <p>A set of methods or functions that define the expected behavior of a component or module.</p> </li> <li> <p>Dependency</p> <p>The relationship between two components or modules where one module relies on the other to perform a specific function or behavior.</p> </li> <li> <p>Encapsulation</p> <p>The practice of bundling data and methods within a single unit or class, and restricting access to the internal workings of that unit.</p> </li> <li> <p>Modularity</p> <p>The practice of dividing a system into smaller, more manageable components or modules.</p> </li> <li> <p>Design Patterns</p> <p>Reusable solutions to common software design problems that have been proven to be effective in practice. Examples include Singleton, Factory Method, and Observer.</p> </li> <li> <p>SOLID</p> <p>An acronym for a set of five principles of software design  Single Responsibility Principle, Open-Closed Principle, Liskov Substitution Principle, Interface Segregation Principle, and Dependency Inversion Principle.</p> </li> <li> <p>GRASP</p> <p>An acronym for a set of nine patterns of software design, each of which focuses on a specific aspect of responsibility assignment or object creation.</p> </li> <li> <p>YAGNI</p> <p>An acronym for <code>You Ain't Gonna Need It</code>, a principle that advocates for avoiding the inclusion of unnecessary or premature features in a system.</p> </li> <li> <p>KISS</p> <p>An acronym for <code>Keep It Simple, Stupid</code>, a principle that advocates for simplicity in design, avoiding unnecessary complexity or over-engineering.</p> </li> <li> <p>Convention over Configuration</p> <p>A practice of adopting a set of sensible defaults and conventions for a system's configuration and behavior, rather than requiring explicit configuration for every detail.</p> </li> </ul>"},{"location":"articles/software-design-principles/#5-references","title":"5. References","text":"<ul> <li>Sentenz software design patterns article.</li> </ul>"},{"location":"articles/software-development-environment/","title":"Software Development Environment","text":"<p>The Software Development Environment (SDE) refers to the overall set of tools, resources, and infrastructure that are used to develop, test, and deploy software applications. This includes hardware, software, development tools, source code management systems, testing frameworks, and production servers. The SDE is designed to support the entire software development process, from requirement gathering to deployment, and is a crucial component of software development projects. An SDE can consist of various components, such as integrated development environments (IDEs), version control systems, testing tools, and deployment platforms. SDE aims to provide a stable, secure, and efficient environment for software development and deployment, while also enabling developers to work efficiently and effectively.</p> <ul> <li>1. Category</li> <li>1.1. Development Environment</li> <li>1.2. Test Environment</li> <li>1.3. Staging Environment</li> <li>1.4. Integration Environment</li> <li>1.5. Continuous Environment</li> <li>1.6. Production Environment</li> <li>2. Best Practice</li> <li>3. References</li> </ul>"},{"location":"articles/software-development-environment/#1-category","title":"1. Category","text":""},{"location":"articles/software-development-environment/#11-development-environment","title":"1.1. Development Environment","text":"<p>This environment is used for developing, testing and debugging software applications. It provides the necessary resources for developers to write code and create new features.</p>"},{"location":"articles/software-development-environment/#12-test-environment","title":"1.2. Test Environment","text":"<p>The test environment is used to validate the software application and to identify any issues that may arise during the testing process. This environment simulates the actual production environment to provide accurate testing results.</p>"},{"location":"articles/software-development-environment/#13-staging-environment","title":"1.3. Staging Environment","text":"<p>The staging environment is a replica of the production environment, used to test and validate software applications before they are deployed to production. This environment is used to identify any issues or bugs before they impact the live production environment.</p>"},{"location":"articles/software-development-environment/#14-integration-environment","title":"1.4. Integration Environment","text":"<p>The integration environment is used to integrate different components of software applications, such as APIs, databases, and servers. This environment is used to test and validate the integration of different systems before deployment to production.</p>"},{"location":"articles/software-development-environment/#15-continuous-environment","title":"1.5. Continuous Environment","text":"<p>The continuous environment is an automated environment that supports the Continuous Pipelines, as integration and delivery of software applications. This environment is used to automate the build, test, analysis, security, and deployment process, reducing the time and effort required to bring new features and updates to production. It allows teams to quickly identify and resolve issues, delivering high-quality software applications to end-users.</p>"},{"location":"articles/software-development-environment/#16-production-environment","title":"1.6. Production Environment","text":"<p>The production environment is the live environment where software applications are deployed and made available to the end-users. This environment is monitored closely to ensure the software is functioning correctly and meeting the performance and availability requirements of the business.</p>"},{"location":"articles/software-development-environment/#2-best-practice","title":"2. Best Practice","text":"<p>Best practices, to create a robust and efficient SDE that ensures high-quality software development and deployment.</p> <ul> <li> <p>DevOps</p> <p>In DevOps methodology, the software development environment (SDE) typically involves the following stages:</p> <ul> <li>Development</li> </ul> <p>This is the stage where software developers write and test code, debug issues, and create new features. Developers use integrated development environments (IDEs) and version control systems to manage their code.</p> <ul> <li>Continuous Integration</li> </ul> <p>This stage involves the automation of build and test processes. Code changes are integrated into a single repository, and automatic tests are run to validate the changes.</p> <ul> <li>Continuous Deployment</li> </ul> <p>In this stage, code changes are automatically deployed to the testing environment, where they are tested and validated.</p> <ul> <li>Staging</li> </ul> <p>This stage involves the testing of software applications in a controlled environment that mimics the production environment. This stage helps to identify any issues or bugs before they impact the live production environment.</p> <ul> <li>Production</li> </ul> <p>This is the final stage where software applications are deployed and made available to end-users. This stage is monitored closely to ensure the software is functioning correctly and meeting performance and availability requirements.</p> <ul> <li>Continuous Monitoring</li> </ul> <p>This stage involves the continuous monitoring of the production environment to identify and resolve any issues. This stage also involves collecting and analyzing data to improve the software development process.</p> </li> <li> <p>Standardization</p> <p>The standardization of processes, tools, and coding conventions across the development team is important to ensure consistency and efficiency. This also allows developers to easily understand each other's code and collaborate more effectively.</p> </li> <li> <p>Version Control</p> <p>Version control is essential to track changes made to the codebase and allows developers to collaborate without the risk of losing work. This helps to ensure that the codebase is consistent and accurate.</p> </li> <li> <p>Logging and Monitoring</p> <p>Logging and Monitoring are important for detecting and identifying issues in the software environment. This helps to identify performance issues, system failures, and other issues that may affect the end-users.</p> </li> <li> <p>Security</p> <p>Security is critical to protect the software and data from unauthorized access or malicious attacks. This includes implementing secure coding practices, regularly reviewing and updating security protocols, and conducting penetration testing.</p> </li> </ul>"},{"location":"articles/software-development-environment/#3-references","title":"3. References","text":"<ul> <li>Codebots SDE article.</li> <li>Oroinc environments types article.</li> </ul>"},{"location":"articles/software-metric/","title":"Software Metric","text":"<p>Software metric are measurements used to assess various aspects of the software development process, project, or product. They help teams track progress, identify bottlenecks, make informed decisions, and improve overall efficiency and quality.</p> <ul> <li>1. Category</li> <li>1.1. Metrics<ul> <li>1.1.1. Primary Metrics</li> <li>1.1.2. Meta-Metrics</li> </ul> </li> <li>1.2. Standards</li> <li>1.3. Frameworks</li> <li>1.4. Tools</li> <li>2. References</li> </ul>"},{"location":"articles/software-metric/#1-category","title":"1. Category","text":""},{"location":"articles/software-metric/#11-metrics","title":"1.1. Metrics","text":""},{"location":"articles/software-metric/#111-primary-metrics","title":"1.1.1. Primary Metrics","text":"<p>Primary metrics are the core measurements used to assess specific aspects of the software development process, project, or product. These metrics directly relate to the goals and objectives of the software development effort and provide insights into the performance, progress, and quality of the development activities.</p> <p>Primary metrics provide valuable insights into the effectiveness of the software development process and its alignment with business goals. They help teams make data-driven decisions, identify areas for improvement, and track progress over time.</p> <ol> <li> <p>Cycle Time</p> <p>The time taken to complete a unit of work, like a user story or a feature, from start to finish.</p> </li> <li> <p>Lead Time</p> <p>The time taken from the initiation of work to its completion, often measured in days or weeks.</p> </li> <li> <p>Velocity</p> <p>A measure of a team's output or productivity, typically used in Agile methodologies like Scrum, calculated based on the number of story points completed in a sprint.</p> </li> <li> <p>Defect Density</p> <p>The number of defects or issues found in the software per unit of code (e.g., per 1000 lines of code).</p> </li> <li> <p>Code Churn</p> <p>The rate of code changes or additions, which can indicate the stability of the codebase.</p> </li> <li> <p>Code Review Turnaround Time</p> <p>The time it takes to review and approve/reject code changes submitted by team members.</p> </li> <li> <p>Test Coverage</p> <p>The percentage of the codebase covered by automated tests.</p> </li> <li> <p>Bugs Open/Closed</p> <p>The number of open and closed bugs over time, helping to assess the health of the project.</p> </li> <li> <p>Release Frequency</p> <p>How often new releases or updates are deployed to production.</p> </li> <li> <p>Customer Satisfaction</p> <p>Feedback from users or stakeholders about the quality and functionality of the software.</p> </li> <li> <p>Effort Variance</p> <p>A comparison of the estimated effort versus the actual effort spent on a task or project.</p> </li> <li> <p>Code Complexity</p> <p>Measures like cyclomatic complexity that assess the complexity of the codebase, potentially indicating areas that need refactoring.</p> </li> <li> <p>Build Success Rate</p> <p>The percentage of successful builds compared to total attempts.</p> </li> <li> <p>Cycle Time</p> <p>The time taken to complete a unit of work, such as a user story, task, or feature. It helps measure the efficiency of the development process.</p> </li> <li> <p>Lead Time</p> <p>The time taken for a code change to move from the initiation of work (e.g., coding) to its deployment in production. It provides insights into the speed of delivery.</p> </li> <li> <p>Velocity</p> <p>A metric used in Agile methodologies like Scrum to measure the rate at which a team completes work during a specific time frame, often a sprint.</p> </li> <li> <p>Defect Density</p> <p>The number of defects or issues found in the software per unit of code, typically per lines of code (LOC) or function points.</p> </li> <li> <p>Code Coverage</p> <p>The percentage of code that is exercised by automated tests, indicating the extent to which the codebase is tested.</p> </li> <li> <p>Deployment Frequency</p> <p>How often new releases or updates are deployed to production, indicating the organization's ability to deliver changes.</p> </li> <li> <p>Change Failure Rate</p> <p>The percentage of code changes that result in defects or issues after deployment, reflecting the stability of the release process.</p> </li> <li> <p>Time to Restore Service</p> <p>The time taken to restore normal service after an incident or outage, measuring the efficiency of incident response.</p> </li> <li> <p>Customer Satisfaction</p> <p>Feedback from users or stakeholders about the quality and functionality of the software.</p> </li> <li> <p>Technical Debt</p> <p>An estimation of the effort required to address accumulated technical issues or shortcuts in the codebase.</p> </li> <li> <p>Code Churn</p> <p>The rate of code changes or additions, indicating the level of activity in the codebase.</p> </li> <li> <p>Code Review Turnaround Time</p> <p>The time taken to review and approve/reject code changes submitted by team members.</p> </li> </ol>"},{"location":"articles/software-metric/#112-meta-metrics","title":"1.1.2. Meta-Metrics","text":"<p>Meta-metrics, also known as <code>metrics about metrics</code>, focus on the measurement process itself. They assess the quality, accuracy, and effectiveness of the primary metrics being collected. Meta-metrics help ensure that the measurement process is reliable, consistent, and provides meaningful insights.</p> <p>By monitoring and optimizing meta-metrics, organizations can ensure the accuracy, reliability, and usefulness of their primary metrics, leading to more informed decision-making and effective process improvement.</p> <ol> <li> <p>Data Completeness</p> <p>Measures the extent to which all required data points are being captured accurately and consistently. Incomplete data can lead to inaccurate analysis and conclusions.</p> </li> <li> <p>Data Accuracy</p> <p>Assesses the correctness and reliability of the data being collected. Incorrect data can result in misleading insights and decisions.</p> </li> <li> <p>Data Timeliness</p> <p>Measures how quickly data is collected and entered into the measurement system. Timely data is crucial for real-time analysis and decision-making.</p> </li> <li> <p>Metric Relevance</p> <p>Evaluates whether the chosen metrics align with the goals and objectives of the measurement process. Irrelevant metrics can waste resources and lead to incorrect conclusions.</p> </li> <li> <p>Metric Validity</p> <p>Determines whether the selected metrics accurately measure what they are intended to measure. Valid metrics provide meaningful insights.</p> </li> <li> <p>Metric Reliability</p> <p>Reflects the consistency and stability of metric measurements over time. Unreliable metrics can lead to uncertainty and lack of trust.</p> </li> <li> <p>Metric Sensitivity</p> <p>Indicates how well a metric captures variations and changes in the process being measured. Metrics should be sensitive enough to detect meaningful differences.</p> </li> <li> <p>Metric Precision</p> <p>Measures the level of detail and accuracy in metric measurements. Precise metrics provide more accurate and refined insights.</p> </li> <li> <p>Metric Interpretability</p> <p>Assesses how easily metric results can be understood and interpreted by stakeholders. Complex metrics can hinder effective communication.</p> </li> <li> <p>Metric Bias</p> <p>Checks for any systematic errors or distortions in metric measurements. Bias can lead to skewed perceptions of performance.</p> </li> <li> <p>Metric Stability</p> <p>Examines the consistency of metric measurements across different contexts and environments. Stable metrics are reliable under varying conditions.</p> </li> <li> <p>Metric Overhead</p> <p>Measures the effort, time, and resources required to collect and analyze metric data. Excessive overhead can discourage the use of metrics.</p> </li> <li> <p>Metric Correlation</p> <p>Analyzes the relationships between different metrics to identify dependencies and interactions that might affect interpretations.</p> </li> </ol>"},{"location":"articles/software-metric/#12-standards","title":"1.2. Standards","text":"<p>Standards and guidelines provide recommendations for Software metric. These standards help organizations establish consistent and effective practices for measuring and improving software development processes.</p> <ol> <li> <p>ISO/IEC 15939 (Software Measurement Process)</p> <p>This international standard provides guidance on establishing a measurement process for software development and maintenance. It covers measurement framework, planning, implementation, and analysis.</p> </li> <li> <p>ISO/IEC 25010 (Systems and Software Quality Requirements and Evaluation - SQuaRE)</p> <p>Part of the SQuaRE series, this standard defines a comprehensive set of quality characteristics and sub-characteristics for software products. It offers guidance on assessing and measuring software quality.</p> </li> <li> <p>CMMI (Capability Maturity Model Integration)</p> <p>While not exclusively focused on metrics, CMMI provides a framework for process improvement in various domains, including software development. It includes guidance on measurement and analysis practices.</p> </li> <li> <p>ISO/IEC TR 90006 (Guidance on the Application of ISO 9001:2015 to IT Service Management and Software Engineering)</p> <p>This technical report provides guidance on adapting ISO 9001 quality management principles to software engineering processes, including measurement and metrics.</p> </li> <li> <p>ISO/IEC TR 20000-7 (Guidance on the Integration and Correlation of ISO/IEC 20000-1:2018 and ISO/IEC 12207:2017)</p> <p>This technical report provides guidance on integrating service management and software engineering processes, which may involve metrics alignment.</p> </li> <li> <p>IEEE Standard 1061 (Software Metrics)</p> <p>This IEEE standard provides definitions and guidelines for various software metrics, helping organizations establish consistent measurement practices.</p> </li> <li> <p>SEI (Software Engineering Institute) Metrics Program</p> <p>The SEI offers guidance on software measurement and metrics through various resources, including books, articles, and training.</p> </li> </ol>"},{"location":"articles/software-metric/#13-frameworks","title":"1.3. Frameworks","text":"<p>NOTE When choosing a framework, consider the organization's goals, methodologies, and specific needs. Remember that metrics should be chosen thoughtfully, aligned with objectives, and not used in isolation. Also, be cautious about potential unintended consequences, such as optimizing for metrics at the expense of overall quality or customer satisfaction.</p> <ol> <li> <p>Goal-Question-Metric (GQM) Framework</p> <p>GQM is a goal-oriented approach that helps define goals, questions, and corresponding metrics. It ensures that metrics are aligned with specific objectives and provide meaningful insights.</p> </li> <li> <p>Key Performance Indicators (KPIs)</p> <p>KPIs are specific metrics that organizations use to measure progress towards strategic goals. They focus on critical success factors and are often used at higher levels of decision-making.</p> </li> <li> <p>Software Engineering Institute (SEI) CMMI</p> <p>The Capability Maturity Model Integration provides a structured framework for process improvement across various domains, including software development. It includes performance measurement and metrics as a key component.</p> </li> <li> <p>ISO/IEC 25010 (SQuaRE)</p> <p>This standard defines a comprehensive set of software quality attributes and their sub-characteristics. It provides a structured approach for measuring software quality and performance.</p> </li> <li> <p>Agile Metrics Frameworks</p> <p>Agile methodologies have their own set of metrics frameworks, such as SAFe (Scaled Agile Framework) metrics and Disciplined Agile Delivery (DAD) metrics, which are tailored to the Agile development context.</p> </li> <li> <p>Balanced Scorecard (BSC)</p> <p>While not specific to software development, BSC is a strategic performance management framework that helps organizations track and manage their overall performance using a balanced set of financial and non-financial metrics.</p> </li> <li> <p>Lean Software Development Metrics</p> <p>Inspired by Lean principles, this framework focuses on minimizing waste, improving flow, and delivering value. Metrics like cycle time, lead time, and work in progress (WIP) are commonly used in this context.</p> </li> <li> <p>DORA</p> <p>The DevOps Research and Assessment (DORA) framework emphasizes Four Key Metrics of DevOps related to deployment frequency, lead time, change failure rate, and time to restore service. It's geared towards improving software delivery and operations.</p> <p>NOTE See DORA for details.</p> </li> <li> <p>SPACE</p> <p>The SPACE framework is a set of metrics that measure the capabilities of software development teams. The SPACE framework divides the capabilities of software development teams into five categories: satisfaction and well-being, performance, activity, collaboration and communication, and efficiency and flow.</p> </li> </ol>"},{"location":"articles/software-metric/#14-tools","title":"1.4. Tools","text":"<p>Tools offer a range of capabilities, from tracking code quality and deployment metrics to monitoring application performance and user experience.</p> <ol> <li> <p>Jira</p> <p>A widely used project management and issue tracking tool that offers customizable dashboards and reporting features to track various metrics.</p> </li> <li> <p>GitLab</p> <p>A platform that provides version control, CI/CD pipelines, and issue tracking, along with built-in metrics and analytics for software development processes.</p> </li> <li> <p>GitHub</p> <p>Similar to GitLab, GitHub offers version control and issue tracking, with additional integrations and extensions for tracking Software metric.</p> </li> <li> <p>Azure DevOps (formerly Visual Studio Team Services)</p> <p>A set of development tools that includes version control, CI/CD pipelines, and work tracking with built-in reporting and analytics.</p> </li> <li> <p>Jenkins</p> <p>An open-source automation server that can be configured to generate various metrics related to build and deployment processes.</p> </li> <li> <p>SonarQube</p> <p>A tool for continuous code quality inspection, providing metrics on code smells, bugs, vulnerabilities, and code coverage.</p> </li> <li> <p>New Relic</p> <p>A monitoring and observability platform that offers insights into application performance, user experience, and infrastructure metrics.</p> </li> <li> <p>Prometheus</p> <p>An open-source monitoring and alerting toolkit that specializes in collecting and displaying metrics from applications and systems.</p> </li> <li> <p>Grafana</p> <p>A popular open-source dashboard and visualization platform that can integrate with various data sources, including metrics from software development processes.</p> </li> <li> <p>Opsgenie</p> <p>A tool for incident management and alerting that can help measure metrics related to incident response times and resolution.</p> </li> <li> <p>Datadog</p> <p>A monitoring and analytics platform that provides insights into application performance, infrastructure metrics, and logs.</p> </li> <li> <p>Trello</p> <p>A visual project management tool that can be customized to track various development metrics using boards and cards.</p> </li> </ol>"},{"location":"articles/software-metric/#2-references","title":"2. References","text":"<ul> <li>Wikipedia software metric article.</li> </ul>"},{"location":"articles/software-testing/","title":"Software Testing","text":"<p>Software testing is the process of evaluating a software application or system to identify any defects or issues. The purpose of software testing is to ensure that the software meets its intended requirements, is functional, reliable, and operates as expected.</p> <ul> <li>1. Category</li> <li>1.1. Testing Patterns<ul> <li>1.1.1. Arrange, Act, Assert</li> <li>1.1.2. Given-When-Then</li> <li>1.1.3. Table-Driven Testing</li> </ul> </li> <li>1.2. Testing Techniques and Practices<ul> <li>1.2.1. Test-Driven Development</li> <li>1.2.2. Behavior-Driven Development</li> <li>1.2.3. Acceptance Test-Driven Development</li> <li>1.2.4. Test Double</li> <li>1.2.4.1. Mock</li> <li>1.2.4.2. Fake</li> <li>1.2.4.3. Stub</li> <li>1.2.4.4. Spy</li> <li>1.2.5. Test Fixtures</li> <li>1.2.6. Code Coverage</li> </ul> </li> <li>1.3. Testing Types<ul> <li>1.3.1. Functional Testing</li> <li>1.3.1.1. Shift-Left Testing</li> <li>1.3.1.2. System Testing</li> <li>1.3.1.3. Integration Testing</li> <li>1.3.1.4. Unit Testing</li> <li>1.3.1.5. Fuzz Testing</li> <li>1.3.1.6. Regression Testing</li> <li>1.3.1.7. End-to-End Testing</li> <li>1.3.1.8. Acceptance Testing<ul> <li>1.3.1.8.1. User Acceptance Testing</li> <li>1.3.1.8.2. Operational Acceptance Testing</li> </ul> </li> <li>1.3.1.9. Alpha Testing</li> <li>1.3.1.10. Beta Testing</li> <li>1.3.1.11. Usability Testing</li> <li>1.3.1.12. Cross-Browser Testing</li> <li>1.3.1.13. Compatibility Testing</li> <li>1.3.1.14. Accessibility Testing</li> <li>1.3.1.15. Sanity Testing</li> <li>1.3.1.16. Smoke Testing</li> <li>1.3.1.17. Exploratory Testing</li> <li>1.3.1.18. Black Box Testing</li> <li>1.3.1.19. White Box Testing</li> <li>1.3.1.20. Gray Box Testing</li> <li>1.3.1.21. A/B Testing</li> <li>1.3.1.22. Monkey Testing</li> <li>1.3.2. Non-Functional Testing</li> <li>1.3.2.1. Security Testing<ul> <li>1.3.2.1.1. Penetration Testing</li> <li>1.3.2.1.2. Vulnerability Scanning</li> </ul> </li> <li>1.3.2.2. Performance Testing<ul> <li>1.3.2.2.1. Load Testing</li> <li>1.3.2.2.2. Stress Testing</li> <li>1.3.2.2.3. Endurance Testing</li> <li>1.3.2.2.4. Spike Testing</li> <li>1.3.2.2.5. Volume Testing</li> <li>1.3.2.2.6. Scalability Testing</li> <li>1.3.2.2.7. Capacity Testing</li> </ul> </li> <li>1.3.2.3. Application Security Testing<ul> <li>1.3.2.3.1. Static Application Security Testing</li> <li>1.3.2.3.2. Dynamic Application Security Testing</li> <li>1.3.2.3.3. Interactive Application Security Testing</li> </ul> </li> </ul> </li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> </ul>"},{"location":"articles/software-testing/#1-category","title":"1. Category","text":"<p>There are different types of software testing, including manual testing and automated testing. Manual testing involves executing test cases manually and observing the results. Automated testing involves the use of tools and scripts to automate the testing process.</p> <p>Process of Software Testing:</p> <ul> <li> <p>Planning</p> <p>This phase involves defining the scope and objectives of testing, identifying the testing approach and techniques to be used, and determining the required resources and timelines.</p> </li> <li> <p>Designing</p> <p>In this phase, test cases and test scenarios are developed based on the requirements and specifications of the software.</p> </li> <li> <p>Execution</p> <p>During this phase, the developed test cases are executed to verify the functionality of the software.</p> </li> <li> <p>Reporting</p> <p>The results of the testing are recorded and reported to stakeholders, including defects or issues found and recommendations for improvements.</p> </li> <li> <p>Retesting</p> <p>If defects are identified, the software is retested to verify that the issues have been resolved.</p> </li> </ul>"},{"location":"articles/software-testing/#11-testing-patterns","title":"1.1. Testing Patterns","text":"<p>Testing patterns are reusable techniques to common testing problems that can be used to improve the effectiveness of software testing. They allow to organize and structure test code in a way that is maintainable, scalable, and extensible.</p>"},{"location":"articles/software-testing/#111-arrange-act-assert","title":"1.1.1. Arrange, Act, Assert","text":"<p>Arrange, Act, Assert (AAA) is a software testing pattern that provides a structured and organized way to write unit tests. It helps make the tests more readable, maintainable, and easy to understand.</p> <p>Feature of AAA:</p> <ul> <li> <p>Arrange</p> <p>Setup the test environment by creating the necessary objects, initializing variables, and providing input data required for the test. This step ensures that the unit under test (the code being tested) has the appropriate conditions to execute the test scenario correctly.</p> </li> <li> <p>Act</p> <p>Involves executing the specific method or functionality of the unit to test. This typically involves calling a method or performing some action on the unit under test using the provided input data from the <code>Arrange</code> phase.</p> </li> <li> <p>Assert</p> <p>Verify the outcome of the test. Check whether the actual result of the action taken in the <code>Act</code> phase matches the expected result that you defined during the <code>Arrange</code> phase. If the outcome is as expected, the test passes, otherwise, it fails, indicating a potential problem in the code.</p> </li> </ul> <p>Example of AAA:</p> <pre><code>def test_add_numbers():\n    # Arrange\n    num1 = 5\n    num2 = 10\n\n    # Act\n    result = add_numbers(num1, num2)\n\n    # Assert\n    assert result == 15, \"Addition of two numbers failed.\"\n</code></pre>"},{"location":"articles/software-testing/#112-given-when-then","title":"1.1.2. Given-When-Then","text":"<p>Given-When-Then (GWT) is a software testing pattern used for structuring and writing test scenarios, particularly in behavior-driven development (BDD) and acceptance test-driven development (ATDD). GWT provides a more human-readable and expressive format for describing test cases and their expected behavior.</p> <p>Features of GWT:</p> <ul> <li> <p>Given</p> <p>The <code>Given</code> section sets up the initial context or the preconditions for the test scenario. It describes the state of the system or the environment before the action being tested occurs. This typically includes creating objects, initializing variables, and any necessary setup steps.</p> </li> <li> <p>When</p> <p>The <code>When</code> section represents the action or event that triggers the behavior being tested. It is the specific operation or method call that you want to verify for correctness.</p> </li> <li> <p>Then</p> <p>The <code>Then</code> section defines the expected outcome or behavior of the system after the action in the <code>When</code> section has been performed. It includes assertions or expectations to check whether the actual result matches the expected result.</p> </li> </ul> <p>Example of GWT:</p> <pre><code>def test_login_with_valid_credentials():\n    # Given\n    username = \"admin\"\n    password = \"password\"\n\n    # When\n    response = login(username, password)\n\n    # Then\n    assert response.status_code == 200\n    assert response.json()[\"message\"] == \"Login successful\"\n</code></pre>"},{"location":"articles/software-testing/#113-table-driven-testing","title":"1.1.3. Table-Driven Testing","text":"<p>Table-Driven Testing is a software testing technique in which test cases are organized in a tabular format, often using a spreadsheet or a list of input-output pairs. It is particularly useful for multiple similar test cases that can be represented in a systematic and concise manner. This approach is commonly used for testing functions or methods that take a set of input values and produce corresponding output values.</p> <p>Table-Driven Testing separates the test case logic from the test code, making it easier to add, modify or remove test cases without changing the test implementation. It promotes maintainability and reduces duplication of test code.</p> <p>NOTE It's essential to strike a balance between using <code>Table-Driven Testing</code> and other testing techniques. While it works well for certain scenarios, not all test cases may fit neatly into a table format. Combining <code>Table-Driven Testing</code> with other testing patterns like <code>Given-When-Then</code> or <code>AAA</code> can provide a comprehensive and flexible testing strategy.</p> <p>Feature of Table-Driven Testing:</p> <ul> <li> <p>Consitency</p> <p>Concise and readable representation of test cases and  test coverage.</p> </li> <li> <p>Maintenability</p> <p>Easy to add or modify test cases without affecting the test implementation.</p> </li> <li> <p>Clarity</p> <p>Simplifies the process of generating test data for a wide range of scenarios.</p> </li> </ul> <p>Example of Table-Driven Testing:</p> <pre><code>func TestPercent(t *testing.T) {\n  t.Parallel()\n\n  type in struct {\n    percent float64\n    value   float64\n  }\n\n  type want struct {\n    value float64\n    err   error\n  }\n\n  tests := []struct {\n    name string\n    in   in\n    want want\n  }{\n    {\n      name: \"valid positive input\",\n      in: in{\n        percent: 25,\n        value:   100,\n      },\n      want: want{\n        value: 25,\n        err:   nil,\n      },\n    },\n    {\n      name: \"valid nagative input\",\n      in: in{\n        percent: 50,\n        value:   -200,\n      },\n      want: want{\n        value: -100,\n        err:   nil,\n      },\n    },\n    {\n      name: \"invalid percentage\",\n      in: in{\n        percent: 150,\n        value:   100,\n      },\n      want: want{\n        value: 0.0,\n        err:   errors.New(out of the range),\n      },\n    },\n  }\n\n  for _, tt := range tests {\n    t.Run(tt.name, func(t *testing.T) {\n      got, err := percent.Percent(tt.in.percent, tt.in.value)\n      if !errors.Is(err, tt.want.err) {\n        t.Errorf(\"Percent() error = %v, want err %v\", err, tt.want.err)\n      }\n      if !cmp.Equal(got, tt.want.value) {\n        t.Errorf(\"Percent(%+v) = %v, want %v\", tt.in, got, tt.want.value)\n      }\n    })\n  }\n}\n</code></pre> <p>NOTE The <code>cmp</code> package is designed for testing, rather than production use. As such, it may panic when it suspects that a comparison is performed incorrectly to provide instruction to users on how to improve the test to be less brittle. Given cmp\u2019s propensity towards panicking, it makes it unsuitable for code that is used in production as a spurious panic may be fatal.</p>"},{"location":"articles/software-testing/#12-testing-techniques-and-practices","title":"1.2. Testing Techniques and Practices","text":""},{"location":"articles/software-testing/#121-test-driven-development","title":"1.2.1. Test-Driven Development","text":"<p>Test-Driven Development (TDD) is a software development process in which test cases are written before any code is written. The idea behind TDD is to ensure that code is thoroughly tested and meets requirements before it is implemented, reducing the likelihood of defects and improving the overall quality of the software.</p> <p>Concept of TDD:</p> <ul> <li> <p>Write a test</p> <p>The developer writes a test case for the functionality they want to implement, with the expectation that the test will fail because the code has not yet been written.</p> </li> <li> <p>Write the code</p> <p>The developer writes the code to make the test pass. This is typically done in small, incremental steps, with each step building on the previous one.</p> </li> <li> <p>Run the test</p> <p>The developer runs the test to ensure that it passes.</p> </li> <li> <p>Refactor the code</p> <p>Once the test passes, the developer can refactor the code as needed to improve its design, readability, or performance.</p> </li> <li> <p>Repeat</p> <p>The process is repeated for each new functionality or feature that is added to the software.</p> </li> </ul> <p>Benefits of TDD:</p> <ul> <li> <p>Improved code quality</p> <p>Because tests are written before the code, TDD helps ensure that the code meets requirements and is thoroughly tested.</p> </li> <li> <p>Faster feedback loop</p> <p>TDD provides fast feedback on the quality of the code, allowing developers to quickly identify and fix issues.</p> </li> <li> <p>Reduced time and cost</p> <p>By catching defects early in the development process, TDD can reduce the time and cost of fixing defects later on.</p> </li> <li> <p>Encourages better design</p> <p>TDD encourages developers to write code that is modular, testable, and maintainable, leading to better overall software design.</p> </li> </ul>"},{"location":"articles/software-testing/#122-behavior-driven-development","title":"1.2.2. Behavior-Driven Development","text":"<p>Behavior-Driven Development (BDD) is a software development approach that focuses on defining the behavior of a system through the use of concrete examples expressed in natural language. It is a collaborative process that involves developers, testers, and business stakeholders working together to ensure that the system is being developed to meet the needs of its users.</p> <p>Key concepts of BDD:</p> <ul> <li> <p>User Stories</p> <p>BDD starts with user stories, which are written in a specific format that describes the expected behavior of the system from the user's perspective.</p> </li> <li> <p>Examples</p> <p>Examples are used to illustrate the behavior of the system, and they are written in natural language using a framework like Gherkin.</p> </li> <li> <p>Scenarios</p> <p>Scenarios are the specific tests that are derived from the user stories and examples. They are used to ensure that the system is behaving as expected.</p> </li> <li> <p>Automation</p> <p>BDD encourages the use of automation to ensure that the scenarios are executed consistently and efficiently.</p> </li> <li> <p>Collaboration</p> <p>BDD is a collaborative process that involves developers, testers, and business stakeholders working together to ensure that the system is being developed to meet the needs of its users.</p> </li> </ul>"},{"location":"articles/software-testing/#123-acceptance-test-driven-development","title":"1.2.3. Acceptance Test-Driven Development","text":"<p>Acceptance Test-Driven Development (ATDD) is a software development approach that aims to ensure that the software meets the customer's requirements and expectations by defining, automating, and executing acceptance tests early in the development cycle.</p> <p>ATDD is an extension of Test-Driven Development (TDD) and is focused on defining and automating acceptance tests before implementing the code. The goal of ATDD is to ensure that the software satisfies the customer's acceptance criteria and to prevent defects by catching them early in the development cycle.</p> <p>Concept of ATDD:</p> <ul> <li> <p>Collaborate with stakeholders</p> <p>The development team collaborates with the customer or product owner to define the acceptance criteria for the software.</p> </li> <li> <p>Define acceptance tests</p> <p>The team defines acceptance tests based on the acceptance criteria, using a behavior-driven development (BDD) approach.</p> </li> <li> <p>Automate acceptance tests</p> <p>The team automates the acceptance tests using a testing framework, such as Selenium or Cucumber.</p> </li> <li> <p>Implement code</p> <p>The team implements the code to meet the acceptance criteria and passes the automated acceptance tests.</p> </li> <li> <p>Repeat</p> <p>The team repeats this process for each new requirement or change in requirements.</p> </li> </ul>"},{"location":"articles/software-testing/#124-test-double","title":"1.2.4. Test Double","text":"<p>Test Double is a concept from software testing and is used in the context of writing unit tests. It refers to a type of test-specific object that is used to replace a real component or collaborator within a test scenario. The purpose of using Test Doubles is to isolate the unit being tested from its dependencies or collaborators, ensuring that the test evaluates only the behavior of the unit itself.</p> <p>By replacing real dependencies with Test Doubles, developers can simulate the behavior of those dependencies and create controlled testing environments. This allows for focused unit testing without the need to involve external systems or actual implementations.</p>"},{"location":"articles/software-testing/#1241-mock","title":"1.2.4.1. Mock","text":"<p>A mock is a Test Double that simulates the behavior of a real object and allows testers to set specific expectations and verify interactions with the unit under test.</p> <p>Mocks can be used to simulate the behavior of external dependencies, such as a database or a web service, during testing, without actually executing any real logic. Mocks can be programmed to expect certain interactions with the dependencies, such as method calls or data requests, and can raise an error if those interactions do not occur as expected. This allows developers to ensure that their code is properly interacting with its dependencies, and can help catch errors or issues early in the development process.</p>"},{"location":"articles/software-testing/#1242-fake","title":"1.2.4.2. Fake","text":"<p>A fake is a simplified implementation of a real object, often used when the real implementation is too complex or time-consuming for testing.</p>"},{"location":"articles/software-testing/#1243-stub","title":"1.2.4.3. Stub","text":"<p>A stub is a simplified version of a real object that provides predefined responses to method calls. Stubs are used to return specific values to test certain scenarios.</p>"},{"location":"articles/software-testing/#1244-spy","title":"1.2.4.4. Spy","text":"<p>A spy is a type of Test Double that records the interactions between the unit under test and its collaborators. It allows testers to observe the behavior of the unit more closely.</p>"},{"location":"articles/software-testing/#125-test-fixtures","title":"1.2.5. Test Fixtures","text":"<p>Test fixtures are a collection of setup tasks, data, and environment configuration that are used to prepare the system under test for testing. They are used in automated testing to ensure that the system under test is in a consistent and known state before executing each test.</p> <p>Test Fixtures can include:</p> <ul> <li> <p>Setup and Teardown scripts</p> <p>These scripts are used to prepare the system under test for testing and to clean up after testing is complete.</p> </li> <li> <p>Test data</p> <p>Test data is used to populate the system under test with data that will be used in the test.</p> </li> <li> <p>Test environment configuration</p> <p>The test environment configuration is used to set up the environment in which the test will run, such as configuring the database, server, or other systems that the system under test interacts with.</p> </li> </ul>"},{"location":"articles/software-testing/#126-code-coverage","title":"1.2.6. Code Coverage","text":"<p>Code coverage is a measure of how much of the source code of a software application is executed during testing. It is used to determine the effectiveness of testing and to identify any gaps in the test cases. Code coverage is usually expressed as a percentage of the total lines of code in the application.</p> <p>Code coverage tools can be used to generate reports that show the percentage of code that was covered during testing, as well as highlighting any untested code.</p> <p>Types of Code Coverage:</p> <ul> <li> <p>Statement Coverage</p> <p>This measures the percentage of statements in the source code that are executed during testing.</p> </li> <li> <p>Branch Coverage</p> <p>This measures the percentage of decision points (such as if/else statements) that are executed during testing.</p> </li> <li> <p>Function Coverage</p> <p>This measures the percentage of functions in the source code that are executed during testing.</p> </li> <li> <p>Condition Coverage</p> <p>This measures the percentage of Boolean expressions (such as <code>if x &gt; 0</code>) that are executed during testing.</p> </li> </ul>"},{"location":"articles/software-testing/#13-testing-types","title":"1.3. Testing Types","text":""},{"location":"articles/software-testing/#131-functional-testing","title":"1.3.1. Functional Testing","text":"<p>Functional testing is a type of software testing that evaluates the individual components or modules of a system to ensure that they are working as expected and that they meet the specified requirements. The purpose of functional testing is to ensure that the application behaves as expected and meets the specified requirements.</p> <p>Functional testing can be performed:</p> <ul> <li> <p>Unit Testing</p> <p>This involves testing individual units of code to ensure that they are working correctly and that they meet the specified requirements.</p> </li> <li> <p>Integration Testing</p> <p>This involves testing the integration points between different components of the system to ensure that they are communicating correctly and functioning as expected.</p> </li> <li> <p>Regression Testing</p> <p>This involves testing the system after a change or update has been made to ensure that existing functionality has not been affected.</p> </li> <li> <p>User Acceptance Testing</p> <p>This involves testing the system with a group of end-users or customers to ensure that it meets their needs and that it is easy to use.</p> </li> <li> <p>Boundary Value Analysis</p> <p>This involves testing the system with values that are at the boundaries of the specified requirements to ensure that it is handling these values correctly.</p> </li> <li> <p>Equivalence Partitioning</p> <p>This involves dividing the input data into different equivalence classes and testing the system with representative values from each class to ensure that it is handling the input data correctly.</p> </li> </ul>"},{"location":"articles/software-testing/#1311-shift-left-testing","title":"1.3.1.1. Shift-Left Testing","text":"<p>Shift-left testing is an approach to software testing where testing activities are moved earlier in the software development life cycle. This approach emphasizes the importance of identifying and addressing defects as early as possible in the development process, rather than waiting until later stages such as system or acceptance testing.</p> <p>In a traditional software development model, testing is typically performed after the development phase, once the code has been completed. However, in a shift-left testing model, testing activities such as unit testing, integration testing, and functional testing are performed much earlier in the development process, often alongside the coding process.</p> <p>By moving testing activities earlier in the development process, shift-left testing aims to identify and fix defects earlier, before they become more difficult and expensive to fix. This can ultimately result in higher-quality software that is delivered more quickly and at a lower cost.</p> <p>Shift-left testing can be achieved through the use of automation tools and techniques such as continuous integration and continuous testing. By automating testing activities and integrating them into the development process, developers can identify and address defects in real time, as they are introduced into the code. This can help ensure that defects are caught early, before they can have a negative impact on the software or the user experience.</p> <p>Shift-left testing can also help to promote a culture of quality within development teams, by emphasizing the importance of testing and quality assurance throughout the entire software development life cycle.</p>"},{"location":"articles/software-testing/#1312-system-testing","title":"1.3.1.2. System Testing","text":"<p>System testing is a type of software testing that is performed on a complete and integrated system to evaluate its compliance with the specified requirements. The objective of system testing is to ensure that the system meets the functional and non-functional requirements and works as expected in a real-world environment.</p> <p>Benefits of System Testing:</p> <ul> <li> <p>Scope</p> <p>System testing covers the entire system and its components, including hardware, software, and networks.</p> </li> <li> <p>Test Environment</p> <p>System testing is typically performed in a test environment that is similar to the production environment to ensure that the system performs as expected in a real-world environment.</p> </li> <li> <p>Test Types</p> <p>System testing can involve a variety of testing types, including functional testing, performance testing, security testing, usability testing, and others, depending on the system requirements.</p> </li> <li> <p>Test Cases</p> <p>System testing requires a comprehensive set of test cases that cover all the system requirements and use cases.</p> </li> <li> <p>Test Execution</p> <p>System testing involves executing the test cases and analyzing the results to identify defects and issues.</p> </li> <li> <p>Defect Management</p> <p>System testing requires effective defect management to track, prioritize, and resolve the defects and issues identified during testing.</p> </li> <li> <p>Acceptance Criteria</p> <p>System testing includes establishing acceptance criteria to determine when the system is ready for release to production.</p> </li> </ul>"},{"location":"articles/software-testing/#1313-integration-testing","title":"1.3.1.3. Integration Testing","text":"<p>Integration testing is a type of software testing that verifies the interactions between different software components or modules to ensure that they function correctly as a group. The purpose of integration testing is to detect and resolve any issues or defects that arise due to the interactions between the components.</p> <p>During integration testing, test cases are designed to test the interactions between different modules and their interfaces. The test cases should be designed to identify any errors or issues that may arise from these interactions, such as data transfer errors, incorrect function calls, and other types of interface errors.</p> <p>Integration testing can be performed:</p> <ul> <li> <p>Top-Down Integration Testing</p> <p>This approach tests the high-level modules first, and then gradually adds lower-level modules and sub-modules for testing.</p> </li> <li> <p>Bottom-Up Integration Testing</p> <p>This approach tests the lower-level modules first, and then gradually adds higher-level modules for testing.</p> </li> <li> <p>Big Bang Integration Testing</p> <p>This approach tests all the modules of an application together as a whole.</p> </li> <li> <p>Sandwich Integration Testing</p> <p>This approach combines the top-down and bottom-up integration testing approaches.</p> </li> </ul>"},{"location":"articles/software-testing/#1314-unit-testing","title":"1.3.1.4. Unit Testing","text":"<p>Unit testing is a software testing technique where individual units or components of a software application are tested in isolation from the rest of the system to ensure that they meet the specified requirements and function as expected. The objective of unit testing is to detect and isolate defects early in the development cycle, before the code is integrated with other components or systems.</p> <p>Unit testing can be performed:</p> <ul> <li> <p>Test-driven development (TDD)</p> <p>This involves writing the tests for the code before actually writing the code itself. The tests are used as a reference for writing the code and to ensure that the code meets the expected requirements.</p> </li> <li> <p>White-box testing</p> <p>This involves testing the internal structures of the software, such as the code and the algorithms used, to ensure that they are functioning correctly.</p> </li> <li> <p>Black-box testing</p> <p>This involves testing the external behavior of the software, without looking at its internal structures, to ensure that it is functioning correctly as per the requirements.</p> </li> <li> <p>Code coverage analysis</p> <p>This involves measuring the extent to which the code has been tested, by analyzing the number of lines of code that have been executed during testing.</p> </li> <li> <p>Mutation testing</p> <p>This involves introducing artificial faults or mutations into the code, to ensure that the tests are able to detect and report such faults.</p> </li> </ul> <p>Benefits of Unit Testing:</p> <ul> <li> <p>Scope</p> <p>Unit testing focuses on testing individual units or components of the software application, such as functions, classes, or methods.</p> </li> <li> <p>Test Environment</p> <p>Unit testing is typically performed in a development environment that simulates the production environment, but with the necessary tools and frameworks to support unit testing.</p> </li> <li> <p>Test Types</p> <p>Unit testing can involve a variety of testing types, such as functional testing, boundary testing, and exception testing, depending on the nature of the unit being tested.</p> </li> <li> <p>Test Frameworks</p> <p>Unit testing requires a test framework, such as GTest, JUnit, NUnit, or pytest, to define test cases, execute tests, and report results.</p> </li> <li> <p>Test Cases</p> <p>Unit testing requires a comprehensive set of test cases that cover all the possible scenarios and outcomes for the unit being tested.</p> </li> <li> <p>Test Execution</p> <p>Unit testing involves executing the test cases and analyzing the results to identify defects and issues.</p> </li> <li> <p>Test Automation</p> <p>Unit testing can be automated using continuous pipeline tools such as Jenkins or Travis CI to improve efficiency and reliability.</p> </li> </ul>"},{"location":"articles/software-testing/#1315-fuzz-testing","title":"1.3.1.5. Fuzz Testing","text":"<p>Fuzz testing, also known as fuzzing, is a type of software testing that involves feeding invalid, unexpected, or random data inputs to a software program to detect vulnerabilities, crashes, and other errors.</p> <p>Fuzz testing can be performed manually or with the use of automated tools, which generate and feed random inputs to the software under test. The goal of fuzz testing is to uncover vulnerabilities and edge cases that might not be discovered through traditional testing methods, such as unit testing and integration testing.</p> <p>Benefits of Fuzz Testing:</p> <ul> <li> <p>Comprehensive coverage</p> <p>Fuzz testing can provide comprehensive test coverage, as it can explore a wide range of potential input combinations that might be difficult to test manually.</p> </li> <li> <p>Detection of edge cases</p> <p>Fuzz testing can help to detect edge cases and unexpected input combinations that might cause the software to crash or behave unpredictably.</p> </li> <li> <p>Early detection of vulnerabilities</p> <p>Fuzz testing can help to identify vulnerabilities early in the software development life cycle, which can save time and money in the long run.</p> </li> <li> <p>Automation</p> <p>Fuzz testing can be automated, which can save time and effort compared to manual testing.</p> </li> <li> <p>Scalability</p> <p>Fuzz testing can be scaled up to test large and complex software systems, making it suitable for testing enterprise-grade applications.</p> </li> </ul>"},{"location":"articles/software-testing/#1316-regression-testing","title":"1.3.1.6. Regression Testing","text":"<p>Regression testing is a type of software testing that verifies that changes or modifications made to the software application do not impact the existing functionality of the application. It involves re-testing the application after a change has been made to ensure that the application still performs as expected and that new changes do not introduce any new bugs or issues.</p> <p>Regression testing is important to ensure that the quality of the software application is maintained throughout the development lifecycle. It is typically performed after a change has been made to the application, such as a bug fix, new feature implementation, or software upgrade.</p>"},{"location":"articles/software-testing/#1317-end-to-end-testing","title":"1.3.1.7. End-to-End Testing","text":"<p>End-to-End testing is a type of software testing that involves testing the entire software application from start to finish, simulating a real-world scenario. The purpose of end-to-end testing is to ensure that all components of the application are working together correctly and that the application meets its intended requirements.</p> <p>End-to-End testing can be performed:</p> <ul> <li> <p>Scenario Testing</p> <p>This involves testing the system under different scenarios and user workflows to ensure that all components are working together as expected.</p> </li> <li> <p>Integration Testing</p> <p>This involves testing the integration points between different components of the system to ensure that they are communicating correctly and functioning as expected.</p> </li> <li> <p>Data Validation</p> <p>This involves validating the data that flows through the system to ensure that it is being processed correctly and accurately.</p> </li> <li> <p>User Interface Testing</p> <p>This involves testing the user interface of the system to ensure that it is easy to use and that all functions and features are working correctly.</p> </li> <li> <p>Performance Testing</p> <p>This involves testing the performance of the system under different load and stress conditions to ensure that it can handle the expected level of user traffic.</p> </li> <li> <p>Security Testing</p> <p>This involves testing the security of the system to ensure that it is protected against potential threats or attacks.</p> </li> </ul>"},{"location":"articles/software-testing/#1318-acceptance-testing","title":"1.3.1.8. Acceptance Testing","text":"<p>Acceptance Testing is a type of software testing that is conducted to determine whether the software meets the specified requirements and is acceptable for delivery to the end-users or stakeholders. The primary objective of acceptance testing is to ensure that the software fulfills the business requirements and functions as expected in the real-world environment.</p>"},{"location":"articles/software-testing/#13181-user-acceptance-testing","title":"1.3.1.8.1. User Acceptance Testing","text":"<p>User Acceptance Testing (UAT) is a type of acceptance testing that is performed by the end-users or customers of the software application. The purpose of UAT is to determine whether the software meets the needs and requirements of the end-users and is ready for release.</p> <p>UAT typically involves creating a set of test cases that cover the functional and non-functional requirements of the software. These test cases are designed to simulate real-world scenarios and usage patterns to ensure that the software meets the needs of the end-users or customers.</p> <p>UAT is usually performed in a controlled environment, such as a test lab or staging environment, and is often supervised by a dedicated testing team or a representative from the software development team. The testing team or representative provides guidance and support to the end-users or customers during the testing process, ensuring that they understand how to perform the tests and how to report any issues or defects that they encounter.</p>"},{"location":"articles/software-testing/#13182-operational-acceptance-testing","title":"1.3.1.8.2. Operational Acceptance Testing","text":"<p>Operational Acceptance Testing (OAT) evaluates whether the software is ready to be deployed and operated in the production environment. It focuses on verifying that all non-functional aspects, such as installation, configuration, and maintenance procedures, are well-documented and successfully executed.</p>"},{"location":"articles/software-testing/#1319-alpha-testing","title":"1.3.1.9. Alpha Testing","text":"<p>Alpha testing is a type of software testing that is performed by the development team or a group of testers in a lab environment before the software is released to the general public. The purpose of alpha testing is to identify any major issues or bugs that may still exist in the software and to ensure that it is stable enough to move on to the next stage of testing.</p> <p>Key characteristics of alpha testing:</p> <ul> <li> <p>Limited audience</p> <p>Alpha testing is typically conducted with a limited group of testers, who are either members of the development team or selected testers.</p> </li> <li> <p>Controlled environment</p> <p>Alpha testing is conducted in a controlled environment, such as a lab or a staging environment, where the testers can closely monitor the software and report any issues that arise.</p> </li> <li> <p>Early stage testing</p> <p>Alpha testing is one of the earliest stages of testing and is usually performed after the initial development of the software is complete.</p> </li> <li> <p>Functional testing</p> <p>Alpha testing focuses on functional testing, which means that the testers are mainly looking for any major issues or bugs that may still exist in the software.</p> </li> <li> <p>Collaborative approach</p> <p>Alpha testing involves a collaborative approach, with the testers working closely with the development team to ensure that any issues are quickly identified and resolved.</p> </li> <li> <p>Not public-facing</p> <p>Alpha testing is not public-facing, which means that the software is not yet available to the general public and is only being tested internally by the development team or a selected group of testers.</p> </li> </ul>"},{"location":"articles/software-testing/#13110-beta-testing","title":"1.3.1.10. Beta Testing","text":"<p>Beta testing is a type of software testing that involves releasing a pre-release version of the software, known as a beta version, to a limited group of external users or customers. The purpose of beta testing is to get feedback from real users on the usability, functionality, and overall quality of the software before it is released to the general public.</p> <p>Different types of beta testing include open beta testing, closed beta testing, and hybrid beta testing, each with their own approach to releasing the beta version of the software to external users.</p> <p>Key characteristics of beta testing:</p> <ul> <li> <p>Limited release</p> <p>Beta testing is typically conducted on a limited scale, with the beta version of the software only being released to a small group of external users or customers.</p> </li> <li> <p>Real-world testing</p> <p>Beta testing provides an opportunity for real-world testing of the software in a live environment, with real users testing the software in their own settings and use cases.</p> </li> <li> <p>Feedback collection</p> <p>The goal of beta testing is to gather feedback from users on the usability, functionality, and overall quality of the software. This feedback is used to identify and fix issues and improve the user experience before the final release.</p> </li> <li> <p>Time-bound</p> <p>Beta testing is usually time-bound, with a specific duration for the testing period. This allows for efficient collection of feedback and timely implementation of fixes before the final release.</p> </li> </ul>"},{"location":"articles/software-testing/#13111-usability-testing","title":"1.3.1.11. Usability Testing","text":"<p>Usability testing is a type of testing that focuses on evaluating how user-friendly and intuitive the software or application is for its intended users. The goal of usability testing is to identify any usability issues and areas of improvement in the user interface (UI) and user experience (UX).</p> <p>Usability testing involves recruiting a group of representative users who match the target audience of the software. The testers are given a set of tasks to perform using the software or application, and their interactions with the software are observed and recorded. The testers are asked to provide feedback on the ease of use, clarity of instructions, and overall satisfaction with the software.</p> <p>Usability testing can be performed at different stages of the software development lifecycle, including during the design phase, after the development phase, and during the post-production phase. The feedback collected during usability testing is used to make design changes and improvements to the software.</p>"},{"location":"articles/software-testing/#13112-cross-browser-testing","title":"1.3.1.12. Cross-Browser Testing","text":"<p>Cross-browser testing is a type of software testing that focuses on verifying the compatibility and functionality of a web application or website across different web browsers and their versions. The purpose of cross-browser testing is to ensure that the web application works consistently and without any errors across different browsers and platforms, and that users have a consistent experience irrespective of the browser or device they are using.</p> <p>During cross-browser testing, a variety of browsers are used, including Google Chrome, Mozilla Firefox, Safari, Microsoft Edge, and Internet Explorer. The web application or website is tested in different versions of each browser to ensure that it works well with older and newer versions of the browsers. The testing includes verifying that the UI elements, layouts, and features of the web application are consistent across different browsers.</p>"},{"location":"articles/software-testing/#13113-compatibility-testing","title":"1.3.1.13. Compatibility Testing","text":"<p>Compatibility testing is a type of software testing that focuses on verifying whether an application or system works as expected across different environments, platforms, and devices. The purpose of compatibility testing is to ensure that the application is compatible with various hardware, software, and network configurations and to identify any compatibility issues that may affect the performance or functionality of the application.</p> <p>Compatibility testing includes testing the application or system on different operating systems, hardware configurations, web browsers, mobile devices, and network environments. The testing process involves verifying that the application works seamlessly across different combinations of these environments and that it is not affected by factors such as screen resolution, memory, processor speed, or network bandwidth.</p>"},{"location":"articles/software-testing/#13114-accessibility-testing","title":"1.3.1.14. Accessibility Testing","text":"<p>Accessibility testing is a type of software testing that is performed to ensure that an application or system can be easily used by people with disabilities. The goal of accessibility testing is to ensure that the software is compliant with accessibility standards and guidelines, such as the Web Content Accessibility Guidelines (WCAG) issued by the World Wide Web Consortium (W3C).</p> <p>Accessibility testing can be performed:</p> <ul> <li> <p>Keyboard-only navigation testing</p> <p>This involves testing the application using only the keyboard to ensure that all functionality can be accessed without the use of a mouse.</p> </li> <li> <p>Screen reader testing</p> <p>This involves testing the application with a screen reader to ensure that all content and functionality is accessible to users who are blind or have low vision.</p> </li> <li> <p>Color contrast testing</p> <p>This involves testing the application to ensure that there is sufficient contrast between text and background colors for users with low vision.</p> </li> <li> <p>Focus testing</p> <p>This involves testing the application to ensure that keyboard focus is properly indicated and that users can easily navigate between elements using the keyboard.</p> </li> <li> <p>ARIA attribute testing</p> <p>This involves testing the application to ensure that it is properly utilizing Accessible Rich Internet Applications (ARIA) attributes to provide additional context and functionality for users with disabilities.</p> </li> <li> <p>Usability testing with users with disabilities</p> <p>This involves testing the application with users who have disabilities to gather feedback on its accessibility and usability.</p> </li> </ul>"},{"location":"articles/software-testing/#13115-sanity-testing","title":"1.3.1.15. Sanity Testing","text":"<p>Sanity testing, also known as quick checking or subset testing, is a type of software testing that is performed to ensure that the application or system is stable and functional enough for further testing. The purpose of sanity testing is to quickly evaluate whether the software is working as expected and whether there are any major defects or issues that need to be addressed before proceeding with more comprehensive testing.</p> <p>Sanity testing is typically performed after a major change or update to the software or after a series of tests have been run on the software. The process involves executing a set of basic and critical test cases that cover the core functionality of the application. These test cases are designed to identify any major issues or defects that might prevent further testing from being carried out.</p>"},{"location":"articles/software-testing/#13116-smoke-testing","title":"1.3.1.16. Smoke Testing","text":"<p>Smoke testing, also known as build verification testing, is a type of software testing that is used to ensure that the basic and critical functionalities of an application or system are working as expected after a new build or release. The purpose of smoke testing is to verify that the application is stable and functional enough to proceed with further testing.</p> <p>Smoke testing is usually performed after a new build or release of software is created. The process involves executing a series of pre-defined test cases that cover the most important and critical features of the application. These test cases are designed to identify any major issues or defects that might prevent further testing from being carried out.</p>"},{"location":"articles/software-testing/#13117-exploratory-testing","title":"1.3.1.17. Exploratory Testing","text":"<p>Exploratory testing is a type of software testing that involves simultaneous test design, execution, and learning. In other words, the tester explores the software product in an unstructured way, experimenting with different inputs, outputs, and configurations, to uncover potential issues and gain a deeper understanding of the software's behavior and capabilities.</p> <p>Key characteristics of exploratory testing:</p> <ul> <li> <p>Creativity and intuition</p> <p>Exploratory testing requires creativity and intuition on the part of the tester, who must be able to identify potential issues and devise new tests on the fly.</p> </li> <li> <p>Ad-hoc testing</p> <p>Exploratory testing is an ad-hoc approach to testing, meaning that there is no predefined test plan or test case.</p> </li> <li> <p>Test design and execution</p> <p>In exploratory testing, test design and execution are performed simultaneously, with the tester creating and executing tests as they explore the software.</p> </li> <li> <p>Learning and discovery</p> <p>Exploratory testing is focused on learning and discovery, with the tester seeking to uncover potential issues and gain a deeper understanding of the software's behavior and capabilities.</p> </li> <li> <p>Documentation</p> <p>Although exploratory testing is an ad-hoc approach, it is still important to document the tests that are performed and any issues that are uncovered, in order to aid in future testing and development efforts.</p> </li> </ul>"},{"location":"articles/software-testing/#13118-black-box-testing","title":"1.3.1.18. Black Box Testing","text":"<p>Black box testing is a type of software testing where the tester does not have access to the internal workings of the system or application being tested. In black box testing, the tester focuses solely on the inputs and outputs of the system, without any knowledge of the underlying code, algorithms, or architecture.</p> <p>Key characteristics of black box testing:</p> <ul> <li> <p>No knowledge of internal code</p> <p>The tester has no knowledge of the internal workings of the software being tested. They only know what the software is supposed to do and how it should behave.</p> </li> <li> <p>Emphasis on requirements and specifications</p> <p>Black box testing is focused on ensuring that the software meets the specified requirements and adheres to the stated specifications.</p> </li> <li> <p>Inputs and outputs</p> <p>The tester focuses on the inputs to the software and the corresponding outputs, verifying that the outputs are correct for the given inputs.</p> </li> <li> <p>Multiple test scenarios</p> <p>Black box testing involves testing the software with multiple test scenarios, including normal and abnormal inputs and usage patterns.</p> </li> <li> <p>Verification of functionality</p> <p>Black box testing is focused on verifying the functionality of the software, including its ability to handle errors and exceptions.</p> </li> <li> <p>User-centric approach</p> <p>Black box testing is focused on ensuring that the software meets the needs and expectations of the end user, without any bias towards the internal workings of the software.</p> </li> </ul>"},{"location":"articles/software-testing/#13119-white-box-testing","title":"1.3.1.19. White Box Testing","text":"<p>White Box Testing is a type of software testing that involves examining the internal workings of a software application to ensure that it is functioning as intended. Unlike black box testing, which only evaluates the application's external behavior, white box testing involves examining the application's source code, architecture, and design to ensure that it meets the necessary quality standards.</p> <p>Key characteristics of white box testing:</p> <ul> <li> <p>Code-based testing</p> <p>White box testing is code-based, meaning that the tester has access to the application's source code and can examine it for potential issues.</p> </li> <li> <p>Knowledge of the system</p> <p>White box testing requires a deep understanding of the system architecture and design, as well as the programming languages and technologies used to build the application.</p> </li> <li> <p>Techniques and methods</p> <p>White box testing uses a variety of techniques and methods to evaluate the software, including unit testing, integration testing, and code reviews.</p> </li> <li> <p>Focus on code coverage</p> <p>White box testing is often focused on achieving a high level of code coverage, meaning that all parts of the application's source code are tested to ensure that they are functioning correctly.</p> </li> <li> <p>Debugging and troubleshooting</p> <p>White box testing is often used for debugging and troubleshooting, as it allows testers to identify and isolate potential issues at the code level.</p> </li> </ul>"},{"location":"articles/software-testing/#13120-gray-box-testing","title":"1.3.1.20. Gray Box Testing","text":"<p>Gray box testing is a type of software testing that combines elements of both black box and white box testing. In gray box testing, the tester has partial knowledge of the internal workings of the system or application being tested, but does not have full access to the source code or underlying architecture.</p> <p>Key characteristics of gray box testing:</p> <ul> <li> <p>Partial knowledge</p> <p>The tester has some knowledge of the internal workings of the system, such as the design and architecture, but not complete knowledge of the code.</p> </li> <li> <p>Test design and execution</p> <p>Gray box testing involves both test design and execution, with the tester using their partial knowledge to create more effective tests.</p> </li> <li> <p>Combination of techniques</p> <p>Gray box testing combines techniques from both black box and white box testing, leveraging the advantages of each to create a more comprehensive testing approach.</p> </li> <li> <p>Focused testing</p> <p>Gray box testing is typically more focused than black box testing, as the tester has some knowledge of the internal workings of the system and can target specific areas for testing.</p> </li> </ul>"},{"location":"articles/software-testing/#13121-ab-testing","title":"1.3.1.21. A/B Testing","text":"<p>A/B testing is a type of testing that involves comparing two different versions of a product or service to determine which one performs better in terms of user engagement or other key metrics. In A/B testing, a group of users is randomly assigned to one of two groups, with each group being presented with a different version of the product or service. The performance of each version is then measured and compared to determine which one is more effective.</p> <p>Key characteristics of A/B testing:</p> <ul> <li> <p>Randomization</p> <p>A/B testing requires randomization to ensure that the results are not biased by factors such as user demographics or behavior.</p> </li> <li> <p>Control and treatment groups</p> <p>A/B testing involves the use of control and treatment groups, with one group being presented with the current version of the product or service (control) and the other group being presented with a modified version (treatment).</p> </li> <li> <p>Key metrics</p> <p>A/B testing is focused on measuring key metrics such as click-through rates, conversion rates, or engagement rates to determine which version of the product or service is more effective.</p> </li> <li> <p>Statistical significance</p> <p>A/B testing requires statistical significance to ensure that the differences between the two versions are not due to chance.</p> </li> </ul> <p>Iterative testing</p> <p>A/B testing is often conducted iteratively, with multiple rounds of testing and refinement to improve the performance of the product or service over time.</p>"},{"location":"articles/software-testing/#13122-monkey-testing","title":"1.3.1.22. Monkey Testing","text":"<p>Monkey testing is a type of software testing that involves randomly generating inputs and feeding them to an application, with the goal of causing it to crash or produce unexpected behavior.</p> <p>The term <code>monkey</code> refers to the idea that the inputs are generated randomly, like a monkey typing on a keyboard.</p> <p>Key characteristics of monkey testing:</p> <ul> <li> <p>Randomness</p> <p>Monkey testing is random in nature, with inputs generated without any specific test plan or test case.</p> </li> <li> <p>Automated</p> <p>Monkey testing is typically automated, with software tools generating the inputs and monitoring the application for crashes or errors.</p> </li> <li> <p>Exploratory</p> <p>Monkey testing is exploratory, with the goal of uncovering potential issues or weaknesses in the application.</p> </li> <li> <p>Chaos Engineering</p> <p>Monkey testing is sometimes considered a form of chaos engineering, which involves intentionally injecting failures into a system to test its resiliency and ability to recover.</p> </li> </ul>"},{"location":"articles/software-testing/#132-non-functional-testing","title":"1.3.2. Non-Functional Testing","text":"<p>Non-functional testing is a type of software testing that focuses on testing the non-functional aspects of the software application, such as performance, scalability, reliability, usability, and security. The purpose of non-functional testing is to ensure that the system or application meets the non-functional requirements and performance goals specified in the project requirements.</p>"},{"location":"articles/software-testing/#1321-security-testing","title":"1.3.2.1. Security Testing","text":"<p>Security testing is a type of software testing that is focused on evaluating the security of an application or system. The goal of security testing is to identify any vulnerabilities or weaknesses in the system and to determine whether the system is protected against potential attacks or threats.</p>"},{"location":"articles/software-testing/#13211-penetration-testing","title":"1.3.2.1.1. Penetration Testing","text":"<p>This involves simulating a real-world attack on the system to identify any vulnerabilities or weaknesses that could be exploited by attackers.</p>"},{"location":"articles/software-testing/#13212-vulnerability-scanning","title":"1.3.2.1.2. Vulnerability Scanning","text":"<p>Identifying potential vulnerabilities within the software. This involves using automated tools and manual techniques to scan the software for known vulnerabilities and weaknesses. It helps in identifying common security issues such as outdated software, misconfigurations, and insecure coding practices.</p>"},{"location":"articles/software-testing/#1322-performance-testing","title":"1.3.2.2. Performance Testing","text":"<p>Performance testing is a type of non-functional testing that is focused on evaluating how well a system or application performs under a specific workload or user load. The goal of performance testing is to identify any performance-related issues or bottlenecks in the system and to determine whether the system can handle the expected user load.</p>"},{"location":"articles/software-testing/#13221-load-testing","title":"1.3.2.2.1. Load Testing","text":"<p>Load testing is a type of software testing that evaluates the system's ability to handle a specific load or volume of users, requests, or transactions. The goal of load testing is to identify performance bottlenecks and other issues related to system scalability and stability under high load conditions.</p>"},{"location":"articles/software-testing/#13222-stress-testing","title":"1.3.2.2.2. Stress Testing","text":"<p>This involves testing the system beyond its normal operational capacity to determine its breaking point or to identify potential failure points.</p>"},{"location":"articles/software-testing/#13223-endurance-testing","title":"1.3.2.2.3. Endurance Testing","text":"<p>This involves testing the system under a sustained workload to evaluate its ability to handle the load over an extended period.</p>"},{"location":"articles/software-testing/#13224-spike-testing","title":"1.3.2.2.4. Spike Testing","text":"<p>This involves testing the system's ability to handle sudden and large increases in user traffic.</p>"},{"location":"articles/software-testing/#13225-volume-testing","title":"1.3.2.2.5. Volume Testing","text":"<p>This involves testing the system's ability to handle large volumes of data.</p>"},{"location":"articles/software-testing/#13226-scalability-testing","title":"1.3.2.2.6. Scalability Testing","text":"<p>This involves testing the system's ability to scale up or down to handle changing user loads or resource demands.</p>"},{"location":"articles/software-testing/#13227-capacity-testing","title":"1.3.2.2.7. Capacity Testing","text":"<p>This involves testing the system's ability to handle a specific number of users or transactions within a defined period.</p>"},{"location":"articles/software-testing/#1323-application-security-testing","title":"1.3.2.3. Application Security Testing","text":"<p>Application security testing is a type of software testing that focuses on identifying and addressing security vulnerabilities and weaknesses in an application. The goal of application security testing is to ensure that an application is secure from potential attacks and threats, such as data breaches, unauthorized access, or theft.</p>"},{"location":"articles/software-testing/#13231-static-application-security-testing","title":"1.3.2.3.1. Static Application Security Testing","text":"<p>Static Application Security Testing (SAST) is a type of application security testing that analyzes the source code of an application to detect security vulnerabilities and coding errors. SAST tools scan the application's source code or compiled binary code to identify potential security flaws, such as SQL injection, cross-site scripting (XSS), buffer overflows, and other types of vulnerabilities.</p> <p>Features of SAST:</p> <ul> <li> <p>Code Analysis</p> <p>SAST tools analyze the source code of an application to identify security vulnerabilities and coding errors.</p> </li> <li> <p>Early Detection</p> <p>SAST testing can detect security flaws early in the development process, before the application is deployed to production.</p> </li> <li> <p>Customizable Rules</p> <p>SAST tools allow users to customize the rules and policies used to detect security flaws, based on their specific requirements.</p> </li> <li> <p>Integration</p> <p>SAST tools can be integrated into the software development life cycle (SDLC) to automate security testing and ensure that security vulnerabilities are identified and addressed during development.</p> </li> <li> <p>False Positives</p> <p>SAST tools can generate false positives, which are security issues reported by the tool that are not actually vulnerabilities.</p> </li> </ul>"},{"location":"articles/software-testing/#13232-dynamic-application-security-testing","title":"1.3.2.3.2. Dynamic Application Security Testing","text":"<p>Dynamic Application Security Testing (DAST) is a type of application security testing that evaluates the security of an application in its running state. Unlike Static Application Security Testing (SAST), which analyzes the application's source code for vulnerabilities, DAST simulates attacks on the running application and identifies potential vulnerabilities in its runtime behavior.</p> <p>DAST tools typically use automated scanners to simulate various types of attacks, such as injection attacks, cross-site scripting (XSS) attacks, and SQL injection attacks. The tools send malicious input to the application, monitor its behavior, and report any vulnerabilities detected.</p> <p>Features of DAST:</p> <ul> <li> <p>Scanning</p> <p>DAST tools scan the application for vulnerabilities while it is running.</p> </li> <li> <p>Black-box testing</p> <p>DAST is a black-box testing technique because it does not require access to the application's source code or internal workings.</p> </li> <li> <p>Automated testing</p> <p>DAST tools use automated scanners to identify potential vulnerabilities.</p> </li> <li> <p>Real-world simulation</p> <p>DAST simulates real-world attack scenarios to identify potential vulnerabilities in the application's runtime behavior.</p> </li> <li> <p>External testing</p> <p>DAST is typically performed by external security testers, rather than by the development team.</p> </li> </ul> <p>Popular DAST tools include OWASP ZAP, Burp Suite, and Acunetix.</p>"},{"location":"articles/software-testing/#13233-interactive-application-security-testing","title":"1.3.2.3.3. Interactive Application Security Testing","text":"<p>Interactive Application Security Testing (IAST) is a type of application security testing that combines aspects of both SAST and DAST. Unlike SAST and DAST, IAST provides real-time feedback on security vulnerabilities while the application is running, allowing for faster identification and remediation of security issues.</p> <p>IAST works by inserting sensors or agents into the application code, which monitor the application's behavior and identify potential security vulnerabilities. These sensors can detect vulnerabilities such as SQL injection, cross-site scripting, and buffer overflows, among others.</p> <p>Features of IAST:</p> <ul> <li> <p>Real-time feedback</p> <p>IAST provides real-time feedback on security vulnerabilities, allowing developers to quickly identify and remediate issues.</p> </li> <li> <p>Accurate results</p> <p>Since IAST monitors the application's behavior while it is running, it can provide more accurate results than SAST or DAST alone.</p> </li> <li> <p>Minimal false positives</p> <p>IAST can provide more targeted results, resulting in fewer false positives than other types of testing.</p> </li> <li> <p>Integration with development tools</p> <p>IAST can integrate with development tools, such as IDEs and build servers, to provide developers with easy access to security feedback.</p> </li> </ul>"},{"location":"articles/software-testing/#2-principle","title":"2. Principle","text":"<p>By following these fundamental principles of software testing, testers can design and execute their testing activities more effectively, identify defects early, and improve the quality of the software application.</p> <ul> <li> <p>Early Testing</p> <p>Finding and fixing defects early in the software development lifecycle is less costly and time-consuming than identifying and fixing them later. Therefore, testing should be started as early as possible in the development process.</p> </li> <li> <p>Exhaustiveness</p> <p>It is practically impossible to test all possible scenarios and combinations of inputs and conditions. Therefore, testers must focus on prioritizing their testing efforts based on the risks and impact of defects.</p> </li> <li> <p>Independence</p> <p>Tests should be independent of each other, meaning that changes or failures in one test should not affect the results of other tests.</p> </li> <li> <p>Repeatability</p> <p>Tests should be repeatable, meaning that the same test cases should produce the same results every time they are executed.</p> </li> <li> <p>Automation</p> <p>Testing should be automated to the extent possible, in order to improve the efficiency and accuracy of the testing process.</p> </li> <li> <p>Defect Tracking</p> <p>A process should be in place to track and manage defects, ensuring that they are identified, documented, and resolved in a timely manner.</p> </li> <li> <p>Continuous Improvement</p> <p>The testing process should be continually reviewed and improved, with the goal of increasing its efficiency and effectiveness over time.</p> </li> <li> <p>Risk-based Approach</p> <p>Tests should be prioritized based on the level of risk associated with the software, with the most critical or high-risk areas being tested first.</p> </li> <li> <p>Early Feedback</p> <p>Feedback should be provided as early as possible, so that the development team can address any issues or defects in a timely manner.</p> </li> <li> <p>Involvement of Stakeholders</p> <p>All stakeholders, including developers, testers, customers, and end users, should be involved in the testing process, in order to ensure that the software meets the needs and expectations of all stakeholders.</p> </li> <li> <p>Verifiability</p> <p>The results of testing should be verifiable, meaning that they can be easily reproduced and validated.</p> </li> <li> <p>Traceability</p> <p>The relationship between tests and requirements should be traceable, allowing test results to be easily linked to specific requirements.</p> </li> <li> <p>Accuracy</p> <p>Tests should be accurate and consistent, ensuring that they produce reliable and repeatable results.</p> </li> <li> <p>Maintainability</p> <p>Tests should be easy to maintain and update, allowing for changes in the software to be easily incorporated into the testing process.</p> </li> <li> <p>Reusability</p> <p>Tests should be reusable, allowing for the same tests to be executed for multiple releases or versions of the software.</p> </li> <li> <p>Cost-effectiveness</p> <p>The testing process should be cost-effective, balancing the cost of testing against the potential costs associated with defects or failures in the software.</p> </li> <li> <p>Efficiency</p> <p>The testing process should be efficient, maximizing the number of tests that can be executed in a given time frame.</p> </li> <li> <p>Adaptability</p> <p>The testing process should be adaptable, allowing for changes in the software or testing environment to be easily incorporated into the testing process.Collaboration The testing process should be a collaborative effort, involving the contributions of multiple stakeholders, including developers, testers, and end users.</p> </li> <li> <p>Continuous Learning</p> <p>The testing process should be an opportunity for continuous learning and improvement, allowing for lessons learned to be incorporated into future testing efforts.</p> </li> </ul>"},{"location":"articles/software-testing/#3-best-practice","title":"3. Best Practice","text":"<p>By following these best practices in software testing, testers can improve the quality of the software application, identify defects early, and reduce the cost and time of testing.</p> <ul> <li> <p>Develop a comprehensive test plan</p> <p>A comprehensive test plan should be developed and documented, outlining the objectives, scope, and approach of the testing process.</p> </li> <li> <p>Define clear and measurable goals</p> <p>Clear and measurable goals should be established for the testing process, such as identifying and resolving a specific number of defects within a given time frame.</p> </li> <li> <p>Implement automated testing</p> <p>Automated testing should be implemented to the extent possible, in order to improve the efficiency and accuracy of the testing process.</p> </li> <li> <p>Use version control for test cases</p> <p>Test cases should be managed using version control, allowing for changes in the software or testing environment to be easily incorporated into the testing process.</p> </li> <li> <p>Prioritize high-risk areas</p> <p>High-risk areas of the software should be prioritized for testing, ensuring that the most critical or high-risk areas are tested first.</p> </li> <li> <p>Involve end users in testing</p> <p>End users should be involved in the testing process, allowing for their perspectives and feedback to be incorporated into the testing process.</p> </li> <li> <p>Perform regular code reviews</p> <p>Regular code reviews should be performed, in order to identify and resolve issues early in the development process.</p> </li> <li> <p>Use test data management tools</p> <p>Test data management tools should be used to manage and organize test data, ensuring that the testing process is efficient and repeatable.</p> </li> <li> <p>Implement continuous testing</p> <p>Continuous testing should be implemented, allowing for the testing process to be integrated into the development process and executed automatically on a regular basis.</p> </li> <li> <p>Monitor and evaluate test results</p> <p>Test results should be monitored and evaluated, in order to identify areas for improvement and make changes to the testing process as needed.</p> </li> <li> <p>Adhere to industry standards and guidelines</p> <p>Industry standards and guidelines should be followed, ensuring that the testing process is consistent and follows best practices.</p> </li> <li> <p>Integrate security testing into the testing process</p> <p>Security testing should be integrated into the testing process, in order to identify and resolve security vulnerabilities.</p> </li> <li> <p>Establish clear communication channels</p> <p>Clear communication channels should be established between all stakeholders involved in the testing process, including developers, testers, and end users.</p> </li> <li> <p>Invest in training and development</p> <p>Investment in training and development should be made, in order to ensure that testers have the necessary skills and knowledge to effectively perform their tasks.</p> </li> <li> <p>Use test management tools</p> <p>Test management tools should be used to manage and track the testing process, including test cases, test plans, and test results.</p> </li> <li> <p>Perform exploratory testing</p> <p>Exploratory testing should be performed, in order to identify potential issues or areas for improvement that may not be easily detected through automated testing.</p> </li> <li> <p>Involve stakeholders in the testing process</p> <p>Stakeholders should be involved in the testing process, including business stakeholders, project managers, and product owners.</p> </li> <li> <p>Encourage collaboration and teamwork</p> <p>Collaboration and teamwork should be encouraged, allowing for the testing process to be a collaborative effort between all stakeholders.</p> </li> <li> <p>Perform regular retrospectives</p> <p>Regular retrospectives should be performed, in order to evaluate the testing process and identify areas for improvement.</p> </li> <li> <p>Stay Up-to-Date</p> <p>Stay up-to-date with emerging technologies and trends, in order to continuously improve the testing process and keep up with the changing landscape of software development.</p> </li> </ul>"},{"location":"articles/software-testing/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Test Suite</p> <p>A collection of test cases that are executed together as part of a testing cycle.</p> </li> <li> <p>Test Case</p> <p>A set of steps and conditions designed to test a specific functionality, feature, scenario or aspect of a software application.</p> </li> <li> <p>Test Plan</p> <p>A detailed document that outlines the testing process, including the scope, objectives, approach and resources required for the testing.</p> </li> <li> <p>Test Harness</p> <p>A test harness is a set of tools and resources used to execute test cases and analyze the results.</p> </li> <li> <p>Test Data</p> <p>Test data is the input used in testing, including the data used to set up test cases and the data used to verify the results.</p> </li> <li> <p>Test Environment</p> <p>A the environment in which testing is performed, including the hardware, software, and other resources needed to execute the testing process.</p> </li> <li> <p>Test Driver</p> <p>A program or tool used to execute test cases and analyze the results.</p> </li> <li> <p>Test Results</p> <p>The outputs of the testing process, including pass/fail results, performance metrics, and other relevant data.</p> </li> <li> <p>Test Metrics</p> <p>Test metrics are measures used to evaluate the quality and effectiveness of the testing process, including defect density, test case coverage, and test case execution time.</p> </li> <li> <p>Debugging</p> <p>Debugging is the process of identifying and fixing defects in a software application.</p> </li> <li> <p>Defect</p> <p>An issue or problem found during testing that affects the functionality, usability, or performance of the software application.</p> </li> <li> <p>Test Execution</p> <p>The process of running test cases and recording the results.</p> </li> <li> <p>Test Automation</p> <p>The process of using software tools and scripts to automate the execution of test cases.</p> </li> <li> <p>Test Report</p> <p>A document that summarizes the testing activities and results and provides an evaluation of the software application's quality.</p> </li> <li> <p>Test Strategy</p> <p>A document that outlines the overall approach, objectives, and scope of testing activities for a project.</p> </li> </ul>"},{"location":"articles/static-site-generators/","title":"Static Site Generators","text":"<p>Static site generators (SSGs) are tools that generate static HTML, CSS, and JavaScript files from templates and content files.</p> <ul> <li>1. Category</li> <li>1.1. SSG Types<ul> <li>1.1.1. File-based SSGs</li> <li>1.1.2. Database-driven SSGs</li> <li>1.1.3. Hybrid SSGs</li> <li>1.1.4. Headless CMS-based SSGs</li> </ul> </li> <li>1.2. Hosting Services</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>3.1. SSG</li> <li>3.2. Deployment</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/static-site-generators/#1-category","title":"1. Category","text":""},{"location":"articles/static-site-generators/#11-ssg-types","title":"1.1. SSG Types","text":""},{"location":"articles/static-site-generators/#111-file-based-ssgs","title":"1.1.1. File-based SSGs","text":"<p>File-based SSGs are static site generators that use text files as the source of content and generate HTML files by processing these files using templates. The text files typically contain metadata and content written in a markup language like Markdown. The SSG processes the text files, applies the appropriate templates, and generates the static HTML files.</p> <p>File-based SSGs are suitable for small websites and blogs, where content is relatively simple and straightforward. They are easy to use, have a low learning curve, and are generally faster and more efficient than CMS-based solutions.</p> <p>File-based SSGs tools:</p> <ul> <li> <p>Jekyll</p> <p>A simple, blog-aware, static site generator that converts Markdown and other formats into HTML and is widely used for GitHub Pages sites.</p> </li> <li> <p>Hugo</p> <p>A fast and modern static site generator written in Go and can handle large websites with ease.</p> </li> <li> <p>Hexo</p> <p>A fast and simple static site generator that supports plugins and themes, and is often used for blogs.</p> </li> </ul>"},{"location":"articles/static-site-generators/#112-database-driven-ssgs","title":"1.1.2. Database-driven SSGs","text":"<p>Database-driven SSGs are static site generators that use a database to store content and generate HTML files by querying the database and processing the results using templates. These SSGs can handle large websites with complex content structures and are suitable for websites that require dynamic content.</p> <p>Database-driven SSGs offer several advantages over file-based SSGs, such as the ability to store complex data structures, handle large amounts of data, and support dynamic content. They can also provide faster build times and easier content management.</p> <p>Database-driven SSGs tools:</p> <ul> <li> <p>Gatsby</p> <p>A React-based static site generator that uses GraphQL to query data from various sources, such as Markdown, JSON, and APIs, and supports Progressive Web App (PWA) development.</p> </li> <li> <p>Next.js</p> <p>A popular React framework that can be used as a static site generator, and supports server-side rendering, dynamic routing, and API routes.</p> </li> </ul>"},{"location":"articles/static-site-generators/#113-hybrid-ssgs","title":"1.1.3. Hybrid SSGs","text":"<p>Hybrid SSGs are static site generators that combine features of file-based and database-driven SSGs. They use a database to store content and metadata, but also allow for custom templates and static files. These SSGs are suitable for websites that require more flexibility than a file-based SSG can provide, but don't need the full complexity of a database-driven SSG.</p> <p>Hybrid SSGs offer the benefits of both file-based and database-driven SSGs, such as simplicity, flexibility, and speed.</p> <p>Hybrid SSGs tools:</p> <ul> <li> <p>Eleventy</p> <p>A simple and flexible SSG that supports multiple templating languages and data sources, and can be used with or without a CMS.</p> </li> <li> <p>Middleman</p> <p>A Ruby-based SSG that supports a variety of data sources, templating languages, and asset management.</p> </li> </ul>"},{"location":"articles/static-site-generators/#114-headless-cms-based-ssgs","title":"1.1.4. Headless CMS-based SSGs","text":"<p>Headless CMS-based SSGs are static site generators that use a headless CMS to manage content and generate static HTML files. A headless CMS is a content management system that provides an API to retrieve content, but doesn't provide any front-end or presentation layer. The SSG retrieves the content via the API, processes it using templates, and generates the static HTML files.</p> <p>Headless CMS-based SSGs offer several advantages, such as the ability to manage content more easily, support multiple channels, and provide a more robust content management system.</p> <p>Headless CMS-based SSGs tools:</p> <ul> <li> <p>Contentful</p> <p>A popular headless CMS that provides a content API and webhooks, and can be integrated with various front-end technologies.</p> </li> <li> <p>Sanity</p> <p>A flexible headless CMS that allows developers to define content models using a schema, and provides a real-time collaborative environment for content editors.</p> </li> <li> <p>Strapi</p> <p>A self-hosted headless CMS that provides an admin panel to manage content, and can be used with various front-end technologies.</p> </li> </ul>"},{"location":"articles/static-site-generators/#12-hosting-services","title":"1.2. Hosting Services","text":"<p>Static Site Generators (SSGs) generate static HTML, CSS, and JavaScript files, which can be hosted on any web server that supports serving static files.</p> <ul> <li> <p>GitHub Pages</p> <p>GitHub Pages is a free hosting service that allows to host static websites and web apps directly from your GitHub repositories. It supports several SSGs, including Jekyll, Hugo, and Gatsby.</p> </li> <li> <p>Netlify</p> <p>Netlify is a cloud hosting platform that provides a variety of features, including continuous deployment, custom domains, HTTPS, and serverless functions. It supports several SSGs, including Gatsby, Hugo, and Jekyll.</p> </li> <li> <p>Amazon S3</p> <p>Amazon S3 is a cloud storage service that can be used to host static websites. It provides features such as high availability, scalability, and low latency. To host a static site on S3, you need to configure your S3 bucket for static website hosting.</p> </li> <li> <p>Google Cloud Storage</p> <p>Google Cloud Storage is another cloud storage service that can be used to host static websites. It provides features such as global distribution, high availability, and scalability. To host a static site on Google Cloud Storage, you need to create a new bucket and configure it for static website hosting.</p> </li> <li> <p>Firebase Hosting</p> <p>Firebase Hosting is a fast and secure hosting service that supports static websites and web apps. It provides features such as automatic SSL, CDN, and custom domains. To host a static site on Firebase Hosting, you need to create a new Firebase project and deploy your site using the Firebase CLI.</p> </li> <li> <p>GitLab Pages</p> <p>GitLab Pages is a free hosting service that allows to host static websites and web apps directly from your GitLab repositories. It supports several SSGs, including Jekyll, Hugo, and Middleman.</p> </li> <li> <p>Surge</p> <p>Surge is a simple and affordable hosting service for static sites. It provides features such as custom domains, SSL, and command-line deployment. To host a static site on Surge, you need to install the Surge CLI and deploy your site using the surge command.</p> </li> <li> <p>Cloudflare Pages</p> <p>Cloudflare Pages is a modern hosting service that offers features such as automatic SSL, CDN, and custom domains. It supports several SSGs, including Hugo, Gatsby, and Next.js. Cloudflare Pages offers a generous free tier and scalable pricing based on usage.</p> </li> <li> <p>Vercel</p> <p>Vercel is a cloud hosting platform that provides features such as continuous deployment, custom domains, SSL, and serverless functions. It supports several SSGs, including Gatsby, Next.js, and Hugo. Vercel offers a generous free tier and scalable pricing based on usage.</p> </li> </ul>"},{"location":"articles/static-site-generators/#2-principle","title":"2. Principle","text":"<ul> <li> <p>Separation of content and presentation</p> <p>SSGs separate the content and presentation layers of a website or web application. Content is stored separately from the website's layout and design, allowing developers to focus on the website's structure and functionality without worrying about the content.</p> </li> <li> <p>Build Process</p> <p>SSGs use a build process to generate static files from templates and content sources. The build process involves parsing the content sources, applying the templates to the content, and generating static HTML, CSS, and JavaScript files. The resulting files can then be deployed to a web server or a hosting service.</p> </li> <li> <p>Templating</p> <p>SSGs use templating languages, such as Liquid, Handlebars, or Mustache, to generate the HTML files. Templating allows developers to reuse code and generate dynamic content, such as blog posts or product pages, from a single template.</p> </li> <li> <p>Version control</p> <p>SSGs work well with version control systems, such as Git, allowing developers to track changes to the website's code and content and collaborate with other developers.</p> </li> <li> <p>Flexibility</p> <p>SSGs are highly flexible and can be used for a wide range of websites and web applications, from simple blogs to complex e-commerce sites. SSGs can also be combined with other web development tools, such as CSS preprocessors or JavaScript frameworks, to enhance the website's functionality and design.</p> </li> <li> <p>Content Sources</p> <p>SSGs support various content sources, such as Markdown files, YAML files, JSON files, and databases. Content sources can be stored locally or remotely, and can be accessed using APIs or plugins.</p> </li> <li> <p>Plugins and Extensions</p> <p>SSGs support plugins and extensions that extend their functionality and allow developers to add custom features. Plugins can be used to integrate with third-party services, such as Google Analytics and Disqus, and add functionality, such as search and forms.</p> </li> <li> <p>Performance and Security</p> <p>SSGs generate static files that can be served directly from a web server or a CDN, which provides better performance and security than dynamic websites. SSGs also support HTTPS encryption and other security features to protect user data.</p> </li> </ul>"},{"location":"articles/static-site-generators/#3-best-practice","title":"3. Best Practice","text":""},{"location":"articles/static-site-generators/#31-ssg","title":"3.1. SSG","text":"<p>Static Site Generators (SSGs) are a type of web development tool that generate static HTML, CSS, and JavaScript files for websites or web applications.</p> <ul> <li> <p>Choose the right SSG</p> <p>Different SSGs have different strengths and weaknesses. Consider the requirements of the website or web application and choose an SSG that best fits those requirements.</p> </li> <li> <p>Use a version control system</p> <p>Use a version control system like Git to track changes to your site's code and content. This allows to collaborate with others, roll back changes if necessary, and easily deploy updates.</p> </li> <li> <p>Use a local development environment</p> <p>Develop the website or web application locally using a development server. This allows developers to test changes and ensure the website is working as expected before deploying it to a live server.</p> </li> <li> <p>Optimize images and other assets</p> <p>Use tools, such as ImageOptim or TinyPNG, to optimize images and other assets before including them in the website. This reduces file size and improves load times.</p> </li> <li> <p>Use caching</p> <p>Use caching to reduce server load and improve website performance. Caching stores frequently accessed data, such as images or stylesheets, on the client's browser, reducing the need for server-side processing.</p> </li> <li> <p>Implement responsive design</p> <p>Implement responsive design to ensure the website is accessible and usable on a wide range of devices and screen sizes.</p> </li> <li> <p>Use HTTPS</p> <p>Use HTTPS encryption to protect sensitive data, such as passwords or credit card information, transmitted between the client and server.</p> </li> <li> <p>Implement SEO best practices</p> <p>Implement search engine optimization (SEO) best practices, such as using descriptive page titles and meta descriptions, to improve the website's visibility in search engine results.</p> </li> <li> <p>Monitor website analytics</p> <p>Monitor website analytics, such as page views and bounce rates, to track the website's performance and identify areas for improvement.</p> </li> <li> <p>Use a CSS preprocessor</p> <p>Use a CSS preprocessor like Sass or Less to write more efficient and organized CSS code. This allows to use variables, mixins, and other features to make your CSS code more modular and reusable.</p> </li> <li> <p>Use a task runner</p> <p>Use a task runner like Gulp or Grunt to automate repetitive tasks, such as minification and concatenation of CSS and JavaScript files, image optimization, and deployment.</p> </li> <li> <p>Use a CDN</p> <p>Use a content delivery network (CDN) to improve site performance and reduce server load. A CDN caches your site's files on servers located around the world, reducing the time it takes for users to access your site.</p> </li> <li> <p>Minifying files</p> <p>Minifying HTML, CSS, and JavaScript files can reduce file size and improve site performance.</p> </li> </ul>"},{"location":"articles/static-site-generators/#32-deployment","title":"3.2. Deployment","text":"<p>Deployment is the process of publishing a website or web application to a web server or hosting platform so that it can be accessed by users. In the case of Static Site Generators, deployment typically involves uploading pre-built HTML, CSS, and JavaScript files to a web server or CDN.</p> <ul> <li> <p>Build the site</p> <p>Use the SSG to generate a build of your static site by compiling templates, generating pages and assets, and optimizing files for performance.</p> </li> <li> <p>Choose a hosting platform</p> <p>There are many hosting platforms available for static sites, including cloud storage providers, content delivery networks (CDNs), and specialized static site hosting services.</p> </li> <li> <p>Upload the files</p> <p>Once a hosting platform chosen, upload the generated files to the platform using a file transfer protocol (FTP) client or the hosting platform's web-based interface.</p> </li> <li> <p>Configure the domain</p> <p>If you are hosting the site on your own domain, you will need to configure your domain's DNS settings to point to the hosting platform's servers.</p> </li> <li> <p>Test the site</p> <p>Once the files have been uploaded and the domain has been configured, test the site to ensure that it is working correctly.</p> </li> <li> <p>Enable HTTPS</p> <p>HTTPS is important for security and SEO, so it is recommended that you enable HTTPS on your static site. Many hosting platforms provide a way to enable HTTPS automatically or with minimal configuration.</p> </li> <li> <p>Configure caching</p> <p>Caching can improve site performance by reducing the amount of data that needs to be downloaded by visitors' browsers. Many hosting platforms provide built-in caching options or integration with CDNs to improve site performance.</p> </li> </ul>"},{"location":"articles/static-site-generators/#4-terminology","title":"4. Terminology","text":"<ul> <li> <p>Template</p> <p>A template is a file that defines the layout and structure of a website or web application. Templates typically use a markup language, such as HTML or Markdown, and may include variables and logic for generating dynamic content.</p> </li> <li> <p>Static site</p> <p>A static site is a website that consists of pre-built HTML, CSS, and JavaScript files that are served directly from a web server, without the need for server-side processing or database queries.</p> </li> <li> <p>Content management system (CMS)</p> <p>A CMS is a software application that allows users to create, manage, and publish digital content, such as blog posts or product descriptions. In the context of SSGs, a headless CMS can be used to manage content separately from the website's layout and design.</p> </li> <li> <p>Front-end framework</p> <p>A front-end framework is a collection of pre-built HTML, CSS, and JavaScript components and tools that can be used to create responsive and dynamic web interfaces. Popular front-end frameworks include React, Vue.js, and Angular.</p> </li> <li> <p>Build process</p> <p>The build process refers to the process of generating static files from source code and content using an SSG. The build process typically involves compiling templates, generating pages and assets, and optimizing files for performance.</p> </li> <li> <p>Plugin</p> <p>A plugin is a piece of software that can be added to an SSG to extend its functionality. Plugins can be used to add new features, optimize site performance, or automate tasks.</p> </li> <li> <p>Deployment</p> <p>Deployment refers to the process of uploading a website or web application to a web server or hosting platform so that it can be accessed by users. Deploying a static site typically involves uploading pre-built HTML, CSS, and JavaScript files to a web server or CDN.</p> </li> <li> <p>Front matter</p> <p>Front matter is metadata that is stored at the beginning of a Markdown file or other content file in an SSG. Front matter can include variables such as the title, author, or date of a blog post, which can be used to generate dynamic content.</p> </li> <li> <p>Server-side rendering (SSR)</p> <p>Server-side rendering is a process used in some web frameworks and platforms that generates HTML on the server before sending it to the client's browser. SSR can be used to improve site performance and SEO, but is not typically used in SSGs.</p> </li> <li> <p>CDN</p> <p>A content delivery network (CDN) is a network of servers located around the world that can be used to distribute content, such as images, CSS files, and JavaScript files, to users more quickly and efficiently. Using a CDN can improve site performance and reduce server load.</p> </li> <li> <p>Minification</p> <p>Minification is the process of removing unnecessary characters, such as whitespace and comments, from HTML, CSS, and JavaScript files. Minification can reduce file size and improve site performance.</p> </li> <li> <p>Incremental build</p> <p>Incremental build is a feature available in some SSGs that allows for faster site rebuilds by only updating files that have changed since the last build, instead of rebuilding the entire site.</p> </li> </ul>"},{"location":"articles/static-site-generators/#5-references","title":"5. References","text":"<ul> <li>Sentenz\u00a0docs\u00a0as\u00a0code\u00a0article.</li> </ul>"},{"location":"articles/tabs-vs-spaces/","title":"Tabs vs Spaces","text":"<p>In the context of programming, tabs and spaces are used to represent white space and are used to format code.</p> <p>Spaces are used to indent lines of code to indicate block structure and improve readability. Tabs are used for the same purpose but take up less space.</p> <p>The preference for tabs or spaces is a matter of personal preference and coding style, however, it is important to be consistent within a project. Most modern text editors and IDEs allow the user to choose between tabs or spaces, and can automatically convert tabs to spaces and vice versa.</p> <ul> <li>1. Tabs</li> <li>2. Spaces</li> <li>3. Conclusion</li> <li>4. References</li> </ul>"},{"location":"articles/tabs-vs-spaces/#1-tabs","title":"1. Tabs","text":"<p>Pros of using Tabs:</p> <ul> <li> <p>Saves space in the file, as a single tab character is typically equivalent to multiple spaces.</p> </li> <li> <p>Allows for faster navigation within a file as tab key moves to the next tab stop.</p> </li> <li> <p>Consistency in indentation is easier to maintain, as tabs can be automatically adjusted to fit a specified tab size.</p> </li> <li> <p>Increases project accessibility for all types of developers, including those with visual impairments, as tabs can be read by screen readers more easily than spaces.</p> </li> </ul> <p>Cons of using Tabs:</p> <ul> <li> <p>Different text editors and systems may display tabs differently, causing indentation to look inconsistent when viewed on different setups.</p> </li> <li> <p>Tabs can be difficult to adjust if the tab size is changed, leading to incorrect indentation and a lack of consistency in the code.</p> </li> </ul>"},{"location":"articles/tabs-vs-spaces/#2-spaces","title":"2. Spaces","text":"<p>Pros of using Spaces:</p> <ul> <li> <p>Spaces are more portable across different systems and text editors, as different systems may interpret tabs differently.</p> </li> <li> <p>Spaces ensure consistent indentation regardless of the editor's tab size setting.</p> </li> <li> <p>Easier to read and make changes to code, especially for new developers who may not be familiar with the project's tab size setting.</p> </li> </ul> <p>Cons of using Spaces:</p> <ul> <li> <p>Code files can become larger and take up more space, as spaces are typically represented by multiple characters.</p> </li> <li> <p>Inconsistent indentation can occur if spaces are manually added or removed, as it can be easy to accidentally add or remove extra spaces.</p> </li> </ul>"},{"location":"articles/tabs-vs-spaces/#3-conclusion","title":"3. Conclusion","text":"<p>Either tabs or spaces can be used for indentation and white space in code, but it is important to choose one and use it consistently within a project. The consideration of accessibility for developers with visual impairments is an important factor that may influence the choice between tabs and spaces.</p>"},{"location":"articles/tabs-vs-spaces/#4-references","title":"4. References","text":"<ul> <li>JetBrain tabs vs spaces article.</li> <li>Reddit\u00a0tabs\u00a0vs\u00a0spaces\u00a0article. </li> <li>GitHub\u00a0tabs\u00a0vs\u00a0spaces\u00a0article.</li> </ul>"},{"location":"articles/technical-dept/","title":"Technical Dept","text":"<p>Technical debt refers to the accumulated consequences of shortcuts, suboptimal solutions, and compromises made during the development process of software. Technical debt can lead to increased complexity, reduced maintainability, decreased productivity, and higher costs in the future.</p> <ul> <li>1. Category</li> <li>1.1. Debt Types<ul> <li>1.1.1. Design Debt</li> <li>1.1.2. Code Debt</li> <li>1.1.3. Test Debt</li> <li>1.1.4. Documentation Debt</li> <li>1.1.5. Infrastructure Debt</li> </ul> </li> <li>1.2. Debt Metrics</li> <li>1.3. Debt Impact</li> <li>2. Principle</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"articles/technical-dept/#1-category","title":"1. Category","text":""},{"location":"articles/technical-dept/#11-debt-types","title":"1.1. Debt Types","text":"<p>Technical debt can arise from various factors, including time constraints, lack of resources, inadequate planning, evolving requirements, and pressure to deliver quickly. It may result from deferred refactoring, incomplete documentation, poor design choices, duplicated code, outdated libraries, or the accumulation of bugs and issues.</p>"},{"location":"articles/technical-dept/#111-design-debt","title":"1.1.1. Design Debt","text":"<p>Design debt refers to the consequences of making design decisions that prioritize short-term gains or expedience over long-term maintainability and scalability. It occurs when design flaws, trade-offs, or compromises are made during the software development process, resulting in a system that becomes increasingly difficult to extend, modify, or maintain.</p> <p>Actively addressing design debt, software projects enhance maintainability, extensibility, and scalability, reducing</p> <p>Types of Design Debt:</p> <ol> <li> <p>Poor Modularization and Coupling</p> <p>Design debt can accumulate when modules or components are tightly coupled or have unclear boundaries. This makes it challenging to understand and modify individual parts of the system without impacting others. Tight coupling hinders code reuse, increases the risk of unintended consequences, and makes it difficult to isolate and fix bugs.</p> </li> <li> <p>Violation of Design Principles</p> <p>Design debt arises when design principles such as SOLID are not followed. Violations of these principles can lead to code that is difficult to understand, modify, or test.</p> </li> <li> <p>Lack of Abstraction and Modularity</p> <p>When design fails to properly abstract concepts and encapsulate functionality, design debt is incurred. Lack of abstraction and modularity can lead to code duplication, increased complexity, and difficulties in making changes or introducing new features.</p> </li> <li> <p>Inefficient Data Structures or Algorithms</p> <p>Design debt can occur when inefficient data structures or algorithms are chosen, resulting in performance issues or scalability limitations. Inadequate design choices may hinder the system's ability to handle larger datasets, increasing response times or resource usage.</p> </li> <li> <p>Inadequate Error Handling and Fault Tolerance</p> <p>When error handling and fault tolerance mechanisms are not adequately considered during the design phase, design debt is accumulated. Insufficient error handling can lead to unexpected system failures, data corruption, or compromised security.</p> </li> <li> <p>Inconsistent or Incomplete APIs</p> <p>Design debt can arise when APIs are inconsistent, poorly documented, or lack clear and intuitive interfaces. Inconsistent APIs make it difficult for developers to understand and use the system correctly, leading to errors, decreased productivity, and increased learning curves.</p> </li> </ol> <p>Handling of Design Debt:</p> <ol> <li> <p>Refactoring</p> <p>Allocate time and resources for refactoring efforts to improve the system's design. Refactoring involves restructuring code and design to improve modularity, reduce coupling, and enhance code quality without changing its external behavior.</p> </li> <li> <p>Design Principles</p> <p>Emphasize the importance of adhering to design principles such as SOLID, DRY , and KISS. Educate developers about these principles and encourage their application during the design and development process.</p> </li> <li> <p>Design Patterns and Architectural Patterns</p> <p>Promote the use of design patterns and architectural patterns that encourage modularity, extensibility, and scalability. Applying well-established patterns and architectural patterns can help mitigate design debt and improve system maintainability.</p> </li> <li> <p>Design Reviews</p> <p>Conduct regular design reviews to identify potential design debt and gather feedback from experienced developers or architects. Design reviews can help catch design flaws early and ensure alignment with best practices.</p> </li> <li> <p>Documentation and Knowledge Sharing</p> <p>Document design decisions, architectural diagrams, and rationale to facilitate knowledge sharing and ensure a common understanding among the development team. Clear and up-to-date documentation helps new team members understand the system's design and make informed modifications.</p> </li> <li> <p>Continuous Improvement</p> <p>Foster a culture of continuous improvement, where design debt is actively recognized, prioritized, and addressed. Encourage developers to identify and communicate design debt issues, and provide the necessary resources and support to tackle them effectively.</p> </li> </ol> <p>Example of Design Debt in Python:</p> <pre><code># Example of poorly written code with unclear variable names and lack of comments\n\ndef calc(x, y):\n    z = x + y\n    a = z * 2\n    if a &gt; 10:\n        return \"High\"\n    else:\n        return \"Low\"\n</code></pre> <p>The code snippet represents code debt, specifically poorly written code. It lacks clear variable names and meaningful comments, making it difficult to understand the purpose and functionality of the code. Poorly written code can lead to maintenance challenges, bugs, and reduced code readability.</p>"},{"location":"articles/technical-dept/#112-code-debt","title":"1.1.2. Code Debt","text":"<p>Code debt refers to the accumulation of suboptimal or inefficient code that compromises the quality, maintainability, and extensibility of a software project. It arises when shortcuts, quick fixes, or trade-offs are made during the development process, often to meet tight deadlines or deliver new features quickly. Code debt can result in increased complexity, decreased productivity, higher maintenance costs, and difficulties in introducing future enhancements or modifications.</p> <p>Actively addressing code debt and prioritizing code quality, software projects can improve maintainability, reduce bugs and regressions, enhance developer productivity, and reduce the cost of future development efforts.</p> <p>Types of Code Debt:</p> <ol> <li> <p>Poor Code Structure and Organization</p> <p>Code debt can accumulate when code lacks proper organization, modularization, or adherence to coding standards. This can make it difficult to understand, maintain, and extend the codebase over time.</p> </li> <li> <p>Code Duplication</p> <p>Repetition of code blocks or functions within the codebase indicates code debt. Duplicated code increases the chances of bugs, requires redundant maintenance, and can result in inconsistencies when changes need to be made.</p> </li> <li> <p>Insufficient or Inconsistent Error Handling</p> <p>Neglecting proper error handling can lead to code debt. This includes not handling exceptions or errors adequately, not providing meaningful error messages, or having inconsistent error-handling practices across the codebase.</p> </li> <li> <p>Lack of Code Comments and Documentation</p> <p>Inadequate or missing code comments and documentation contribute to code debt. Well-documented code helps other developers understand the purpose, usage, and intricacies of the code. Lack of documentation can slow down onboarding, increase the learning curve, and impede code maintenance.</p> </li> <li> <p>Complex and Unmaintainable Code</p> <p>Code that is overly complex, convoluted, or difficult to understand is a sign of code debt. Complex code can be challenging to debug, modify, and test, leading to longer development cycles and increased risks of introducing new bugs.</p> </li> <li> <p>Inefficient Algorithms or Data Structures</p> <p>Code debt can arise from using inefficient algorithms or data structures. This can result in suboptimal performance, scalability issues, and limitations in handling larger datasets or increased user loads.</p> </li> </ol> <p>Handling of Code Debt:</p> <ol> <li> <p>Refactoring</p> <p>Allocate time for refactoring code to improve its structure, readability, and maintainability. Refactoring involves restructuring code without changing its external behavior, removing duplication, and applying better design patterns.</p> </li> <li> <p>Code Reviews</p> <p>Encourage regular code reviews to identify code debt issues and provide constructive feedback to improve code quality. Code reviews help catch issues early, ensure adherence to coding standards, and facilitate knowledge sharing among team members.</p> </li> <li> <p>Automated Testing</p> <p>Invest in automated testing to detect and prevent code debt. Test-driven development (TDD) practices can help ensure that code is thoroughly tested and validated, reducing the likelihood of introducing new issues during development.</p> </li> <li> <p>Coding Standards and Guidelines</p> <p>Establish coding standards and guidelines to promote consistent coding practices. Consistency in coding style, naming conventions, and code structure helps make the codebase more readable, maintainable, and less prone to errors.</p> </li> <li> <p>Documentation</p> <p>Maintain up-to-date and comprehensive documentation to facilitate code understanding and knowledge sharing. Documenting code functionality, usage, and important considerations helps new team members quickly grasp the codebase and promotes efficient collaboration.</p> </li> <li> <p>Continuous Pipelines</p> <p>Implement CI/CD practices to automate the build, test, and deployment processes. CI/CD pipelines enforce quality checks and ensure that code debt is not introduced during the integration and deployment stages.</p> </li> </ol> <p>Example of Code Debt in Java:</p> <pre><code>// Example of code with poor modularity and tightly coupled components\n\npublic class Customer {\n    private Order order;\n\n    public void placeOrder(Product product) {\n        // Code for placing the order\n    }\n\n    public void calculateTotalPrice() {\n        // Code for calculating the total price using order details\n    }\n\n    public void sendConfirmationEmail() {\n        // Code for sending the confirmation email to the customer\n    }\n\n    // Other methods...\n}\n</code></pre> <p>The code snippet illustrates design debt, specifically a lack of modularity. The <code>Customer</code> class is responsible for various tasks such as placing an order, calculating the total price, and sending a confirmation email. This lack of separation of concerns and tight coupling between different responsibilities can result in code that is harder to maintain, understand, and extend.</p>"},{"location":"articles/technical-dept/#113-test-debt","title":"1.1.3. Test Debt","text":"<p>Test debt refers to the accumulation of issues and shortcomings in the testing process of a software project. It occurs when proper testing practices are neglected or compromised, leading to inadequate test coverage, unreliable test results, and delayed bug detection. Test debt can have significant implications on software quality, product stability, and the efficiency of the development process.</p> <p>Types of Test Debt:</p> <ol> <li> <p>Insufficient Test Coverage</p> <p>Test debt can accumulate when the test coverage is incomplete or inadequate. This means that certain parts of the code or specific scenarios are not adequately tested, increasing the risk of undetected bugs and regressions.</p> </li> <li> <p>Lack of Automated Tests</p> <p>Manual testing is time-consuming, error-prone, and difficult to scale. If automated testing is not prioritized or implemented, test debt can accrue. Manual testing cannot match the speed, accuracy, and repeatability of automated tests, leading to longer testing cycles and reduced efficiency.</p> </li> <li> <p>Ineffective Test Suites</p> <p>Test suites that are poorly designed, maintainable, or organized can contribute to test debt. This includes tests that are flaky, unreliable, or difficult to debug, making it harder to identify and isolate issues when they arise.</p> </li> <li> <p>Limited Performance and Load Testing</p> <p>Neglecting performance and load testing can result in performance-related issues going unnoticed until they impact the end users. Inadequate performance testing may lead to scalability problems, bottlenecks, or unexpected resource consumption.</p> </li> <li> <p>Lack of Integration and End-to-End Testing</p> <p>If integration testing and end-to-end testing are not properly performed, test debt can accumulate. This can result in undetected issues arising from the interaction between different components or subsystems of the software.</p> </li> <li> <p>Incomplete Test Documentation</p> <p>Poorly documented test cases and test plans can hinder collaboration and knowledge sharing among team members. Lack of clear documentation makes it difficult to understand test coverage, reproduce test scenarios, and track the progress of testing efforts.</p> </li> </ol> <p>Handling of Test Debts:</p> <ol> <li> <p>Test-Driven Development (TDD)</p> <p>Adopt TDD practices to ensure that tests are written before the code is implemented. This helps in building a comprehensive test suite and ensures that tests are an integral part of the development process.</p> </li> <li> <p>Test Automation</p> <p>Prioritize the automation of tests to improve test efficiency, reduce manual effort, and enhance test coverage. Automated tests enable quick feedback loops, facilitate regression testing, and increase the overall reliability of the testing process.</p> </li> <li> <p>Continuous Integration and Continuous Testing</p> <p>Implement continuous integration (CI) and continuous testing practices to ensure that tests are executed consistently and frequently as part of the development pipeline. This helps identify issues early and ensures a quick feedback loop for developers.</p> </li> <li> <p>Improve Test Coverage</p> <p>Analyze the codebase to identify areas of low test coverage and prioritize efforts to improve coverage. Focus on critical functionalities, high-risk areas, and scenarios that are prone to errors or regressions.</p> </li> <li> <p>Enhance Test Suites</p> <p>Regularly review and refactor test suites to improve maintainability, reliability, and readability. Eliminate flaky tests, update outdated test cases, and ensure proper test data management.</p> </li> <li> <p>Performance and Load Testing</p> <p>Incorporate performance and load testing into the testing process to identify performance bottlenecks, scalability issues, and other performance-related problems. Simulate real-world usage scenarios to validate the software's behavior under various load conditions.</p> </li> <li> <p>Document Test Cases and Plans</p> <p>Maintain up-to-date and well-documented test cases, test plans, and test reports. Clear documentation helps in understanding the test coverage, reproducing test scenarios, and tracking the progress of testing efforts.</p> </li> </ol> <p>Example of Test Debt in Go:</p> <pre><code>// Inadequate testing\nfunc divide(a, b int) int {\n    return a / b\n}\n</code></pre> <p>The code snippet demonstrates lacks proper testing. The <code>divide</code> function performs integer division but does not handle the case of dividing by zero. Without adequate tests to cover different scenarios, such as division by zero or handling edge cases, the code may produce unexpected results or panic at runtime.</p>"},{"location":"articles/technical-dept/#114-documentation-debt","title":"1.1.4. Documentation Debt","text":"<p>Documentation debt is the technical debt that arises from not documenting the code, architecture, or system design. It results from choosing to prioritize functionality over documentation or neglecting to update documentation as the software changes. Documentation debt can have negative impacts on the project's maintainability, usability, and collaboration among team members.</p> <p>Actively addressing documentation debt and maintaining high-quality documentation, software projects can improve usability, reduce support requests, enhance collaboration among team members, and facilitate a smoother onboarding process for new developers or users.</p> <p>Types of Documentation Debt:</p> <ol> <li> <p>Outdated Documentation</p> <p>When documentation is not regularly updated, it becomes outdated and loses its relevance. Outdated documentation can mislead developers, users, or other stakeholders, leading to confusion, wasted effort, and potential errors.</p> </li> <li> <p>Incomplete or Inaccurate Documentation</p> <p>Documentation debt can arise when important information is missing or incomplete. Inadequate documentation makes it difficult for developers or users to understand the software's features, functionality, or intended usage. Inaccurate documentation can lead to incorrect implementation or usage of the software.</p> </li> <li> <p>Lack of Tutorials or Examples</p> <p>Insufficient tutorials or examples can hinder developers' ability to learn and utilize the software effectively. Well-documented examples and tutorials provide practical guidance and demonstrate how to use the software in real-world scenarios.</p> </li> <li> <p>Poorly Organized Documentation</p> <p>Documentation that is disorganized, difficult to navigate, or lacks a clear structure can contribute to documentation debt. If finding relevant information or understanding the documentation's structure becomes challenging, developers may avoid using the documentation, leading to a loss of valuable insights and guidance.</p> </li> <li> <p>Inconsistent or Incoherent Documentation</p> <p>Documentation debt can arise from inconsistencies in terminology, style, or formatting across different sections or documents. Incoherent documentation makes it harder for users or developers to understand and follow the information provided.</p> </li> <li> <p>Lack of Maintenance Documentation</p> <p>When maintenance-related information, such as troubleshooting guides, release notes, or upgrade instructions, is not properly documented, documentation debt can accumulate. This can lead to difficulties in maintaining or upgrading the software and result in longer resolution times for issues.</p> </li> </ol> <p>Handling of Documentation Debt:</p> <ol> <li> <p>Establish Documentation Standards</p> <p>Define documentation standards, guidelines, and templates to ensure consistency and clarity across all documentation. This includes formatting, terminology, structure, and style guidelines.</p> </li> <li> <p>Regularly Update and Review Documentation</p> <p>Allocate time and resources to review and update documentation regularly. Set up a process to ensure that outdated or inaccurate information is corrected, and new features or changes are documented promptly.</p> </li> <li> <p>Provide Clear and Comprehensive Examples</p> <p>Develop clear and comprehensive examples and use cases to illustrate the software's functionality and best practices. These examples should cover common scenarios and provide guidance on how to use different features effectively.</p> </li> <li> <p>Improve Documentation Structure and Navigation</p> <p>Organize documentation in a logical structure, with clear headings, subheadings, and a table of contents. Implement a search functionality or index to enable easy navigation and searching within the documentation.</p> </li> <li> <p>Document Maintenance and Troubleshooting</p> <p>Ensure that documentation includes maintenance-related information, troubleshooting guides, release notes, and upgrade instructions. This helps users and developers handle common issues, understand changes, and perform necessary maintenance tasks.</p> </li> <li> <p>Gather Feedback</p> <p>Actively seek feedback from users, developers, or other stakeholders to identify areas for improvement in documentation. Incorporate feedback to address any gaps or issues and continually enhance the documentation quality.</p> </li> </ol> <p>Example of Documentation Debt in Python:</p> <pre><code># Example of code with insufficient documentation\n\ndef calculate_factorial(n):\n    \"\"\"\n    Function to calculate the factorial of a given number.\n\n    Parameters:\n    - n: The number for which factorial needs to be calculated.\n\n    Returns:\n    - The factorial of the given number.\n    \"\"\"\n    result = 1\n    for i in range(1, n+1):\n        result *= i\n    return result\n</code></pre> <p>The code snippet represents documentation debt, specifically insufficient documentation. While the code itself calculates the factorial of a number, the lack of detailed explanations and descriptions within the code comments makes it challenging for other developers to understand the purpose and usage of the function. Insufficient documentation can hinder collaboration, maintenance, and knowledge sharing among team members.</p>"},{"location":"articles/technical-dept/#115-infrastructure-debt","title":"1.1.5. Infrastructure Debt","text":"<p>Infrastructure debt refers to the accumulation of issues and shortcomings in the underlying infrastructure and technical foundations of a software system. It occurs when the infrastructure is not properly maintained, lacks necessary upgrades, or does not scale to meet the growing demands of the application. Infrastructure debt can have significant implications on system stability, performance, security, and the ability to support future growth.</p> <p>Addressing infrastructure debt involves proactively investing in regular infrastructure maintenance, upgrades, and modernization efforts. It requires monitoring the technology stack, staying up to date with the latest releases and security patches, implementing automation, and adopting best practices for infrastructure management. By actively managing infrastructure debt, organizations can ensure a stable, scalable, and secure environment for their software systems.</p> <p>Actively addressing infrastructure debt and investing in the maintenance, scalability, and resilience of the underlying infrastructure, software projects can enhance system stability, performance, and security while enabling future growth and expansion.</p> <p>Types of Infrastructure Debt:</p> <ol> <li> <p>Outdated Hardware or Operating Systems</p> <p>Using outdated or aging hardware components can result in slower performance, limited processing power, and increased risk of hardware failures. Outdated operating systems may lack security patches or support for newer software components.</p> </li> <li> <p>Unsupported Software Versions</p> <p>Running software components on outdated or unsupported versions can lead to security vulnerabilities and compatibility issues. Infrastructure debt can accumulate when operating systems, web servers, databases, or other software components are not regularly updated. It is crucial to apply patches, security updates, and upgrades to ensure a secure and stable infrastructure.</p> </li> <li> <p>Inefficient Network Configuration</p> <p>Poor network design or configuration can result in bottlenecks, latency, or unreliable connectivity. Infrastructure debt can accumulate if network equipment, such as switches or routers, is not properly configured or if network capacity is not scaled to meet the system's demands. Optimizing network infrastructure can improve performance, reduce downtime, and enhance user experience.</p> </li> <li> <p>Inadequate Data Storage and Management</p> <p>Insufficient or outdated database systems can lead to performance issues, data integrity problems, and difficulty in scaling. Infrastructure debt may arise from using outdated database versions, inefficient query patterns, or lack of proper indexing. Upgrading to newer database versions, optimizing queries, and implementing efficient data storage strategies can help address this debt.</p> </li> <li> <p>Manual and Error-Prone Deployment Processes</p> <p>Infrastructure debt can accumulate when deployment processes are manual, error-prone, or lack automation. Relying on manual steps, inconsistent configurations, or manual environment setup can lead to deployment failures, longer release cycles, and increased risk of errors. Investing in automated deployment tools, configuration management systems, and infrastructure-as-code practices can help reduce this debt.</p> </li> <li> <p>Inadequate Monitoring and Alerting</p> <p>Neglecting proper monitoring and alerting mechanisms can lead to undetected issues, longer resolution times, and reduced system reliability. Infrastructure debt can accumulate if monitoring tools are not implemented or if alerting systems are not configured to notify appropriate stakeholders in case of failures or performance degradation. Establishing robust monitoring and alerting practices ensures timely identification and resolution of issues.</p> </li> <li> <p>Lack of Disaster Recovery and Backup Mechanisms</p> <p>If adequate disaster recovery and backup mechanisms are not in place, infrastructure debt can accumulate. Insufficient backup strategies increase the risk of data loss and system downtime in the event of failures or disasters.</p> </li> </ol> <p>Handling of Infrastructure Debt:</p> <ol> <li> <p>Regular Infrastructure Upgrades</p> <p>Establish a plan for regular infrastructure upgrades, including hardware, operating systems, and other critical components. Keep track of vendor updates, security patches, and performance improvements to ensure the infrastructure remains up-to-date.</p> </li> <li> <p>Scalability Planning and Architecture</p> <p>Design the infrastructure with scalability in mind. Evaluate the anticipated growth and implement appropriate scalability mechanisms, such as load balancing, horizontal scaling, or cloud-based infrastructure services.</p> </li> <li> <p>Monitoring and Alerting Systems</p> <p>Implement robust monitoring and alerting systems to proactively detect and respond to infrastructure issues. Set up alerts for critical metrics, establish performance baselines, and implement automated notifications to enable timely resolution of problems.</p> </li> <li> <p>Automation and Infrastructure as Code (IaC)</p> <p>Embrace infrastructure automation and Infrastructure as Code practices to ensure consistent and repeatable infrastructure provisioning, configuration, and management. Automation reduces the risk of manual errors, improves consistency, and enables version control of infrastructure configurations.</p> </li> <li> <p>Disaster Recovery and Backup Strategies</p> <p>Develop and implement comprehensive disaster recovery and backup strategies. Regularly test the recovery mechanisms to ensure data integrity and system availability in the event of failures or disasters.</p> </li> <li> <p>Configuration Management Tools</p> <p>Adopt configuration management tools to automate and standardize infrastructure configuration and provisioning. These tools help enforce consistency, enable efficient change management, and simplify the deployment of infrastructure updates.</p> </li> <li> <p>Performance Optimization</p> <p>Continually monitor and optimize the infrastructure's performance. Identify potential bottlenecks, tune configurations, and leverage caching or optimization techniques to maximize resource utilization and improve overall system performance.</p> </li> </ol> <p>Example of Infrastructure Debt in Ruby:</p> <pre><code># Example of code using outdated or deprecated libraries\n\nrequire 'sinatra'\nrequire 'mysql'\n\n# Code using the old MySQL library, which is no longer supported\ndb = Mysql.connect('localhost', 'user', 'password', 'database')\n\nget '/' do\n  # Code for handling requests\nend\n</code></pre> <p>The code snippet exemplifies infrastructure debt, specifically using outdated or unsupported technology. The code imports the deprecated MySQL library, which may no longer receive updates or support. Relying on outdated technologies can introduce security vulnerabilities, compatibility issues, and difficulty in integrating with modern systems. It is essential to keep the technology stack up to date to leverage improvements and avoid potential risks.</p>"},{"location":"articles/technical-dept/#12-debt-metrics","title":"1.2. Debt Metrics","text":"<p>Keeping dashboards with code health metrics. These can range from dashboards that show test coverage or the number of <code>TODO</code> tags, to more sophisticated dashboards including metrics like cyclomatic complexity or maintainability index.</p> <ul> <li> <p>Bugs</p> <p>Software developers should count and track their bugs. This includes both fixed and unfixed bugs. Noting the unfixed bugs allows development teams to focus on them and fix them during their agile iterations. Noting the fixed bugs helps teams measure how effective their tech debt management process is. Bugs on a weekly or monthly basis indicates a quality decline of the codebase.</p> </li> <li> <p>Code Quality</p> <p>Code complexity can really damage the development team and the organization as a whole. Code complexity metrics are Cyclomatic complexity, maintainability index, Class coupling, Lines of code, Depth of Inheritance. The lower each of these measures, the better. These metrics also helps organisations know exactly which code to rework or refactor in order to reduce complexity and improve the backend side of the software.</p> </li> <li> <p>Code Cohesion</p> <p>A high code cohesion usually means that the code is more maintainable, reusable, and robust. It also minimises the amount of people who need to get involved in the code, which can greatly reduce complexity and decrease the chances of bit rot. High cohesion is when you have a class that does a well defined job.</p> </li> <li> <p>Code Coverage</p> <p>A declining percentage in code coverage percentage and code coverage per feature is a signal for a growing tech debt.</p> </li> <li> <p>Continuous Pipelines</p> <p>An increasing number of failed continuous pipelines is a strong indicator for instability in the codebase. This can be related to design and code debt.</p> </li> <li> <p>Feature Throughput</p> <p>Days it takes to push a new feature to the main branch. It\u2019s a supporting metric that can indicate growing tech debt. Not each feature has the same size, but if you see a decline over multiple weeks, it\u2019s time to intervene.</p> </li> <li> <p>Issues with Non-functional Requirements</p> <p>Measuring metrics such as application performance, UX (increasingly difficult to use), or loss of compatibility are solid indicators for increased technical debt.</p> </li> </ul>"},{"location":"articles/technical-dept/#13-debt-impact","title":"1.3. Debt Impact","text":"<p>Technical debt can have significant impacts on software development projects and the businesses that rely on them. Effective management of technical debt is critical to minimizing these impacts and ensuring that software development projects remain on track, on budget, and deliver value to customers.</p> <ul> <li> <p>Increased development time and costs</p> <p>Technical debt can increase the time and costs associated with software development projects because developers may need to spend additional time refactoring code, resolving issues, and addressing technical debt items.</p> </li> <li> <p>Reduced Code Quality</p> <p>Technical debt can result in reduced code quality, making the codebase more difficult to understand, read, and maintain. This can lead to increased bugs, lower productivity, and decreased developer morale.</p> </li> <li> <p>Increased Maintenance Expenses</p> <p>Technical debt can result in increased maintenance expenses because the codebase may be more difficult to modify, extend, or test. This can increase the time and costs associated with fixing bugs and implementing new features.</p> </li> <li> <p>Decreased System Performance</p> <p>Technical debt can negatively impact system performance because suboptimal code may be less efficient and consume more system resources. This can result in slower system performance and decreased user satisfaction.</p> </li> <li> <p>Decreased Innovation</p> <p>Technical debt can limit innovation because developers may be forced to spend more time addressing technical debt items instead of working on new features and functionality.</p> </li> <li> <p>Increased Business Risk</p> <p>Technical debt can increase business risk because it can result in system downtime, security vulnerabilities, and decreased customer satisfaction. These risks can have significant financial and reputational impacts on businesses.</p> </li> </ul> <p>Technical debt can have a significant impact on developer psychology, affecting their motivation, job satisfaction, and overall well-being. Effective management of technical debt is critical for ensuring that developers can perform their job effectively and maintain a healthy work-life balance.</p> <ul> <li> <p>Frustration and Demotivation</p> <p>Dealing with technical debt can be frustrating and demotivating for developers. Technical debt can make it difficult to achieve project goals, lead to increased rework and debugging, and result in a lack of progress. This can lead to frustration and demotivation among developers, making it difficult for them to stay engaged and productive.</p> </li> <li> <p>Burnout</p> <p>Technical debt can contribute to developer burnout, a state of emotional, physical, and mental exhaustion caused by prolonged stress and overwork. Developers may feel overwhelmed and unsupported when dealing with technical debt, leading to burnout and a decrease in overall job satisfaction.</p> </li> <li> <p>Imposter Syndrome</p> <p>Developers may experience imposter syndrome, a feeling of inadequacy and self-doubt, when dealing with technical debt. Technical debt can make developers feel like they are not doing their job effectively, leading to a lack of confidence and feelings of inadequacy.</p> </li> <li> <p>Quality of Work</p> <p>Technical debt can impact the quality of work produced by developers. When technical debt is not properly managed, developers may be forced to take shortcuts and make compromises in the quality of their work, leading to a decrease in code quality and technical debt accumulation over time.</p> </li> <li> <p>Team Dynamics</p> <p>Technical debt can also impact team dynamics, causing tension and conflict among team members. When technical debt is not properly managed, it can result in finger-pointing and blame-shifting, leading to a breakdown in communication and collaboration.</p> </li> </ul>"},{"location":"articles/technical-dept/#2-principle","title":"2. Principle","text":"<p>Managing technical debt is essential for ensuring that software remains maintainable, scalable, and efficient over time. Effective management of technical debt requires identifying technical debt items, prioritizing them, and allocating resources for their management. It also involves tracking technical debt levels, communicating technical debt management efforts to stakeholders, and proactively managing technical debt to prevent it from accumulating.</p> <ul> <li> <p>Recognize Technical Debt</p> <p>Technical debt must be recognized as a natural and inevitable part of software development. It is a result of making trade-offs between short-term and long-term goals, and it can occur at different stages of the software development life cycle. Recognizing technical debt helps teams to prioritize and address it more effectively.</p> </li> <li> <p>Manage Technical Debt Proactively</p> <p>Technical debt should be managed proactively to prevent it from accumulating and causing problems. This requires teams to identify and address technical debt items as early as possible, allocate resources for technical debt management, and continuously monitor and track technical debt levels.</p> </li> <li> <p>Understand the Cost of Technical Debt</p> <p>Technical debt has a cost that goes beyond the initial effort of developing software. The cost of technical debt can include increased development and maintenance costs, decreased agility and flexibility, and decreased customer satisfaction. Understanding the cost of technical debt helps teams to prioritize and justify technical debt management efforts.</p> </li> <li> <p>Prioritize Technical Debt</p> <p>Technical debt items should be prioritized based on their impact on the system and the cost of addressing them. Prioritization helps teams to focus on the most critical technical debt items and make informed decisions about technical debt management.</p> </li> <li> <p>Collaborate Across Teams</p> <p>Technical debt management is a collaborative effort that requires coordination across different teams and stakeholders. Collaboration helps teams to identify technical debt items more effectively, share knowledge and expertise, and align technical debt management efforts with business goals.</p> </li> <li> <p>Adopt Best Practices</p> <p>Technical debt can be reduced by adopting best practices for software development. Best practices include following coding standards, using modern technologies and tools, conducting code reviews, and investing in testing and automation.</p> </li> <li> <p>Balance Short-Term and Long-Term Goals</p> <p>Technical debt management requires a balance between short-term and long-term goals. While addressing technical debt may require additional effort and resources in the short term, it can lead to improved software quality, increased agility, and reduced costs in the long term.</p> </li> <li> <p>Prioritization</p> <p>Prioritizing technical debt items is critical for effective management. Technical debt items should be prioritized based on their impact on the system and the cost of addressing them. Prioritization helps teams to focus on the most critical technical debt items and make informed decisions about technical debt management.</p> </li> <li> <p>Communication</p> <p>Communication is critical for effective technical debt management. Technical debt management efforts should be communicated clearly to all stakeholders. Communication helps to build awareness of technical debt management, foster a culture of collaboration and continuous improvement, and align technical debt management efforts with business goals.</p> </li> <li> <p>Technical Debt Reduction Plan</p> <p>A technical debt reduction plan should be developed to identify technical debt items, prioritize them, and track progress in addressing them. The plan should be updated regularly to reflect changes in the system and to ensure that technical debt is being managed effectively.</p> </li> <li> <p>Training and Education</p> <p>Training and education can help to reduce technical debt by improving the skills and knowledge of developers. Training and education should be provided regularly to ensure that developers are aware of best practices and can apply them effectively.</p> </li> <li> <p>Proactive Management</p> <p>Proactive management is critical for effective technical debt management. Technical debt should be managed proactively to prevent it from accumulating and causing problems. This requires teams to identify and address technical debt items as early as possible, allocate resources for technical debt management, and continuously monitor and track technical debt levels.</p> </li> </ul>"},{"location":"articles/technical-dept/#3-best-practice","title":"3. Best Practice","text":"<p>Managing technical debt requires a proactive and continuous effort from the software development team.</p> <ul> <li> <p>Refactoring</p> <p>Refactoring is the process of improving the code's design without changing its behavior. Regular refactoring can help to reduce technical debt by improving the code's readability, maintainability, and performance. Refactoring should be performed regularly as part of the development process to ensure that technical debt does not accumulate.</p> </li> <li> <p>Code Review</p> <p>Code review is a practice where developers review each other's code to identify issues, suggest improvements, and ensure that the code follows best practices. Code review can help to identify technical debt items and prevent them from being introduced into the codebase. Code review should be performed regularly as part of the development process.</p> </li> <li> <p>Automated Testing</p> <p>Automated testing can help to  reduce technical debt by catching bugs and regressions early in the development process. Automated tests should be written for critical parts of the system and should be run regularly to ensure that the system behaves as expected.</p> </li> <li> <p>Documentation</p> <p>Documentation can help to reduce technical debt by making the code easier to understand and maintain. Documentation should be created for the system's architecture, design, and code, and should be kept up-to-date as the system evolves.</p> </li> <li> <p>Continuous Pipelines</p> <p>Continuous pipelines, such as continuous integration and deployment (CI/CD) can help to reduce technical debt by ensuring that changes are tested and deployed quickly and consistently. Continuous pipelines should be used to automate testing, building, and deploying the system, reducing the risk of errors and regressions.</p> </li> <li> <p>Version Control</p> <p>Version control systems can help to reduce technical debt by tracking changes to the codebase and making it easy to revert changes if necessary. Version control should be used to manage the codebase and to collaborate effectively with other developers.</p> </li> <li> <p>Agile Development</p> <p>Agile development methodologies can help to reduce technical debt by encouraging collaboration, continuous improvement, and rapid feedback. Agile development should be used to ensure that technical debt is managed effectively throughout the development process.</p> </li> <li> <p>Technical Debt Tracking</p> <p>Technical debt should be tracked regularly to ensure that it is being managed effectively. Technical debt tracking should include identifying technical debt items, prioritizing them, and tracking progress in addressing them.</p> </li> <li> <p>Metrics and Monitoring</p> <p>Metrics and monitoring can help to identify technical debt items and track progress in addressing them. Metrics such as code quality, test coverage, and technical debt levels should be monitored regularly to ensure that technical debt is being managed effectively.</p> </li> </ul>"},{"location":"articles/technical-dept/#4-terminology","title":"4. Terminology","text":"<p>By following the principles and best practices, and understanding the terminology associated with technical debt, software development teams can effectively manage technical debt and ensure that their codebase remains maintainable, scalable, and efficient over time.</p> <ul> <li> <p>Technical Debt</p> <p>Technical debt is a metaphor for the accumulated cost of additional work that arises when developers choose to take shortcuts or make trade-offs that result in less-than-optimal code quality.</p> </li> <li> <p>Technical Debt Interest</p> <p>Interest refers to the additional costs incurred over time due to technical debt. Like financial debt, technical debt incurs \"interest\" in the form of additional development costs, maintenance expenses, and other indirect costs.</p> </li> <li> <p>Technical Debt Interest Rate</p> <p>The technical debt interest rate is the rate at which technical debt incurs additional costs over time. The technical debt interest rate can be used to estimate the long-term costs of technical debt and to prioritize technical debt management efforts.</p> </li> <li> <p>Code Smell</p> <p>Code smells are indications that there may be technical debt in the codebase. Code smells are often subtle and can be difficult to detect, but they can indicate larger problems with the code's design, readability, or maintainability.</p> </li> <li> <p>Refactoring</p> <p>Refactoring is the process of improving the code's design without changing its behavior. Refactoring can help to reduce technical debt by improving the code's readability, maintainability, and performance.</p> </li> <li> <p>Clean Code</p> <p>Clean code refers to code that is easy to understand, read, and maintain. Clean code is free of code smells, follows best practices, and is well-organized and structured.</p> </li> <li> <p>Legacy Code</p> <p>Legacy code refers to code that is no longer actively developed or maintained. Legacy code is often difficult to modify, extend, or test, and can pose a significant technical debt challenge for development teams.</p> </li> <li> <p>Technical Debt Ratio</p> <p>The technical debt ratio is a metric that indicates the amount of technical debt in the codebase relative to the code's overall quality. The technical debt ratio is typically expressed as a percentage and can be used to track technical debt over time.</p> </li> <li> <p>Technical Debt Backlog</p> <p>The technical debt backlog is a list of technical debt items that need to be addressed in the codebase. The technical debt backlog is typically prioritized based on the impact of the technical debt item on the system and the cost of addressing it.</p> </li> </ul>"},{"location":"articles/technical-dept/#5-references","title":"5. References","text":"<ul> <li>Google technical debt book.</li> <li>Stepsize technical debt article.</li> <li>Stepsize tactics to prevent technical debt article.</li> <li>Stepsize tools to track technical debt article.</li> <li>Stepsize technical debt report 2021 article.</li> <li>Martin Fowler technical debt quadrant article.</li> </ul>"},{"location":"articles/twelve-factor-app/","title":"Twelve-Factor App","text":"<p>The Twelve-Factor App methodology is a software design and development methodology that outlines best practices for building scalable, maintainable, and cloud-native software applications. It provides a theoretical framework for designing and building software systems that can run and scale Software-as-a-Service (SaaS) applications in modern cloud environments.</p> <ul> <li>1. Category</li> <li>1.1. Codebase</li> <li>1.2. Dependencies</li> <li>1.3. Config</li> <li>1.4. Backing Services</li> <li>1.5. Building, Release, Run</li> <li>1.6. Processes</li> <li>1.7. Port Binding</li> <li>1.8. Concurrency</li> <li>1.9. Disposability</li> <li>1.10. Dev/Prod Parity</li> <li>1.11. Logs</li> <li>1.12. Admin Processes</li> <li>2. References</li> </ul>"},{"location":"articles/twelve-factor-app/#1-category","title":"1. Category","text":"<p>The Twelve-Factor App principles are technology-agnostic, meaning that they can be applied with any programming language, framework, or infrastructure.</p>"},{"location":"articles/twelve-factor-app/#11-codebase","title":"1.1. Codebase","text":"<p>The <code>Codebase</code> principle states that each application should have a single, version-controlled codebase that is used to deploy the application across all environments.</p> <p>In practice, this means that developers should use a version control system, such as Git or SVN, to manage the source code for their application. The codebase should include all of the application's code, dependencies, and configuration files, and should be used to deploy the application across all environments, from development to production.</p> <p>By using a single, version-controlled codebase, developers can more easily manage and maintain their applications, and can ensure that all changes to the application are tracked and versioned. This can also help to ensure that all environments are consistent, and can reduce the risk of issues that may arise from differences between environments.</p> <p>Benefits to using a single, version-controlled codebase:</p> <ul> <li> <p>Improved collaboration</p> <p>By using a version control system, developers can more easily collaborate on the application's codebase, and can track changes and revisions over time.</p> </li> <li> <p>Better versioning</p> <p>By versioning the application's codebase, developers can more easily manage and maintain different versions of the application, and can ensure that all changes are tracked and versioned.</p> </li> <li> <p>Easier deployment</p> <p>By using a single, version-controlled codebase, developers can more easily deploy the application across different environments, and can ensure that all environments are consistent and up-to-date.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#12-dependencies","title":"1.2. Dependencies","text":"<p>The <code>Dependencies</code> principle states that applications should explicitly declare and isolate their dependencies. In a modern software application, dependencies can include libraries, frameworks, and other third-party software components that your application relies on to function properly.</p> <p>In practice, this means that developers should define their application's dependencies in a manifest or configuration file, such as a <code>package.json</code> file in Node.js or a <code>requirements.txt</code> file in Python. Dependencies should be declared explicitly, including the version numbers of each dependency, and should be isolated from the application's runtime environment. This can be achieved using tools such as virtual environments, Docker containers, or other containerization technologies.</p> <p>By explicitly declaring and isolating dependencies, developers can ensure that their applications are consistent across all environments, and can avoid potential issues that may arise from differences in dependencies or dependency versions. This can also help to ensure that the application can be easily deployed and scaled across different environments, and can be easily updated or modified over time.</p> <p>Benefits to explicitly declaring and isolating dependencies:</p> <ul> <li> <p>Improved consistency</p> <p>By explicitly declaring and isolating dependencies, developers can ensure that their applications are consistent across all environments, and can avoid potential issues that may arise from differences in dependencies or dependency versions.</p> </li> <li> <p>Better scalability</p> <p>By isolating dependencies from the application's runtime environment, developers can more easily deploy and scale their applications across different environments, and can ensure that the application remains consistent and reliable.</p> </li> <li> <p>Easier maintenance</p> <p>By explicitly declaring dependencies, developers can more easily update or modify their applications over time, and can ensure that changes to dependencies do not impact the application's runtime environment.</p> </li> </ul> <p>NOTE See package managers for details.</p>"},{"location":"articles/twelve-factor-app/#13-config","title":"1.3. Config","text":"<p>The <code>Config</code> principle states that an application's configuration should be stored in environment variables, and should be separate from the application code.</p> <p>In practice, this means that developers should store any configuration information that may change between environments, such as database credentials or API keys, in environment variables rather than hard-coding them into the application code. This can be achieved using a configuration management tool, such as Puppet or Chef, or by using a platform-as-a-service (PaaS) provider, such as Heroku, that provides built-in support for environment variables.</p> <p>By storing configuration information in environment variables, developers can more easily manage and maintain their applications, and can ensure that the same application code can be used across different environments, without needing to modify the code for each environment. This can also help to ensure that sensitive information, such as passwords or API keys, are not hard-coded into the application code, and are instead stored securely in environment variables.</p> <p>Benefits to storing configuration information as environment variables:</p> <ul> <li> <p>Better portability</p> <p>By storing configuration information in environment variables, developers can more easily deploy their applications across different environments, without needing to modify the application code for each environment.</p> </li> <li> <p>Improved security</p> <p>By storing sensitive configuration information in environment variables, rather than hard-coding them into the application code, developers can improve the security of their applications and reduce the risk of security breaches. Further,  it is possible to restrict access to this information and ensure that it is not accidentally committed to source control or otherwise exposed.</p> </li> <li> <p>Higher scalability</p> <p>By separating configuration information from the application code, it is possible to scale the application horizontally by running multiple instances of the same codebase with different configuration settings.</p> </li> <li> <p>Easier maintenance</p> <p>By storing configuration information in environment variables, developers can more easily update or modify the application configuration over time, without needing to modify the application code.</p> </li> </ul> <p>NOTE Avoid storing sensitive information such as secrets and credentials in environment variables, as any process running on the same machine can access them and they can be accidentally exposed through logs or debugging messages. Instead use a secure secret manager such as Hashicorp Vault or Dapr to store sensitive information. However, in cases where environment variables are used, it is recommended to avoid global variables and instead use an .env file to store configuration information. The .env file should be kept secure and not accessible to unauthorized users.</p>"},{"location":"articles/twelve-factor-app/#14-backing-services","title":"1.4. Backing Services","text":"<p>The <code>Backing Services</code> principle states that any external resources that the application depends on, such as databases, message queues, or caching systems, should be treated as attached resources, and accessed via a well-defined API.</p> <p>In practice, this means that applications should not depend on specific instances of backing services, but rather should be designed to work with a variety of different providers, and to use service discovery and configuration management tools to locate and connect to the appropriate services.</p> <p>By treating backing services as attached resources, and accessing them via a well-defined API, applications can be more easily scaled, deployed, and updated, since the dependencies on the underlying services are managed separately from the application code.</p> <p>Benefits to treating backing services as attached resources:</p> <ul> <li> <p>Portability</p> <p>By treating backing services as attached resources, applications can be easily moved between different environments or cloud providers, since the services can be easily swapped out or reconfigured.</p> </li> <li> <p>Scalability</p> <p>By abstracting away the underlying implementation details of the backing services, applications can be more easily scaled horizontally by running multiple instances of the same codebase with different backing services.</p> </li> <li> <p>Maintainability</p> <p>By decoupling the application code from the underlying services, it becomes easier to update or replace the services without impacting the application code.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#15-building-release-run","title":"1.5. Building, Release, Run","text":"<p>The <code>Building, Release, Run</code> principle states that an application should be built, released, and run as discrete stages, with each stage having a distinct responsibility. It is a process-oriented approach that emphasizes the separation of concerns between the development, deployment, and operation of the application.</p> <p>In practice, this means that the development process should be broken down into three stages:</p> <ul> <li> <p>Build</p> <p>This stage involves compiling the application code and any required dependencies, and creating a build artifact that can be deployed to different environments. The output of this stage is a deployable artifact, such as a executable, library, Docker container or ZIP file.</p> </li> <li> <p>Release</p> <p>This stage involves taking the build artifact and combining it with the configuration information needed for the specific environment. This creates a release artifact which is specific version to the environment in which the application will run. The release should be immutable, meaning that it should not be changed once it has been created.</p> </li> <li> <p>Run</p> <p>This stage involves running the application in the environment for which it was released, using the specific release artifact created in the previous stage. The release artifact is deployed to a production environment and started as a process. This process should be stateless and should not rely on any shared resources.</p> </li> </ul> <p>By breaking down the development process into these discrete stages, developers can more easily manage and maintain their applications, and can ensure that each stage has a distinct responsibility. This can also help to ensure that the application is easily deployable and scalable across different environments, and that changes can be made to the application without affecting the runtime environment.</p> <p>Benefits of stages:</p> <ul> <li> <p>Consistency</p> <p>By breaking down the development process into discrete stages, developers can ensure that each stage has a distinct responsibility, which can improve the consistency and reliability of the application across different environments.</p> </li> <li> <p>Portability</p> <p>By separating the application code from the configuration and runtime environment, the application can be deployed to multiple environments without modification.</p> </li> <li> <p>Scalability</p> <p>By separating the application runtime from the application code, the application can be scaled horizontally by running multiple instances of the application in parallel.</p> </li> <li> <p>Maintainability</p> <p>By separating the application code from the configuration and runtime environment, the application can be updated or modified and maintained without affecting the production environment.</p> </li> </ul> <p>NOTE See continuous pipelines for details.</p>"},{"location":"articles/twelve-factor-app/#16-processes","title":"1.6. Processes","text":"<p>The <code>Processes</code> principle states that applications should be designed as stateless processes, which do not maintain any application state in memory.</p> <p>In practice, this means that applications should avoid storing session state, caches, or any other kind of stateful data in memory. Instead, applications should store all stateful data in external services, such as databases or caching systems, and use stateless processes to access this data via well-defined APIs.</p> <p>By designing applications as stateless processes, developers can make them more easily scalable, deployable, and fault-tolerant. Stateless processes can be scaled horizontally by running multiple instances of the same codebase, and can be deployed and updated independently of each other. Since each process does not maintain any application state in memory, it can be easily replaced or restarted without affecting the overall application state.</p> <p>Benefits to designing applications as stateless processes:</p> <ul> <li> <p>Scalability</p> <p>By designing applications as stateless processes, it becomes easier to scale the application horizontally by running multiple instances of the same codebase.</p> </li> <li> <p>Resilience</p> <p>Since stateless processes do not maintain any application state in memory, they can be easily replaced or restarted without affecting the overall application state, making the application more resilient to failures.</p> </li> <li> <p>Maintainability</p> <p>By separating application state from the application logic, it becomes easier to maintain and update the application over time, without risking data corruption or inconsistencies.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#17-port-binding","title":"1.7. Port Binding","text":"<p>The <code>Port Binding</code> principle states that applications should be self-contained and should not rely on the availability of any external dependencies to run.</p> <p>In practice, this means that applications should listen on a designated port and bind to it when started. The application should not assume anything about the host environment, such as the availability of a specific port or network interface, but should instead use environment variables or configuration files to specify the port number and other network settings.</p> <p>By adhering to the Port Binding principle, applications can be more easily deployed and scaled across different environments, since the application code does not rely on any external dependencies or assumptions about the host environment.</p> <p>Benefits of Port Binding:</p> <ul> <li> <p>Portability</p> <p>By using environment variables or configuration files to specify network settings, applications can be easily deployed to different environments without modification.</p> </li> <li> <p>Scalability</p> <p>By binding to a designated port and listening for incoming requests, applications can be more easily scaled horizontally by running multiple instances of the same codebase on different ports or hosts.</p> </li> <li> <p>Robustness</p> <p>By using a designated port and adhering to a well-defined protocol, applications can be more easily monitored, debugged, and tested, since the network interface is clearly specified and documented.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#18-concurrency","title":"1.8. Concurrency","text":"<p>The <code>Concurrency</code> principle states that applications should be designed to take advantage of concurrency and parallelism in order to maximize resource utilization and responsiveness.</p> <p>In practice, this means that applications should be designed to handle multiple requests and processes concurrently, rather than relying on a single thread or process to handle all incoming requests. This can be achieved through the use of asynchronous programming models, such as event-driven architectures or reactive programming, or by using techniques such as thread pooling and load balancing to distribute requests across multiple threads or processes.</p> <p>By designing applications to take advantage of concurrency, developers can improve the performance, scalability, and responsiveness of their applications, especially in high-traffic or high-load scenarios. Concurrency allows multiple requests to be processed simultaneously, reducing latency and increasing throughput.</p> <p>Benefits to designing applications to take advantage of concurrency:</p> <ul> <li> <p>Scalability</p> <p>By handling multiple requests concurrently, applications can more easily scale horizontally by running multiple instances of the same codebase on different hosts or clusters.</p> </li> <li> <p>Responsiveness</p> <p>By processing requests asynchronously, applications can provide more responsive user experiences, since users do not have to wait for long-running operations to complete before receiving a response.</p> </li> <li> <p>Resource utilization</p> <p>By maximizing resource utilization through concurrency, applications can more efficiently use available resources, reducing costs and increasing efficiency.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#19-disposability","title":"1.9. Disposability","text":"<p>The <code>Disposability</code> principle states that applications should be designed to be easily disposable and replaceable, and should be able to start up and shut down quickly without causing any data loss or corruption.</p> <p>In practice, this means that applications should be designed to handle graceful shutdowns and restarts, and should be able to recover quickly from crashes or other failures. Applications should not rely on long-lived processes or in-memory state, but should instead use external storage systems, such as databases or file systems, to store application data.</p> <p>By designing applications to be disposable, developers can ensure that their applications can be easily updated, scaled, and replaced without causing any disruptions to the user experience or data integrity. Disposable applications are also more fault-tolerant, since they can recover quickly from crashes or other failures without losing data or requiring manual intervention.</p> <p>Benefits to designing applications to be disposable:</p> <ul> <li> <p>Scalability</p> <p>Disposable applications can be easily scaled horizontally by running multiple instances of the same codebase, allowing for more efficient resource utilization and improved performance.</p> </li> <li> <p>Resilience</p> <p>Disposable applications are more resilient to failures, since they can quickly recover from crashes or other failures without losing data or requiring manual intervention.</p> </li> <li> <p>Maintainability</p> <p>By separating application logic from application state, disposable applications are easier to maintain and update over time, without risking data corruption or inconsistencies.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#110-devprod-parity","title":"1.10. Dev/Prod Parity","text":"<p>The <code>Dev/Prod Parity</code> principle states that the development, testing, and production environments should be as similar as possible in order to minimize differences and avoid issues that may arise from environment-specific configuration or dependencies.</p> <p>In practice, this means that developers should strive to use the same tools, libraries, and configurations in all environments, and should avoid making changes to the production environment that are not tested and approved in the development and testing environments. This can be achieved through the use of automated build and deployment processes, version control, and configuration management tools.</p> <p>By ensuring that the development, testing, and production environments are as similar as possible, developers can reduce the risk of issues arising from environment-specific differences, such as incompatible dependencies, configuration mismatches, or differences in hardware or software. This can help to minimize the time and effort required to deploy and maintain applications, and can improve the overall quality and reliability of the application.</p> <p>Benefits to ensuring dev/prod parity:</p> <ul> <li> <p>Reduced risk</p> <p>By minimizing environment-specific differences, developers can reduce the risk of issues arising from incompatibilities or mismatches.</p> </li> <li> <p>Improved quality</p> <p>By ensuring that changes are tested and approved in the development and testing environments before being deployed to production, developers can improve the quality and reliability of the application.</p> </li> <li> <p>Faster deployments</p> <p>By using automated build and deployment processes, developers can reduce the time and effort required to deploy and maintain applications, allowing for faster and more frequent deployments.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#111-logs","title":"1.11. Logs","text":"<p>The <code>Logs</code> principle states that applications should treat logs as event streams, and should generate logs in a standardized format that can be easily aggregated and analyzed.</p> <p>In practice, this means that developers should design their applications to generate logs in a standardized format, such as JSON or syslog, and should use logging frameworks and libraries that support structured logging. Logs should be generated for all significant events, such as requests, errors, and warnings, and should be sent to a centralized logging service, such as Splunk, Logstash or Elasticsearch, where they can be aggregated, analyzed, and searched.</p> <p>By treating logs as event streams, developers can gain valuable insights into the behavior and performance of their applications, and can quickly identify and diagnose issues that may arise. Centralized logging also makes it easier to manage and maintain logs, and can provide a centralized source of truth for debugging and troubleshooting.</p> <p>NOTE For more information see the article about Logging and Monitoring.</p> <p>Benefits to treating logs as event streams:</p> <ul> <li> <p>Improved troubleshooting</p> <p>By generating logs for all significant events, developers can quickly identify and diagnose issues that may arise, reducing the time and effort required for troubleshooting.</p> </li> <li> <p>Better performance monitoring</p> <p>By analyzing log data, developers can gain insights into the behavior and performance of their applications, and can identify bottlenecks or other issues that may be impacting performance.</p> </li> <li> <p>Easier maintenance</p> <p>By centralizing logs in a single location, developers can more easily manage and maintain logs, and can ensure that all logs are retained for the appropriate length of time.</p> </li> </ul> <p>Example of conform Logs principle:</p> <pre><code>package main\n\nimport (\n    \"os\"\n    \"github.com/rs/zerolog\"\n    \"github.com/rs/zerolog/log\"\n)\n\nfunc main() {\n    // Set up the logger to output JSON format\n    log.Logger = log.Output(zerolog.ConsoleWriter{Out: os.Stderr})\n\n    // Set the logging level to debug\n    zerolog.SetGlobalLevel(zerolog.DebugLevel)\n\n    // Log some events\n    log.Debug().Msg(\"This is a debug message\")\n    log.Info().Msg(\"This is an info message\")\n    log.Warn().Msg(\"This is a warning message\")\n    log.Error().Msg(\"This is an error message\")\n}\n</code></pre>"},{"location":"articles/twelve-factor-app/#112-admin-processes","title":"1.12. Admin Processes","text":"<p>The <code>Admin Processes</code> principle states that applications should provide administrative processes as one-off processes that can be run independently of the application's main processes.</p> <p>In practice, this means that developers should design their applications to provide a set of administrative processes that can be used to perform tasks such as database migrations, backups, and other maintenance tasks. These administrative processes should be run independently of the application's main processes, and should be designed to run in a single execution environment, rather than being part of the application's ongoing runtime.</p> <p>By providing administrative processes as one-off processes, developers can more easily manage and maintain their applications, and can avoid potential issues that may arise from running administrative tasks as part of the application's ongoing runtime. This can also help to ensure that administrative tasks are performed in a consistent and repeatable manner, regardless of the underlying environment.</p> <p>Benefits to providing administrative processes as one-off processes:</p> <ul> <li> <p>Improved reliability</p> <p>By running administrative tasks as one-off processes, developers can reduce the risk of issues that may arise from running administrative tasks as part of the application's ongoing runtime.</p> </li> <li> <p>Better manageability</p> <p>By providing administrative processes as one-off processes, developers can more easily manage and maintain their applications, and can ensure that administrative tasks are performed in a consistent and repeatable manner.</p> </li> <li> <p>Easier scaling</p> <p>By separating administrative tasks from the application's main processes, developers can more easily scale their applications and ensure that administrative tasks do not impact the performance or reliability of the application's main processes.</p> </li> </ul>"},{"location":"articles/twelve-factor-app/#2-references","title":"2. References","text":"<ul> <li>Sentenz package managers article.</li> <li>Sentenz continuous pipelines article.</li> <li>Sentenz logging and monitoring article.</li> <li>Google twelve-factor app article.</li> <li>IBM twelve plus factors article.</li> <li>IBM seven missing factors article.</li> <li>Github twelve-factor app repository.</li> </ul>"},{"location":"articles/unit-testing-frameworks/","title":"Unit-Testing Frameworks","text":"<ul> <li>1. Unit Testing</li> <li>1.1. GoogleTest</li> <li>1.2. Boost.Test</li> <li>1.3. Catch2</li> <li>1.4. Doctest</li> <li>1.5. pytest</li> <li>1.6. go</li> <li>2. Code Coverage</li> <li>2.1. gcov</li> <li>2.2. go</li> <li>3. Profiling</li> <li>3.1. gprof</li> <li>3.2. go</li> <li>4. References</li> </ul>"},{"location":"articles/unit-testing-frameworks/#1-unit-testing","title":"1. Unit Testing","text":"<p>Unit testing aims to check individual units of the source code separately. A unit is the smallest part of code that can be tested in isolation.</p> <p>Benefits of unit testing:</p> <ul> <li> <p>Modularize code</p> <p>As code's testability depends on its design, unit tests facilitate breaking it into specialized test pieces.</p> </li> <li> <p>Avoid regressions</p> <p>A suite of unit tests can run it iteratively to ensure that everything keeps working correctly every time you add new functionality or introduce changes.</p> </li> <li> <p>Document code</p> <p>Running, debugging, or even just reading tests can give a lot of information about how the original code works, software testing can be used as implicit documentation.</p> </li> </ul> <p>Best practices for unit testing:</p> <ul> <li>Creating tests for all publicly exposed functions, including class constructors and operators.</li> <li>Covering all code paths and checking both trivial and edge cases, including those with incorrect input data (see negative testing).</li> <li>Assuring that each test works independently and doesn't prevent other tests from execution.</li> <li>Organizing tests in a way that the order in which you run them doesn't affect the results.</li> </ul> <p>A single unit test is a method that checks some specific functionality and has clear pass/fail criteria. The generalized structure of a single test looks like this:</p> <pre><code>Test (TestGroupName, TestName)   {\n    1 - setup block\n    2 - running the under-test functionality\n    3 - checking the results (assertions block)\n}\n</code></pre>"},{"location":"articles/unit-testing-frameworks/#11-googletest","title":"1.1. GoogleTest","text":"Mock Fixture Fuzzing Code Coverage Build System Built-in Built-in FuzzTest gcov CMake / Bazel <p>GoogleTest is Google Testing and Mocking Framework, it includes a rich set of fatal and non-fatal assertions, provides instruments for creating fixtures and test groups, gives informative messages, and exports the results in XML.</p> <p>To learn more about Google Test, explore the samples in the framework's repository. Also, take a look at Advanced options for details of other noticeable Google Test features such as value-parametrized tests and type-parameterized tests.</p> <p>Tools for GoogleTest:</p> <ul> <li>FuzzTest <p>FuzzTest is a C++ testing framework for writing and executing fuzz tests, which are property-based tests.</p> </li> </ul>"},{"location":"articles/unit-testing-frameworks/#12-boosttest","title":"1.2. Boost.Test","text":"Mock Fixture Fuzzing Code Coverage Build System FakeIt Built-in \u2014 gcov CMake <p>Boost.Test unit testing frameworks for C++. Boost unit testing framework (Boost.Test) is a part of the Boost library. It is a fully-functional and scalable framework, with wide range of assertion macros, XML output, and other features.</p> <p>Boost.Test doesn't provide mocking functionality. However, combine it with standalone mocking frameworks such as Hippomocks, FakeIt, or Trompeloeil.</p>"},{"location":"articles/unit-testing-frameworks/#13-catch2","title":"1.3. Catch2","text":"Mock Fixture Fuzzing Code Coverage Build System FakeIt Built-in \u2014 gcov CMake <p>Catch2 unit testing frameworks for C++. It's a header-only testing system: to create tests with Catch2, download and include only one header file, <code>catch.hpp</code>. The framework's name stands for <code>C++ Automated Test Cases in Headers (version two)</code>.</p> <p>Catch2 doesn't provide mocking functionality. However, combine it with standalone mocking frameworks such as Hippomocks, FakeIt, or Trompeloeil.</p>"},{"location":"articles/unit-testing-frameworks/#14-doctest","title":"1.4. Doctest","text":"Mock Fixture Fuzzing Code Coverage Build System FakeIt Built-in \u2014 gcov CMake <p>Doctest unit testing frameworks for C++. Doctest is a single-header framework with self-registering tests. Doctest was designed after Catch and shares some parts of the Catch's code.</p> <p>Doctest doesn't provide mocking functionality. However, combine it with standalone mocking frameworks such as Hippomocks, FakeIt, or Trompeloeil.</p>"},{"location":"articles/unit-testing-frameworks/#15-pytest","title":"1.5. pytest","text":"Mock Fixture Fuzzing Code Coverage Build System pytest-mock pytest fixtures atheris fuzzing pytest-cov \u2014 <p>pytest unit testing frameworks for Python. The pytest framework allows to write small, readable tests, and can be scaled to support complex functional tests for applications and libraries.</p>"},{"location":"articles/unit-testing-frameworks/#16-go","title":"1.6. go","text":""},{"location":"articles/unit-testing-frameworks/#2-code-coverage","title":"2. Code Coverage","text":"<p>In computer science, test coverage is a measure (in percent) of the degree to which the source code of a program is executed when a particular test suite is run.</p>"},{"location":"articles/unit-testing-frameworks/#21-gcov","title":"2.1. gcov","text":"<p>Gcov is a source code coverage analysis and statement-by-statement profiling tool. Gcov generates exact counts of the number of times each statement in a program is executed and annotates source code to add instrumentation. Gcov comes as a standard utility with the GNU Compiler Collection (GCC) suite. The gcov utility gives information on how often a program executes segments of code. It produces a copy of the source file, annotated with execution frequencies.</p> <p>When passing the coverage flags manually, one of the following options can be used, depending on which compiler and coverage tools is preferred:</p> <ul> <li> <p>GCC</p> <p><code>-fprofile-arcs</code> <code>-ftest-coverage</code> or <code>--coverage</code></p> </li> <li> <p>Clang/Clang-cl</p> <ul> <li>Use the same flags as for GCC to get the gcov-style coverage collected with llvm-cov gcov.</li> <li>Use <code>-fprofile-instr-generate</code> <code>-fcoverage-mapping</code> to invoke the Clang\u2019s instrumentation-based profiling which uses a pair of the <code>llvm-profdata merge</code> and <code>llvm-cov</code> export commands.</li> </ul> </li> <li> <p>CMake</p> <p>Provide the flags by setting the <code>CMAKE_CXX_FLAGS</code> variable (<code>CMAKE_C_FLAGS</code> for C projects) or using other alternatives like the <code>add_compile_options</code> command. On linker errors while building the project with gcov compiler flags, try passing the same flags to the linker through <code>add_link_options</code> or <code>set(CMAKE_EXE_LINKER_FLAGS \"\")</code>.</p> </li> </ul>"},{"location":"articles/unit-testing-frameworks/#22-go","title":"2.2. go","text":"<pre><code>go test -race -coverprofile=coverage.out -covermode=atomic ./...\n</code></pre>"},{"location":"articles/unit-testing-frameworks/#3-profiling","title":"3. Profiling","text":""},{"location":"articles/unit-testing-frameworks/#31-gprof","title":"3.1. gprof","text":""},{"location":"articles/unit-testing-frameworks/#32-go","title":"3.2. go","text":""},{"location":"articles/unit-testing-frameworks/#4-references","title":"4. References","text":"<ul> <li>JetBrains unit testing article.</li> <li>JetBrains code coverage article.</li> <li>Codecov about article.</li> </ul>"},{"location":"articles/xops/","title":"XOps","text":"<p>XOps, an umbrella term used for a combination of IT tech like DevOps, DevSecOps, AIOps, MLOps, GitOps, and BizDevOps. XOps supposedly helps to shorten a system development cycles and provides continuous delivery with high software quality.</p> <p>XOps automates processes, manages pipelines and workflows through scripts. Developers are able to extend XOps with custom interfaces, connectors, and functions. XOps aids in making business processes more efficient and automating everything that can be automated to achieve business goals.</p> <ul> <li>1. DevOps</li> <li>2. GitOps</li> <li>3. DevSecOps</li> <li>4. SysOps</li> <li>5. BizDevOps</li> <li>6. DataOps</li> <li>7. MLOps</li> <li>8. AIOps</li> <li>9. CloudOps</li> <li>10. References</li> </ul>"},{"location":"articles/xops/#1-devops","title":"1. DevOps","text":"<p>DevOps is a combination of the terms development and operations, meant to represent a collaborative or shared approach to the tasks performed by a company's application development and IT operations teams.</p> <p>A DevOps approach is one of many techniques IT staff use to execute IT projects that meet business needs. DevOps can coexist with Agile software development, IT service management frameworks, such as ITIL, project management directives, such as Lean and Six Sigma and other strategies.</p> <p>While DevOps is not a technology, DevOps environments generally apply common methodologies.</p> <ul> <li> <p>Code Repositories</p> <p>Version-controlled source code repositories enable multiple developers to work on code. Developers check code out and in, and they can revert to a previous version of code if needed. These tools keep a record of modifications made to the source code. Without tracking, developers may struggle to follow which changes are recent and which versions of the code are available to end users.</p> <p>In a Continuous pipeline, a code change committed in the version-control repository automatically triggers next steps, such as a static code analysis or build and unit tests. Tools for source code management include Git and GitHub.</p> </li> <li> <p>Artifact Repositories</p> <p>Source code is compiled into an artifact for testing. Artifact repositories enable version-controlled, object-based outputs. Artifact management is a good practice for the same reasons as version-controlled source code management. Examples of artifact repositories include JFrog Artifactory and Nexus Repository.</p> </li> <li> <p>Continuous Pipelines</p> <p>Continuous pipelines enables DevOps teams to frequently validate and deliver applications to the end user through automation during the development lifecycle. The continuous integration tool initializes processes so that developers can create, test and validate code in a shared repository as often as needed without manual work. Continuous delivery extends these automatic steps through production-level tests and configuration setups for release management. Continuous deployment goes a step further, invoking tests, configuration and provisioning, as well as monitoring and potential rollback capabilities. Common tools for CI, CD or both include Jenkins, GitLab and CircleCI.</p> </li> <li> <p>Containers</p> <p>Containers are isolated runtimes for software on a shared OS. Containers provide abstraction that enables code to work the same on different underlying infrastructure from development to testing and staging, and then to production. Docker is the most well-known containerization software, while Microsoft offers specific Windows container options. Container orchestrators -- such as Kubernetes and commercial Kubernetes distributions Red Hat OpenShift and Amazon Elastic Kubernetes Service -- deploy, scale and maintain containers automatically.</p> </li> <li> <p>Configuration Management</p> <p>Configuration management systems enable IT to provision and configure software, middleware and infrastructure based on a script or template. The DevOps team can set up deployment environments for software code releases and enforce policies on servers, containers and VMs through a configuration management tool. Changes to the deployment environment can be version controlled and tested, so DevOps teams can manage infrastructure as code. Configuration management tools include Puppet and Chef.</p> </li> </ul>"},{"location":"articles/xops/#2-gitops","title":"2. GitOps","text":"<p>GitOps is a specific system operations process (ops) tied to a specific tool (Git). Git is an evolution of Infrastructure as Code (IaC) and a DevOps best practice that uses Git as the single source of truth and control mechanism for building, updating, and deleting the infrastructure architecture. Weaveworks is credited with creating the term GitOps.</p>"},{"location":"articles/xops/#3-devsecops","title":"3. DevSecOps","text":"<p>DevSecOps is a set of principles and practices for protecting software, infrastructure, applications and data. DevSecOps stands for development, security, and operations. It's an approach to culture, automation, and platform design that integrates security as a shared responsibility throughout the IT lifecycle.</p>"},{"location":"articles/xops/#4-sysops","title":"4. SysOps","text":""},{"location":"articles/xops/#5-bizdevops","title":"5. BizDevOps","text":"<p>BizDevOps, also known as DevOps 2.0, is an approach to software development that encourages developers, operations staff and business teams to work together so the organisation can develop software more quickly, be more responsive to user demand, and ultimately maximise revenue.</p> <p>BizDevOps</p>"},{"location":"articles/xops/#6-dataops","title":"6. DataOps","text":"<p>This is a popular process in analytics that is slowly picking up at a good pace. It tries to reduce the cycle time of data analytics projects while improving the quality; it starts right from the beginning of the pipeline (data preparation) and is deployed to various points in the analytics chain and IT operations. Technology is used to automate the design and management of data delivery while adhering to appropriate levels of governance.</p>"},{"location":"articles/xops/#7-mlops","title":"7. MLOps","text":"<p>MLOps refers to creating, deploying and maintaining machine learning models. It is an umbrella term that involves combining a variety of methods such as DevOps, machine learning, and handling of data that can simplify and build more efficient ways of deploying machine learning algorithms. All of this has to be done while keeping the business goals in mind.</p> <p>MLOps</p>"},{"location":"articles/xops/#8-aiops","title":"8. AIOps","text":""},{"location":"articles/xops/#9-cloudops","title":"9. CloudOps","text":"<p>CloudOps refers to managing activities involving optimising IT workloads or services in the cloud. It comes with different aspects such as cloud architecture, software development, security as well as compliance. The goal here is to improve the accessibility and efficiency of cloud services in the business.</p>"},{"location":"articles/xops/#10-references","title":"10. References","text":"<ul> <li>Atlassian devops article.</li> <li>GitHub devops article.</li> <li>Microsoft devops article.</li> <li>Microsoft devsecops article.</li> <li>Google DORA metrics article.</li> <li>Google managing infrastructure as code with Terraform, Cloud Build, and GitOps article.</li> <li>GitLab GitOps article.</li> <li>RedHad GitOps article.</li> <li>RedHad DevOps article.</li> </ul>"},{"location":"convention/","title":"Convention","text":"<p>Conventions, standards, and specifications administered by organizations or institutions.</p> <ul> <li>Changelog</li> <li>CLI Conventions</li> <li>Container</li> <li>Conventional Commits</li> <li>IEC 62443</li> <li>ISO 8601</li> <li>Makefile Conventions</li> <li>Material Design</li> <li>POSIX</li> <li>Semantic Versioning</li> <li>SPDX</li> </ul>"},{"location":"convention/changelog/","title":"Changelog","text":"<p>A changelog is a document or file that tracks and records the changes, updates, and additions made to a software project over time. It serves as a historical record of the project's evolution and provides a summary of the modifications introduced in each version or release.</p> <ul> <li>1. Category</li> <li>2. References</li> </ul>"},{"location":"convention/changelog/#1-category","title":"1. Category","text":"<p>Changelogs are commonly used in software development to keep stakeholders, users, and developers informed about the changes implemented in the software. They typically include information about new features, bug fixes, improvements, known issues, and other relevant details.</p> <p>Changelogs can be maintained manually by the development team, or they can be automatically generated based on commit messages, pull requests, or other version control system data.</p> <p>Components of Changelog:</p> <ol> <li> <p>Version</p> <p>Each version or release is typically assigned a unique identifier, such as a version number or a release name. This helps in tracking and distinguishing different iterations of the software.</p> </li> <li> <p>Date</p> <p>The date when the release was made or the changes were implemented.</p> </li> <li> <p>Categories</p> <p>Changelogs often organize changes into different categories or sections, such as <code>feat</code>, <code>fix</code>, <code>refactor</code> or <code>deprecated</code>. This grouping helps users quickly locate relevant updates based on their interests or concerns.</p> </li> <li> <p>Descriptions</p> <p>Each change or update is described concisely, providing details about the specific modifications made, new features added, bugs fixed, or any other relevant information.</p> </li> <li> <p>References</p> <p>Changelogs may include references to related items, such as issue tracking numbers, pull requests, or commit hashes.</p> </li> </ol> <p>Benefits of Changelog:</p> <ol> <li> <p>Transparency</p> <p>Users and stakeholders can easily see what changes have been made and what to expect from each software release.</p> </li> <li> <p>Documentation</p> <p>Changelogs serve as historical documentation of a software project, allowing developers to understand the evolution of the codebase over time.</p> </li> <li> <p>Communication</p> <p>Changelogs facilitate effective communication within the development team and with users, ensuring that everyone is aware of the changes and updates in the software.</p> </li> <li> <p>Troubleshooting</p> <p>Changelogs can help users identify whether a particular issue they are experiencing has been addressed in a recent release or if it requires further investigation.</p> </li> </ol> <p>Example of Changelog in Markdown:</p> <pre><code># [1.2.0](https://github.com/project/compare/v1.1.0...v1.2.0) (2023-03-03)\n\n\n### Bug Fixes\n\n* add `cpmopts` to `cmp` library ([#35](https://github.com/project/issues/35)) ([3365809](https://github.com/project/commit/336580)), closes [#34](https://github.com/project/issues/34)\n\n\n### Features\n\n* create a database infrastructure ([#43](https://github.com/project/issues/43)) ([bc93f4b](https://github.com/project/commit/bc93f4)), closes [#40](https://github.com/project/issues/40)\n\n# 1.0.0 (2022-02-14)\n\n\n### Features\n\n* initial commit ([11d5eed](https://github.com/project/commit/11d5ee))\n</code></pre>"},{"location":"convention/changelog/#2-references","title":"2. References","text":"<ul> <li>Sentnez conventional commits article.</li> <li>Sentnez semantic versioning article.</li> <li>Github keep a changelog repository.</li> <li>GNU style of change logs article.</li> <li>Github managing releases in a repository article.</li> </ul>"},{"location":"convention/cli-conventions/","title":"CLI Conventions","text":"<p>The Command-Line Interface (CLI) conventions consist of several guidelines that must be brought to the Harmony.</p> <ul> <li>1. Category</li> <li>1.1. POSIX Utility Conventions</li> <li>1.2. GNU CLI Standards</li> </ul>"},{"location":"convention/cli-conventions/#1-category","title":"1. Category","text":""},{"location":"convention/cli-conventions/#11-posix-utility-conventions","title":"1.1. POSIX Utility Conventions","text":"<p>Utility Conventions</p>"},{"location":"convention/cli-conventions/#12-gnu-cli-standards","title":"1.2. GNU CLI Standards","text":"<p>Standards for Command Line Interfaces</p>"},{"location":"convention/container/","title":"Container","text":"<ul> <li>1. Category</li> <li>1.1. Open Container Initiative</li> <li>1.2. Compose Specification</li> <li>2. References</li> </ul>"},{"location":"convention/container/#1-category","title":"1. Category","text":""},{"location":"convention/container/#11-open-container-initiative","title":"1.1. Open Container Initiative","text":"<p>The Open Container Initiative (OCI) is an open governance structure for the express purpose of creating open industry standards around container formats and runtimes.</p>"},{"location":"convention/container/#12-compose-specification","title":"1.2. Compose Specification","text":"<p>The Compose Specification is a developer-focused standard for defining cloud and platform agnostic container-based applications. The compose specification establishes a standard for the definition of multi-container platform-agnostic applications.</p>"},{"location":"convention/container/#2-references","title":"2. References","text":"<ul> <li>Github open container initiative repository.</li> <li>Github compose specification repository.</li> </ul>"},{"location":"convention/conventional-commits/","title":"Conventional Commits","text":"<p>Conventional Commits is a commit message convention that provides a standardized and structured format for commit messages in software development projects. It aims to make commit messages more readable, informative, and useful for both humans and automated tools.</p> <ul> <li>1. Category</li> <li>1.1. Structure</li> <li>1.2. Types</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"convention/conventional-commits/#1-category","title":"1. Category","text":"<p>Conventional Commits provide a consistent and informative commit message format, making it easier for developers, reviewers, and automated tools to understand the purpose and impact of each commit in a project.</p> <p>Conventional Commits help with automated tools like release notes generators, changelogs, and semantic versioning systems. By following this convention, teams can create more structured commit histories, making it easier to understand the changes made to the codebase over time.</p> <p>NOTE Conventional Commits is a convention and not a strict requirement. Its adoption and enforcement depend on the development team and the project's guidelines.</p>"},{"location":"convention/conventional-commits/#11-structure","title":"1.1. Structure","text":"<p>The structure of a Conventional Commit follows a specific format to provide a standardized and consistent way of writing commit messages. A Conventional Commit message consists of the <code>header</code>, the <code>body</code>, and the <code>footer</code>.</p> <p>Structure of Conventional Commits:</p> <ol> <li> <p>Header</p> <p>The header is the first line of the commit message and contains essential information about the commit. It has the following format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n</code></pre> <ul> <li> <p><code>&lt;type&gt;</code></p> <p>Indicates the type of the commit, such as a new feature (<code>feat</code>), bug fix (<code>fix</code>), documentation update (<code>docs</code>), etc.</p> </li> <li> <p><code>&lt;scope&gt;</code> (optional)</p> <p>Denotes the scope or module of the project that the commit affects. It can be a specific component, file, or functionality. The scope is enclosed in parentheses.</p> </li> <li> <p><code>&lt;subject&gt;</code></p> <p>Briefly describes the commit's purpose in a concise and imperative manner (e.g., <code>add authentication module</code>, <code>fix validation bug</code>).</p> </li> </ul> </li> <li> <p>Body</p> <p>The body of the commit message provides additional information about the commit. It is optional but highly recommended, especially for complex changes. The body should be wrapped at 72 characters and provides more details about the motivation behind the change, any related issues, or any other relevant contextual information.</p> </li> <li> <p>Footer</p> <p>The footer section is also optional and is used for providing any additional information related to the commit. Some common uses of the footer include referencing issues or tasks, documenting breaking changes, listing co-authored-by details, or including links to external resources.</p> </li> </ol> <p>Example of Conventional Commits:</p> <p>Example of a complete Conventional Commit message with all three parts:</p> <pre><code>feat(user): add password reset functionality\n\n- Implement the password reset feature according to user requirements.\n- Add an API endpoint for initiating password reset.\n- Include email notification to the user with reset instructions.\n\nCloses #123\n</code></pre> <p>In the example:</p> <ul> <li>The header indicates that it is a new feature (<code>feat</code>) related to the user module (<code>user</code>).</li> <li>The body provides a bulleted list describing the implementation details.</li> <li>The footer references the issue number <code>#123</code> that this commit resolves or relates to.</li> </ul>"},{"location":"convention/conventional-commits/#12-types","title":"1.2. Types","text":"<p>Conventional Commits categorize commits into different types to provide more context and make it easier to understand the nature of the changes. While the conventional commit types can vary slightly depending on the project and team preferences.</p> <p>NOTE The convention also allows for additional commit types and scopes specific to the project's needs.</p> <p>Types of Conventional Commits:</p> <ol> <li> <p><code>feat</code></p> <p>Used for commits that introduce new features or functionalities to the project. It indicates the addition of significant user-visible changes.</p> </li> <li> <p><code>fix</code></p> <p>Commits categorized as <code>fix</code> address bug fixes or resolve issues in the codebase. They indicate corrections to existing features or functionality.</p> </li> <li> <p><code>docs</code></p> <p>Documentation plays a vital role in software projects. The <code>docs</code> category is used for commits that update or add documentation, including readme files, API documentation, or user guides.</p> </li> <li> <p><code>style</code></p> <p>The <code>style</code> category is used for commits that focus on code style changes, such as formatting, indentation, or whitespace modifications. These commits do not affect the functionality of the code but improve its readability and maintainability.</p> </li> <li> <p><code>refactor</code></p> <p>Commits categorized as <code>refactor</code> involve making changes to the codebase that neither fix a bug nor add a new feature. Refactoring aims to improve code structure, organization, or efficiency without changing external behavior.</p> </li> <li> <p><code>test</code></p> <p>Used for changes that add or modify test cases, test frameworks, or other related testing infrastructure.</p> </li> <li> <p><code>chore</code></p> <p>Typically used for routine or miscellaneous tasks related to the project, such as code reformatting, updating dependencies, or making general project maintenance.</p> </li> <li> <p><code>build</code></p> <p>Used when a commit affects the build system or external dependencies. It includes changes to build scripts, configurations, or tools used in the project.</p> </li> <li> <p><code>ci</code></p> <p>Stands for <code>continuous integration</code>. This type is used for changes to the project's continuous integration or deployment configurations, scripts, or infrastructure.</p> </li> <li> <p><code>perf</code></p> <p>Short for <code>performance</code>, this type is used when a commit improves the performance of the code or optimizes certain functionalities.</p> </li> </ol>"},{"location":"convention/conventional-commits/#2-principles","title":"2. Principles","text":"<p>While Conventional Commits provide a specific structure and format for commit messages, there are some underlying principles and benefits associated with their adoption. By adhering to these principles, Conventional Commits foster clarity, collaboration, and maintainability in software development projects. They promote better communication and documentation practices, simplify versioning and release processes, and improve overall code quality and project management.</p> <ul> <li> <p>Readability and Understandability</p> <p>Conventional Commits aim to make commit messages more readable and understandable for humans. By following a standardized format and including clear and concise information, it becomes easier for developers, reviewers, and other stakeholders to grasp the purpose and impact of each commit.</p> </li> <li> <p>Semantic Versioning</p> <p>Conventional Commits align with semantic versioning principles. By categorizing commits into types such as <code>feat</code> (new feature), <code>fix</code> (bug fix), or <code>chore</code> (maintenance), it becomes possible to automate the generation of release notes and determine the appropriate version numbers based on the types of changes introduced.</p> </li> <li> <p>Changelog Generation</p> <p>Conventional Commits enable automated changelog generation. The structured commit messages provide the necessary information to generate changelogs, which summarize the changes between different versions of the software. This helps users and stakeholders easily track and understand the evolution of the project.</p> </li> <li> <p>Collaboration and Code Review</p> <p>Consistent commit message formats improve collaboration within development teams. Clear commit messages enhance code review processes, allowing reviewers to understand the intent and impact of changes more efficiently. It helps maintain a clean commit history and makes it easier to trace back changes when needed.</p> </li> <li> <p>Documentation and Communication</p> <p>Conventional Commits contribute to better documentation and communication practices. By providing meaningful commit messages, projects can establish a historical record of changes, including the reasoning behind each change. This documentation serves as a valuable resource for future developers, maintainers, and users who need to understand the evolution of the codebase.</p> </li> <li> <p>Tooling and Automation</p> <p>Conventional Commits facilitate the development of tooling and automation around commit messages. Various tools, such as release note generators, semantic versioning plugins, and commit message linters, can leverage the structured commit format to automate tasks and extract useful information from commit messages.</p> </li> </ul>"},{"location":"convention/conventional-commits/#3-best-practice","title":"3. Best Practice","text":"<p>The effective implementation of Conventional Commits in a project, improves code quality, leads to better collaboration and streamlined release processes.</p> <ul> <li> <p>Consistency</p> <p>Consistency is key to deriving the maximum benefit from Conventional Commits. Ensure that all developers in a project follow the convention consistently to maintain a unified commit history and facilitate automated tooling.</p> </li> <li> <p>Clear and Concise Messages</p> <p>Write commit messages that are clear, concise, and describe the purpose of the change. Use imperative verbs in the subject line and provide additional details in the body when necessary.</p> </li> <li> <p>Scope Appropriately</p> <p>Use the optional scope element in the header to indicate the specific module, component, or area of the project that the commit affects. Keep scopes meaningful and relevant to provide more context to readers.</p> </li> <li> <p>Separate Concerns</p> <p>Keep commits focused on a single concern or change. If a commit involves multiple unrelated changes, consider splitting them into separate commits to maintain clarity and facilitate easier review.</p> </li> <li> <p>Use Body and Footer When Needed</p> <p>Provide additional information in the body section when the commit requires more context or explanation. Use the footer to reference related issues, document breaking changes, or include other relevant details.</p> </li> <li> <p>Follow Semantic Versioning</p> <p>Consider the impact of the commit and choose the appropriate commit type (<code>feat</code>, <code>fix</code>, etc.) according to semantic versioning guidelines. This helps automate versioning and release processes.</p> </li> <li> <p>Use Tooling and Automation</p> <p>Leverage tools and plugins that support Conventional Commits. These tools can automate the generation of release notes, changelogs, and enforce commit message formats. They can also help ensure consistent adherence to the convention across the team.</p> </li> <li> <p>Integrate with Workflows</p> <p>Integrate Conventional Commits into the development workflows and processes. For example, as a requirement for pull requests or code reviews, or incorporate it into the continuous integration/continuous delivery (CI/CD) pipeline.</p> </li> <li> <p>Educate and Communicate</p> <p>Educate the development team about Conventional Commits and its benefits. Encourage open discussions and provide guidance on how to write effective commit messages. Clear communication and understanding within the team lead to better adoption and collaboration.</p> </li> </ul>"},{"location":"convention/conventional-commits/#4-terminology","title":"4. Terminology","text":"<p>The terms are fundamental to understanding and working with Conventional Commits. By using this terminology consistently, developers can effectively communicate and document the changes made to the codebase in a structured and standardized manner.</p> <ul> <li> <p>Commit</p> <p>In version control systems like Git, a commit represents a snapshot of the changes made to the codebase at a particular point in time. It includes a commit message describing the changes.</p> </li> <li> <p>Commit Message</p> <p>A commit message is a textual description of the changes introduced in a commit. In the context of Conventional Commits, the commit message follows a specific format and structure to provide standardized and meaningful information about the commit.</p> </li> <li> <p>Header</p> <p>The header is the first line of a commit message and contains the essential information about the commit, including the type, scope, and subject of the changes. It follows the format <code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;</code>.</p> </li> <li> <p>Type</p> <p>The type represents the category or nature of the changes made in a commit. Common types include <code>feat</code> (new feature), <code>fix</code> (bug fix), <code>docs</code> (documentation update), <code>style</code> (code style changes), <code>refactor</code> (code refactoring), <code>test</code> (adding or modifying tests), and <code>chore</code> (maintenance tasks or non-functional changes).</p> </li> <li> <p>Scope</p> <p>The scope (optional) provides additional context about the portion of the codebase that the commit affects. It can be a specific component, module, file, or functionality. The scope is enclosed in parentheses and helps narrow down the scope of the changes.</p> </li> <li> <p>Subject</p> <p>The subject is a brief and concise description of the commit's purpose or the main change introduced. It should be written in an imperative form, starting with a verb. For example, <code>add authentication module</code> or <code>fix validation bug</code>.</p> </li> <li> <p>Body</p> <p>The body (optional) is the section of the commit message that provides additional information and details about the changes made in the commit. It can include a more extensive description, motivations, references to related issues, or any other relevant contextual information.</p> </li> <li> <p>Footer</p> <p>The footer (optional) is the section of the commit message that appears after an empty line following the body. It can include supplementary information such as references to related issues or tasks, breaking changes, co-authored-by details, or links to external resources.</p> </li> </ul>"},{"location":"convention/conventional-commits/#5-references","title":"5. References","text":"<ul> <li>Sentnez commit message guide article.</li> <li>Sentnez semantic versioning article.</li> <li>Sentnez changelog article.</li> <li>Github conventional commits repository.</li> </ul>"},{"location":"convention/iec-62443/","title":"IEC 62443","text":"<p>IEC 62443 is a series of international standards developed by the International Electrotechnical Commission (IEC) that provides a framework for implementing cybersecurity in industrial automation and control systems (IACS). The series comprises of several parts, each of which addresses a specific aspect of IACS cybersecurity.</p> <p>The standards are designed to provide a systematic approach to IACS security, addressing the entire lifecycle of the system from concept to retirement. This includes defining security requirements, designing secure systems, implementing security controls, and continuously monitoring and improving the security posture of the system.</p> <p>IEC 62443 provides a comprehensive approach to IACS cybersecurity that takes into account the unique requirements and challenges of these systems. The standards are designed to be flexible and scalable, allowing organizations to tailor their approach to their specific needs and risk profile.</p> <p>The IEC 62443 series is widely recognized as a leading standard for IACS cybersecurity and is used by organizations around the world to help secure their critical infrastructure.</p> <ul> <li>1. Category</li> <li>1.1. IEC 62443-1 General Concepts<ul> <li>1.1.1. IEC 62443-1-1 Terminology, concepts and models</li> <li>1.1.2. IEC 62443-1-2 Master glossary of terms and abbreviations</li> <li>1.1.3. IEC 62443-1-3 System security compliance metrics</li> <li>1.1.4. IEC 62443-1-4 IACS security lifecycle and use-case</li> </ul> </li> <li>1.2. IEC 62443-2 Policies and Procedures<ul> <li>1.2.1. IEC 62443-2-1 Security program requirements for IACS asset owners</li> <li>1.2.2. IEC 62443-2-2 IACS Security Program Ratings</li> <li>1.2.3. IEC TR 62443-2-3 Patch management in the IACS environment</li> <li>1.2.4. IEC 62443-2-4 Security program requirements for IACS service providers</li> </ul> </li> <li>1.3. IEC 62443-3 System Security<ul> <li>1.3.1. IEC TR 62443-3-1 Security technologies for IACS</li> <li>1.3.2. IEC 62443-3-2 Security risk assessment for system design</li> <li>1.3.3. IEC 62443-3-3 System security requirements and security levels</li> </ul> </li> <li>1.4. IEC 62443-4 Component Security<ul> <li>1.4.1. IEC 62443-4-1 Secure product development lifecycle requirements</li> <li>1.4.2. IEC 62443-4-2 Technical security requirements for IACS components</li> </ul> </li> <li>2. Terminology</li> </ul>"},{"location":"convention/iec-62443/#1-category","title":"1. Category","text":"<p>The IEC 62443 series of standards is organized into four main categories, each of which addresses a different aspect of IACS security.</p>"},{"location":"convention/iec-62443/#11-iec-62443-1-general-concepts","title":"1.1. IEC 62443-1 General Concepts","text":"<p>The standard provides an introduction to the IEC 62443 series and describes the general concepts and terminology used in IACS security.</p>"},{"location":"convention/iec-62443/#111-iec-62443-1-1-terminology-concepts-and-models","title":"1.1.1. IEC 62443-1-1 Terminology, concepts and models","text":"<p>The IEC 62443-1-1 standard provides a common language and conceptual framework for IACS security. It defines terms and concepts used throughout the series and provides a model for understanding the components and interactions of IACS.</p> <p>Key terms defined in IEC 62443-1-1:</p> <ul> <li> <p>Cybersecurity</p> <p>The prevention of damage to, protection of, and restoration of systems, networks, and information</p> </li> <li> <p>Risk</p> <p>The potential for loss, damage or destruction of an asset as a result of a threat exploiting a vulnerability.</p> </li> <li> <p>Asset</p> <p>A resource or component of an organization that has value and requires protection.</p> </li> <li> <p>Threat</p> <p>A potential cause of an unwanted incident that may result in harm to a system or organization.</p> </li> <li> <p>Vulnerability</p> <p>A weakness of an asset or group of assets that can be exploited by one or more threats.</p> </li> <li> <p>Control</p> <p>A measure that reduces the risk of an unwanted incident by preventing, detecting or correcting it.</p> </li> <li> <p>Security management system (SMS)</p> <p>A systematic approach to managing security risks associated with IACS.</p> </li> <li> <p>Security policy</p> <p>A document that communicates an organization's approach to managing security risks associated with IACS.</p> </li> </ul>"},{"location":"convention/iec-62443/#112-iec-62443-1-2-master-glossary-of-terms-and-abbreviations","title":"1.1.2. IEC 62443-1-2 Master glossary of terms and abbreviations","text":"<p>The standard includes a master glossary of terms and abbreviations to provide a common language and understanding of key concepts and terminology that are commonly used in the IEC 62443 series of standards.</p>"},{"location":"convention/iec-62443/#113-iec-62443-1-3-system-security-compliance-metrics","title":"1.1.3. IEC 62443-1-3 System security compliance metrics","text":"<p>IEC 62443-1-3 provides a systematic approach to defining and measuring system security compliance metrics for IACS. This can help organizations ensure that their IACS meet the necessary security requirements and provide a high level of protection against cyber threats.</p> <p>The standard defines several key concepts related to system security compliance metrics, including:</p> <ul> <li> <p>Security level (SL)</p> <p>A measure of the degree of protection provided by a security control or set of controls.</p> </li> <li> <p>Security assurance level (SAL)</p> <p>A measure of the confidence that a security control or set of controls will perform its intended function in a given environment.</p> </li> <li> <p>Security capability level (SCL)</p> <p>A measure of the degree to which a security control or set of controls can meet the security requirements of a given environment.</p> </li> <li> <p>Security compliance level (SCL)</p> <p>A measure of the degree to which a system meets the security requirements specified in a security policy or standard.</p> </li> </ul> <p>IEC 62443-1-3 provides a framework for defining and measuring system security compliance metrics based on these concepts. The framework includes four key steps:</p> <ul> <li> <p>Define security requirements</p> <p>The first step is to define the security requirements for the system based on the security policy or standard. This includes defining the SLs for each security control and the overall system.</p> </li> <li> <p>Assess security capability</p> <p>The next step is to assess the security capability of the system by evaluating the effectiveness of each security control in meeting its SL.</p> </li> <li> <p>Evaluate security assurance</p> <p>The third step is to evaluate the security assurance of the system by assessing the confidence that each security control will perform its intended function in the given environment.</p> </li> <li> <p>Measure compliance</p> <p>The final step is to measure the security compliance of the system by comparing the security capability and security assurance levels to the security requirements.</p> </li> </ul>"},{"location":"convention/iec-62443/#114-iec-62443-1-4-iacs-security-lifecycle-and-use-case","title":"1.1.4. IEC 62443-1-4 IACS security lifecycle and use-case","text":"<p>IEC 62443-1-4 provides a comprehensive framework for the security lifecycle and use-case of IACS, which can help organizations ensure that their systems are designed, implemented, and operated in a secure manner that meets their specific security requirements and objectives.</p> <p>The standard defines a security lifecycle model for IACS that includes six phases:</p> <ul> <li> <p>Initiation</p> <p>In this phase, the need for security is identified, and the scope and objectives of the security program are defined.</p> </li> <li> <p>Requirements and risk assessment</p> <p>In this phase, the security requirements for the IACS are identified, and a risk assessment is conducted to identify potential threats and vulnerabilities.</p> </li> <li> <p>Design and implementation</p> <p>In this phase, the security controls and countermeasures are designed and implemented to mitigate the identified risks.</p> </li> <li> <p>Verification</p> <p>In this phase, the effectiveness of the security controls is verified through testing, evaluation, and auditing.</p> </li> <li> <p>Operation and maintenance</p> <p>In this phase, the IACS is operated and maintained in accordance with the security requirements and the security controls are monitored and updated as necessary.</p> </li> <li> <p>Decommissioning</p> <p>In this phase, the IACS is decommissioned and the security controls are removed or transferred to another system.</p> </li> </ul> <p>The standard also provides guidelines for the use-case of IACS security. A use-case is a description of how the system will be used to meet a specific set of objectives or requirements. The use-case should consider the specific security requirements and the potential threats and vulnerabilities of the system.</p> <p>The use-case guidelines include the following steps:</p> <ol> <li> <p>Identify the system components and their roles in the use-case.</p> </li> <li> <p>Identify the security requirements and objectives for the use-case.</p> </li> <li> <p>Identify the potential threats and vulnerabilities that could impact the use-case.</p> </li> <li> <p>Design and implement security controls and countermeasures to mitigate the identified risks.</p> </li> <li> <p>Verify the effectiveness of the security controls through testing and evaluation.</p> </li> <li> <p>Operate and maintain the system in accordance with the security requirements and the use-case.</p> </li> </ol>"},{"location":"convention/iec-62443/#12-iec-62443-2-policies-and-procedures","title":"1.2. IEC 62443-2 Policies and Procedures","text":"<p>The standard describes the requirements for establishing, implementing, maintaining, and continually improving an IACS SMS.</p>"},{"location":"convention/iec-62443/#121-iec-62443-2-1-security-program-requirements-for-iacs-asset-owners","title":"1.2.1. IEC 62443-2-1 Security program requirements for IACS asset owners","text":"<p>IEC 62443-2-1 provides a framework of requirements and guidelines for implementing a Security Management System (SMS) for Industrial Automation and Control Systems (IACS) for asset owners. The SMS framework helps asset owners to establish, maintain, and continually improve their security programs in a structured and efficient manner.</p> <p>Compliance with IEC 62443-2-1 help asset owners to identify and mitigate security risks in their IACS environments and to establish a culture of security awareness and continuous improvement. The standard provides a comprehensive set of security program requirements that include:</p> <ul> <li> <p>Governance and Organization</p> <p>Establishing and maintaining security governance and organizational structure, roles, and responsibilities for IACS security.</p> </li> <li> <p>Asset Management</p> <p>Managing the inventory of IACS components, understanding the associated risks and vulnerabilities, and defining asset criticality.</p> </li> <li> <p>Risk Assessment</p> <p>Conducting risk assessments of IACS components to identify and prioritize security risks and define appropriate mitigation measures.</p> </li> <li> <p>Security Requirements</p> <p>Establishing and implementing security requirements for IACS components based on risk assessments and security goals.</p> </li> <li> <p>Secure Development</p> <p>Ensuring that IACS components are designed, developed, and tested in accordance with security requirements.</p> </li> <li> <p>Security Operations</p> <p>Defining and implementing security policies, procedures, and controls for secure operations and maintenance of IACS components.</p> </li> <li> <p>Incident and Vulnerability Management</p> <p>Establishing processes for identifying, reporting, assessing, and responding to security incidents and vulnerabilities in IACS components.</p> </li> <li> <p>Continual Improvement</p> <p>Establishing processes for continual improvement of the security program and for measuring and monitoring the effectiveness of security controls.</p> </li> </ul>"},{"location":"convention/iec-62443/#122-iec-62443-2-2-iacs-security-program-ratings","title":"1.2.2. IEC 62443-2-2 IACS Security Program Ratings","text":"<p>IEC 62443-2-2 provides guidance on how to rate and evaluate the effectiveness of an Industrial Automation and Control Systems (IACS) security program. The standard provides a framework for developing security program ratings that can be used to compare different security programs and to identify areas for improvement.</p> <p>The security program ratings are based on a set of security program requirements and guidelines defined in IEC 62443-2-1. These requirements cover all aspects of the security program, including governance and organization, risk assessment, security requirements, secure development, security operations, incident and vulnerability management, and continual improvement.</p> <p>The security program ratings framework can be used by organizations to evaluate their own security programs, or to compare their security programs with those of other organizations. The ratings can also be used by regulators and other stakeholders to evaluate the security posture of critical infrastructure sectors and to identify areas for improvement.</p> <p>The security program ratings framework defines four levels of security program maturity:</p> <ul> <li> <p>Baseline</p> <p>This level indicates that the security program meets the minimum requirements defined in IEC 62443-2-1, but there are areas for improvement.</p> </li> <li> <p>Managed</p> <p>At this level, the security program is managed and monitored, and there are processes in place for measuring and reporting on security program effectiveness.</p> </li> <li> <p>Established</p> <p>This level indicates that the security program is well-established and integrated into the organization's culture and operations. There are mature processes in place for managing security risks, responding to incidents and vulnerabilities, and continually improving the security program.</p> </li> <li> <p>Robust</p> <p>At this level, the security program is considered best-in-class and has achieved a high level of maturity. The organization has a proactive and strategic approach to managing security risks and has demonstrated a commitment to continual improvement.</p> </li> </ul>"},{"location":"convention/iec-62443/#123-iec-tr-62443-2-3-patch-management-in-the-iacs-environment","title":"1.2.3. IEC TR 62443-2-3 Patch management in the IACS environment","text":"<p>IEC TR 62443-2-3 is a Technical Report that provides guidance for patch management in Industrial Automation and Control Systems (IACS) environments. Patch management is an essential process to address vulnerabilities in IACS software and firmware that can be exploited by attackers to compromise the security of IACS.</p> <p>The Technical Report covers the following aspects of patch management:</p> <ul> <li> <p>Patch Management Process</p> <p>Describes the patch management process, which includes identification of patchable assets, vulnerability assessment, patch prioritization, testing, deployment, and validation.</p> </li> <li> <p>Patch Management Roles and Responsibilities</p> <p>Defines the roles and responsibilities of various stakeholders involved in patch management, including asset owners, vendors, system integrators, and service providers.</p> </li> <li> <p>Patch Management Best Practices</p> <p>Provides best practices for effective patch management, such as maintaining an up-to-date inventory of assets, establishing a patch management policy, testing patches before deployment, and validating the effectiveness of patches.</p> </li> <li> <p>Patch Management Tools</p> <p>Describes the tools and technologies that can be used for patch management, such as vulnerability scanners, patch management software, and configuration management databases.</p> </li> </ul>"},{"location":"convention/iec-62443/#124-iec-62443-2-4-security-program-requirements-for-iacs-service-providers","title":"1.2.4. IEC 62443-2-4 Security program requirements for IACS service providers","text":"<p>IEC 62443-2-4 provides security program requirements for IACS service providers, which are entities that provide services related to the design, development, implementation, and maintenance of IACS. The standard recognizes that service providers play a critical role in the security of IACS, as they are responsible for ensuring that their services meet the security requirements of their customers and the IACS industry.</p> <p>The standard defines security program requirements for IACS service providers in the following areas:</p> <ul> <li> <p>Management Commitment</p> <p>Demonstrating management's commitment to information security and establishing a security program that addresses the unique needs of IACS service providers.</p> </li> <li> <p>Security Management System</p> <p>Developing and implementing a security management system (SMS) that includes policies, procedures, and controls for managing security risks and ensuring the confidentiality, integrity, and availability of information.</p> </li> <li> <p>Personnel Security</p> <p>Ensuring that personnel are qualified, trustworthy, and trained to perform their roles and responsibilities in a secure manner.</p> </li> <li> <p>Physical Security</p> <p>Protecting the physical environment of IACS service providers, including facilities, equipment, and assets, against unauthorized access, theft, and damage.</p> </li> <li> <p>Communications and Operations Management</p> <p>Establishing and maintaining secure communications and operations management practices to ensure the secure delivery of services and the protection of IACS information.</p> </li> <li> <p>Access Control</p> <p>Implementing access control measures to ensure that only authorized personnel and entities have access to IACS information and resources.</p> </li> <li> <p>System Development and Maintenance</p> <p>Developing and maintaining secure IACS systems, including processes for testing, validation, and patch management.</p> </li> <li> <p>Incident Management</p> <p>Establishing an incident management program that includes processes for detecting, responding to, and recovering from security incidents.</p> </li> </ul>"},{"location":"convention/iec-62443/#13-iec-62443-3-system-security","title":"1.3. IEC 62443-3 System Security","text":"<p>The standard provides guidance on defining the security requirements for an IACS and establishing security levels based on the criticality of the assets being protected.</p>"},{"location":"convention/iec-62443/#131-iec-tr-62443-3-1-security-technologies-for-iacs","title":"1.3.1. IEC TR 62443-3-1 Security technologies for IACS","text":"<p>IEC TR 62443-3-1 is a technical report that provides an overview of the security technologies that can be used to protect IACS from cyber attacks. The standard is designed to help organizations select and implement appropriate security technologies to protect their IACS environments.</p> <p>The standard covers a wide range of security technologies, including:</p> <ul> <li> <p>Access control</p> <p>The standard provides guidance on access control technologies such as authentication, authorization, and access control lists (ACLs).</p> </li> <li> <p>Network security</p> <p>The standard provides guidance on network security technologies such as firewalls, intrusion detection and prevention systems (IDPS), and virtual private networks (VPNs).</p> </li> <li> <p>Data security</p> <p>The standard provides guidance on data security technologies such as encryption, data loss prevention (DLP), and digital signatures.</p> </li> <li> <p>Application security</p> <p>The standard provides guidance on application security technologies such as secure coding practices, application firewalls, and web application firewalls.</p> </li> <li> <p>Physical security</p> <p>The standard provides guidance on physical security technologies such as surveillance systems, access control systems, and security alarms.</p> </li> <li> <p>Communication security</p> <p>The standard provides guidance on communication security technologies such as secure protocols, secure email, and secure instant messaging.</p> </li> <li> <p>Cloud security</p> <p>The standard provides guidance on cloud security technologies such as encryption, virtualization, and access control.</p> </li> <li> <p>Industrial control system security</p> <p>The standard provides guidance on ICS-specific security technologies such as secure remote access, secure firmware updates, and secure protocol gateways.</p> </li> </ul>"},{"location":"convention/iec-62443/#132-iec-62443-3-2-security-risk-assessment-for-system-design","title":"1.3.2. IEC 62443-3-2 Security risk assessment for system design","text":"<p>The standard defines security requirements as the set of security objectives and measures necessary to ensure the protection of IACS assets, including people, information, and physical assets. The security requirements are derived from the security risk assessment and should be based on the principles of confidentiality, integrity, and availability including identifying potential threats, vulnerabilities, and consequences.</p> <p>The standard also defines security levels as a set of security requirements that must be met to ensure a certain level of security for IACS assets. The security levels are used to provide a common language and framework for describing the security requirements and to enable the comparison of different security solutions. The security requirements cover a range of areas, including access control, data integrity, network security, physical security, and security management.</p> <p>There are four security levels defined in the standard, with level 4 being the highest level of security:</p> <ul> <li> <p>Security Level 1</p> <p>Basic Security This level is appropriate for systems where the impact of a security breach is low, and the likelihood of a breach is also low. The security requirements at this level focus on basic measures such as password policies, user authentication, and basic network segmentation.</p> </li> <li> <p>Security Level 2</p> <p>Enhanced Security This level is appropriate for systems where the impact of a security breach is moderate, and the likelihood of a breach is also moderate. The security requirements at this level include more advanced measures such as secure communication protocols, intrusion detection systems, and more robust access control mechanisms.</p> </li> <li> <p>Security Level 3</p> <p>High Security This level is appropriate for systems where the impact of a security breach is high, and the likelihood of a breach is also high. The security requirements at this level include more advanced measures such as encryption, advanced intrusion detection and prevention systems, and more comprehensive network segmentation.</p> </li> <li> <p>Security Level 4</p> <p>Very High Security This level is appropriate for systems where the impact of a security breach is extremely high, and the likelihood of a breach is also extremely high. The security requirements at this level include the most advanced measures such as physically isolated networks, highly secure access control mechanisms, and advanced threat intelligence systems.</p> </li> </ul> <p>The standard recommends that the security requirements and security levels be documented in a security specification for the IACS. The security specification should include a description of the security objectives, the security requirements for each security level, and the procedures for verifying compliance with the security requirements.</p>"},{"location":"convention/iec-62443/#133-iec-62443-3-3-system-security-requirements-and-security-levels","title":"1.3.3. IEC 62443-3-3 System security requirements and security levels","text":"<p>IEC 62443-3-3 provides a comprehensive framework for defining and implementing appropriate security requirements and security levels based on IEC 62443-3-2 for IACS systems. The standard is designed to help organizations define and implement appropriate security requirements and security levels for their IACS environments.</p> <p>The standard defines security levels as a way to measure the security robustness of an IACS system. Security levels are assigned based on the level of protection needed for the system based on its criticality, the consequences of a security breach, and the potential impact on safety, production, and the environment.</p> <p>The standard provides guidance on defining security requirements for IACS systems based on their security level. The security requirements are divided into categories, including access control, communications security, system integrity, data confidentiality, and data integrity. The requirements for each category vary depending on the security level assigned to the system.</p> <p>IEC 62443-3-3 also provides guidance on the process of selecting and implementing security measures to meet the defined security requirements. The standard recommends a risk-based approach to security, where risks are identified, assessed, and mitigated through the selection and implementation of appropriate security measures.</p>"},{"location":"convention/iec-62443/#14-iec-62443-4-component-security","title":"1.4. IEC 62443-4 Component Security","text":"<p>The standard provides guidance on incorporating security into the product development lifecycle of IACS components, including hardware, software, and firmware.</p>"},{"location":"convention/iec-62443/#141-iec-62443-4-1-secure-product-development-lifecycle-requirements","title":"1.4.1. IEC 62443-4-1 Secure product development lifecycle requirements","text":"<p>IEC 62443-4-1 provides a set of requirements for developing and implementing a secure SDL for IACS products. By following these requirements, organizations can ensure that their products are designed and implemented with security in mind, reducing the likelihood of security vulnerabilities being introduced into the product. The standard is designed to help organizations develop and implement a secure SDL for their IACS products.</p> <p>The standard provides a set of requirements that should be considered when developing and implementing a secure SDL for IACS products. These requirements include:</p> <ul> <li> <p>Security management</p> <p>The standard requires that the SDL should be integrated with the organization's overall security management system (SMS) and that the SDL should be aligned with the organization's security policies and procedures.</p> </li> <li> <p>Secure design</p> <p>The standard provides guidance on designing products with security in mind, including threat modeling, risk assessment, and security requirements definition.</p> </li> <li> <p>Secure coding</p> <p>The standard provides guidance on writing secure code, including coding standards, code reviews, and testing.</p> </li> <li> <p>Security testing</p> <p>The standard provides guidance on testing products for security vulnerabilities, including vulnerability scanning, penetration testing, and fuzz testing.</p> </li> <li> <p>Secure deployment</p> <p>The standard provides guidance on securely deploying products, including secure configuration, hardening, and deployment processes.</p> </li> <li> <p>Security maintenance</p> <p>The standard provides guidance on maintaining product security throughout its lifecycle, including patch management, vulnerability management, and incident response.</p> </li> </ul>"},{"location":"convention/iec-62443/#142-iec-62443-4-2-technical-security-requirements-for-iacs-components","title":"1.4.2. IEC 62443-4-2 Technical security requirements for IACS components","text":"<p>IEC 62443-4-2 provides a set of technical security requirements for IACS components, including network devices, controllers, and sensors. The standard is designed to help organizations ensure that their IACS components are developed and implemented with security in mind.</p> <p>The standard provides a set of technical security requirements that should be considered when developing and implementing IACS components. These requirements include:</p> <ul> <li> <p>Security capabilities</p> <p>The standard requires that IACS components should have security capabilities that are appropriate for the intended use of the component. This includes capabilities such as access control, secure communication, and secure storage.</p> </li> <li> <p>Secure communication</p> <p>The standard requires that IACS components should use secure communication protocols to protect against eavesdropping, tampering, and other attacks.</p> </li> <li> <p>Secure storage</p> <p>The standard requires that IACS components should use secure storage mechanisms to protect against unauthorized access and tampering.</p> </li> <li> <p>Access control</p> <p>The standard requires that IACS components should use access control mechanisms to ensure that only authorized users can access the component and its data.</p> </li> <li> <p>Security monitoring</p> <p>The standard requires that IACS components should have security monitoring capabilities to detect and respond to security incidents.</p> </li> <li> <p>Security updates</p> <p>The standard requires that IACS components should have the ability to receive security updates to address known vulnerabilities.</p> </li> </ul>"},{"location":"convention/iec-62443/#2-terminology","title":"2. Terminology","text":"<p>IEC 62443 provides a comprehensive terminology to describe the different aspects of securing IACS.</p> <ul> <li> <p>Industrial automation and control systems (IACS)</p> <p>Systems that control and monitor industrial processes, such as manufacturing, energy production, and transportation.</p> </li> <li> <p>Cybersecurity</p> <p>The practice of protecting computer systems, networks, and data from unauthorized access, theft, damage, or other malicious actions.</p> </li> <li> <p>Threat</p> <p>A potential event or action that could harm a system or network.</p> </li> <li> <p>Vulnerability</p> <p>A weakness in a system or network that could be exploited by an attacker.</p> </li> <li> <p>Risk</p> <p>The likelihood and potential impact of a threat exploiting a vulnerability.</p> </li> <li> <p>Security management system (SMS)</p> <p>A comprehensive set of policies, procedures, and controls that are implemented to manage an organization's information security risks.</p> </li> <li> <p>Security level</p> <p>A measure of the effectiveness of a security control or set of controls in mitigating a risk.</p> </li> <li> <p>Security capability</p> <p>A set of security features or functions that are designed to address a specific security need.</p> </li> <li> <p>Security zone</p> <p>A logical grouping of assets or systems based on their security requirements.</p> </li> <li> <p>Conduit</p> <p>A pathway for data flow between security zones.</p> </li> </ul>"},{"location":"convention/iso-8601/","title":"ISO 8601","text":"<p>ISO 8601 is an international standard for representing date and time in a machine-readable format. The standard was first published by the International Organization for Standardization (ISO).</p> <ul> <li>1. Category</li> <li>1.1. UTC</li> <li>1.2. Date</li> <li>1.3. Time</li> <li>1.4. Date and Time</li> <li>1.5. Duration</li> <li>1.6. Time Zone</li> <li>1.7. Time Offset</li> <li>1.8. Time Interval</li> <li>1.9. Recurring Time Intervals</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>References</li> </ul>"},{"location":"convention/iso-8601/#1-category","title":"1. Category","text":"<p>ISO 8601 standardized date format provides a clear and unambiguous way of representing dates that can be easily understood and interpreted by users worldwide.</p>"},{"location":"convention/iso-8601/#11-utc","title":"1.1. UTC","text":"<p>Coordinated Universal Time (UTC) is the primary time standard used in ISO 8601 for timekeeping and data exchange. UTC is a time standard that is based on the International System of Units (SI) second, which is defined as the duration of 9,192,631,770 cycles of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium-133 atom.</p> <p>In ISO 8601, UTC is represented as <code>Z</code> (which stands for <code>Zulu</code> time) at the end of the time string, following the time offset, if present. For example, the time 12:30:00 PM UTC would be represented as <code>12:30:00Z</code> in ISO 8601 format.</p> <p>Using UTC in ISO 8601 ensures that the representation of time is standardized and easily understood across different languages and cultures, making it useful for international communication and data exchange.</p>"},{"location":"convention/iso-8601/#12-date","title":"1.2. Date","text":"<p>Represents a specific date in the format <code>YYYY-MM-DD</code>, where:</p> <ul> <li> <p><code>YYYY</code></p> <p>Represents the year with four digits.</p> </li> <li> <p><code>MM</code></p> <p>Represents the month with two digits (01-12).</p> </li> <li> <p><code>DD</code></p> <p>Represents the day with two digits (01-31).</p> </li> </ul>"},{"location":"convention/iso-8601/#13-time","title":"1.3. Time","text":"<p>Represents a specific time in the format <code>hh:mm:ss.sss</code>, where:</p> <ul> <li> <p><code>hh</code></p> <p>Represents the hour with two digits (00-23).</p> </li> <li> <p><code>mm</code></p> <p>Represents the minute with two digits (00-59).</p> </li> <li> <p><code>ss</code></p> <p>Represents the second with two digits (00-59).</p> </li> <li> <p><code>sss</code></p> <p>Represents the millisecond with three digits (000-999).</p> </li> </ul>"},{"location":"convention/iso-8601/#14-date-and-time","title":"1.4. Date and Time","text":"<p>Represents a specific point in time in the format <code>YYYY-MM-DDThh:mm:ss.sssZ</code>, where:</p> <ul> <li> <p><code>YYYY</code></p> <p>Represents the year with four digits.</p> </li> <li> <p><code>MM</code></p> <p>Represents the month with two digits (01-12).</p> </li> <li> <p><code>DD</code></p> <p>Represents the day with two digits (01-31).</p> </li> <li> <p><code>T</code></p> <p>Separates the date from the time.</p> </li> <li> <p><code>hh</code></p> <p>Represents the hour with two digits (00-23).</p> </li> <li> <p><code>mm</code></p> <p>Represents the minute with two digits (00-59).</p> </li> <li> <p><code>ss</code></p> <p>Represents the second with two digits (00-59).</p> </li> <li> <p><code>sss</code></p> <p>Represents the millisecond with three digits (000-999).</p> </li> <li> <p><code>Z</code></p> <p>Represents the time zone in UTC (Coordinated Universal Time).</p> </li> </ul> <p>An example of the ISO 8601 format for <code>April 23, 2023, at 10:30:45 AM</code> in Eastern Standard Time (UTC-5) would be <code>2023-04-23T10:30:45.000-05:00</code>.</p>"},{"location":"convention/iso-8601/#15-duration","title":"1.5. Duration","text":"<p>Duration is used to represent the amount of time between two events or the duration of an event. The format for durations is <code>P[n]Y[n]M[n]DT[n]H[n]M[n]S</code>, where:</p> <ul> <li> <p><code>P</code></p> <p>Represents the duration designator, which indicates that this is a duration value.</p> </li> <li> <p><code>n</code></p> <p>Represents the number of years (Y), months (M), days (D), hours (H), minutes (M), or seconds (S) in the duration. The number can be an integer or a decimal.</p> </li> <li> <p><code>T</code></p> <p>Represents the time designator, which indicates that the time values (hours, minutes, seconds) follow the date values (years, months, days).</p> </li> </ul> <p>For example, a duration of 1 year, 2 months, 3 days, 4 hours, 5 minutes, and 6 seconds would be represented as follows:</p> <pre><code>P1Y2M3DT4H5M6S\n</code></pre> <p>Note that if any of the values are zero, they can be omitted. For example, a duration of 1 hour and 30 minutes would be represented as:</p> <pre><code>PT1H30M\n</code></pre>"},{"location":"convention/iso-8601/#16-time-zone","title":"1.6. Time Zone","text":"<p>The standardized format of time zones in ISO 8601 ensures that the representation of time zones is consistent and easily understood across different languages and cultures, making it useful for international communication and data exchange.</p> <p>In ISO 8601, time zones are represented using the format <code>\u00b1hh:mm</code> or <code>\u00b1hh</code>, where:</p> <ul> <li> <p><code>\u00b1</code></p> <p>The plus (+) or minus (-) sign indicates the direction of the offset from Coordinated Universal Time (UTC).</p> </li> <li> <p><code>hh</code></p> <p>Represents the number of hours in the time zone offset, ranging from 00 to 14.</p> </li> <li> <p><code>mm</code></p> <p>Represents the number of minutes in the time zone offset, ranging from 00 to 59.</p> </li> </ul> <p>For example, the time zone of Eastern Standard Time (EST) in the United States, which is 5 hours behind UTC, would be represented as <code>-05:00</code> in ISO 8601 format. Similarly, the time zone of India Standard Time (IST), which is 5 hours and 30 minutes ahead of UTC, would be represented as <code>+05:30</code>.</p>"},{"location":"convention/iso-8601/#17-time-offset","title":"1.7. Time Offset","text":"<p>In ISO 8601, time offsets are used to indicate the difference between Coordinated Universal Time (UTC) and the local time.</p> <p>The standardized format of time offsets in ISO 8601 ensures that the representation of time differences between UTC and local time is consistent and easily understood across different languages and cultures</p> <p>The time offset is represented as <code>\u00b1hh:mm</code>, where:</p> <ul> <li> <p><code>\u00b1</code></p> <p>The plus (+) or minus (-) sign indicates whether the local time is ahead of or behind UTC, respectively.</p> </li> <li> <p><code>HH</code></p> <p>Represents the number of hours in the time offset, ranging from 00 to 14.</p> </li> <li> <p><code>mm</code></p> <p>Represents the number of minutes in the time offset, ranging from 00 to 59.</p> </li> </ul> <p>For example, a time offset of 5 hours and 30 minutes behind UTC (i.e., India Standard Time) would be represented as <code>UTC-05:30</code> in ISO 8601 format.</p>"},{"location":"convention/iso-8601/#18-time-interval","title":"1.8. Time Interval","text":"<p>In ISO 8601, time intervals are represented using the format <code>P[n]Y[n]M[n]DT[n]H[n]M[n]S</code>, where:</p> <ul> <li> <p><code>P</code></p> <p>Represents the duration designator and is always the first character of the string.</p> </li> <li> <p><code>n</code></p> <p>Represents the number of years (Y), months (M), days (D), hours (H), minutes (M), or seconds (S) in the time interval.</p> </li> </ul> <p>For example, a time interval of 3 years, 2 months, and 15 days would be represented as <code>P3Y2M15D</code> in ISO 8601 format. Similarly, a time interval of 4 hours and 30 minutes would be represented as <code>PT4H30M</code>.</p> <p>ISO 8601 also allows for the use of decimal fractions of a second in the time interval format. For example, a time interval of 1 minute and 30.5 seconds would be represented as <code>PT1M30.5S</code>.</p> <p>The ISO 8601 time interval format is useful for representing durations, elapsed times, and intervals between two points in time. Its standardized format ensures that the representation of time intervals is unambiguous and easily understood across different languages and cultures.</p>"},{"location":"convention/iso-8601/#19-recurring-time-intervals","title":"1.9. Recurring Time Intervals","text":"<p>Represented as a combination of a start time, a duration, and a repetition rate. This format allows users to represent complex patterns of recurring time intervals in a standardized and unambiguous way.</p> <p>The recurring time interval format follows the following syntax:</p> <pre><code>R[repetition]/[duration]/[start time]/[time interval]\n</code></pre> <ul> <li> <p>Repetition</p> <p>This is represented as the letter <code>R</code> followed by the repetition rate in the form of an integer or a decimal number. For example, to represent an event that occurs every hour, the repetition would be <code>R1</code>.</p> </li> <li> <p>Duration</p> <p>This is represented as the letter <code>P</code> followed by a time duration in the format of <code>Thh:mm:ss.sss</code>. For example, to represent an event that lasts for 30 minutes, the duration would be <code>PT30M</code>.</p> </li> <li> <p>Start Time</p> <p>This is represented as a date and time in the format of <code>YYYY-MM-DDThh:mm:ss.sss</code>. This specifies the start time for the recurring time interval.</p> </li> <li> <p>Time Interval</p> <p>This is represented as the letter <code>P</code> followed by a time duration in the format of <code>Thh:mm:ss.sss</code>. This specifies the interval between each recurrence of the event. For example, to represent an event that occurs every hour and lasts for 30 minutes, the time interval would be <code>PT1H</code> (i.e., one hour).</p> </li> </ul> <p>This format can be used to represent a wide range of recurring time intervals, from simple daily events to complex schedules that occur at irregular intervals. An example of a recurring time interval expression for an event that occurs every hour and lasts for 30 minutes, starting on April 23, 2023, at 12:00:00 PM UTC:</p> <pre><code>R/PT30M/2023-04-23T12:00:00Z/PT1H\n</code></pre>"},{"location":"convention/iso-8601/#2-principles","title":"2. Principles","text":"<p>ISO 8601 is based on several principles that aim to ensure that the date and time representation is standardized, unambiguous, and internationally recognized.</p> <ul> <li> <p>Complete representation</p> <p>The date and time representation should include all relevant information, including the year, month, day, hour, minute, second, and time zone.</p> </li> <li> <p>Ordering</p> <p>The date and time values should be ordered from the largest to smallest units, with the year coming first, followed by the month, day, hour, minute, and second.</p> </li> <li> <p>Separators</p> <p>Clear and unambiguous separators should be used between the different parts of the date and time representation, such as <code>-</code> between the year, month, and day, and <code>:</code> between the hours, minutes, and seconds.</p> </li> <li> <p>Unambiguous interpretation</p> <p>The date and time representation should be unambiguous, regardless of the location or language of the user. This is achieved by using a fixed format for the representation, which is easily recognizable and interpretable by anyone who is familiar with ISO 8601.</p> </li> <li> <p>Internationalization</p> <p>The date and time representation should be independent of language and cultural differences, and be easily understood by people from different countries and regions.</p> </li> </ul>"},{"location":"convention/iso-8601/#3-best-practice","title":"3. Best Practice","text":"<p>The best practices ensure that the use of ISO 8601 is consistent, accurate, and easily interpretable by others.</p> <ul> <li> <p>Use the full representation</p> <p>Whenever possible, use the full representation of the date and time format, including the time zone. This ensures that the representation is complete and unambiguous.</p> </li> <li> <p>Use separators consistently</p> <p>Use separators consistently in the date and time representations to make them easier to read and interpret. For example, use <code>-</code> to separate the year, month, and day, and <code>:</code> to separate the hours, minutes, and seconds.</p> </li> <li> <p>Use UTC</p> <p>Whenever possible, use Coordinated Universal Time (UTC) in the date and time representations. This helps to avoid confusion and ensures that the representation is consistent across different time zones.</p> </li> <li> <p>Be aware of time zone differences</p> <p>When working with dates and times across different time zones, be aware of the differences and adjust the representation accordingly. For example, use the time zone offset to represent the difference between the local time and UTC.</p> </li> <li> <p>Validate inputs</p> <p>When parsing or validating date and time inputs, make sure that they conform to the ISO 8601 standard to avoid errors and inconsistencies.</p> </li> <li> <p>Document the use of ISO 8601</p> <p>Document the use of ISO 8601 in the code or documentation to ensure that other developers can easily understand and work with the date and time representations.</p> </li> </ul>"},{"location":"convention/iso-8601/#4-terminology","title":"4. Terminology","text":"<p>Understanding the terms is essential for working with ISO 8601 and ensuring accurate and consistent representations of date and time values.</p> <ul> <li> <p>Date</p> <p>A date is a specific point in time, represented by a combination of year, month, and day.</p> </li> <li> <p>Time</p> <p>Time refers to a specific moment in the day, represented by a combination of hours, minutes, and seconds.</p> </li> <li> <p>Time zone</p> <p>A time zone is a region of the world that observes a standardized time, usually based on the Coordinated Universal Time (UTC) offset.</p> </li> <li> <p>UTC</p> <p>Coordinated Universal Time (UTC) is the primary time standard by which the world regulates clocks and time. It is the basis for the standardization of time zones around the world.</p> </li> <li> <p>Time interval</p> <p>A time interval is a period of time between two points in time, represented by a start and end date and time.</p> </li> <li> <p>Duration</p> <p>Duration refers to the length of time between two points in time, represented by a combination of years, months, days, hours, minutes, and seconds.</p> </li> <li> <p>Recurring time interval</p> <p>A recurring time interval is a period of time that repeats at regular intervals, such as daily, weekly, or monthly.</p> </li> <li> <p>Time offset</p> <p>A time offset is the difference between the local time and Coordinated Universal Time (UTC).</p> </li> <li> <p>Combined date and time</p> <p>A combined date and time is a representation of both the date and time in a single value.</p> </li> <li> <p>Leap year</p> <p>A leap year is a year that is one day longer than a regular year, with an additional day added to the calendar in February.</p> </li> </ul>"},{"location":"convention/iso-8601/#references","title":"References","text":"<ul> <li>ISO iso 8601 date and time format article.</li> </ul>"},{"location":"convention/makefile-conventions/","title":"Makefile Conventions","text":"<p>Makefile Conventions for GNU programs. Makefile is a text file that defines targets and rules which are executed by Make utility.</p>"},{"location":"convention/makefile-conventions/#references","title":"References","text":"<ul> <li>readthedocs Makefile style guide article.</li> </ul>"},{"location":"convention/material-design/","title":"Material Design","text":"<p>Material is an adaptable system of guidelines, components, and tools that support the best practices of user interface design. Backed by open-source code, Material streamlines collaboration between designers and developers, and helps teams quickly build beautiful products.</p> <ul> <li>Color Tool <p>Create, share, and apply color palettes to UI, as well as measure the accessibility level of any color combination.</p> </li> </ul>"},{"location":"convention/material-design/#references","title":"References","text":"<ul> <li>Material material components repository.</li> </ul>"},{"location":"convention/posix/","title":"POSIX","text":"<p>The Portable Operating System Interface POSIX is a group of standards specified by the IEEE Computer Society to maintain compatibility between operating systems. POSIX defines both system- and user-level application programming interfaces (API), as well as command-line shells and utility interfaces, to ensure software compatibility (portability) with variants of Unix and other operating systems.</p>"},{"location":"convention/semantic-versioning/","title":"Semantic Versioning","text":"<p>Semantic Versioning (SemVer) is a versioning scheme for software that aims to convey meaning about the underlying code changes and their impact on compatibility.</p> <p>Semantic Versioning provides a clear and structured way to communicate changes in software versions, making it easier for developers and users to understand the impact of updates on compatibility. It has gained widespread adoption in the software development community and is commonly used in open-source projects and libraries.</p> <ul> <li>1. Category</li> <li>2. Principles</li> <li>3. Best Practice</li> <li>4. Terminology</li> <li>5. References</li> </ul>"},{"location":"convention/semantic-versioning/#1-category","title":"1. Category","text":"<p>The format of a semantic version number is represented as <code>MAJOR.MINOR.PATCH</code>.</p> <p>Components of SemVer:</p> <ol> <li> <p>MAJOR version</p> <p>The <code>MAJOR</code> number is increased by incompatible changes in the software. It indicates that the new version may introduce breaking changes and is not backward compatible with previous versions. Users of the software may need to modify their code or configurations to work with the new version.</p> <ul> <li>Examples: 1.0.0, 2.0.0, 3.0.0</li> </ul> </li> <li> <p>MINOR version</p> <p>The \"MINOR\" number is increased as new features or functions are added to the software in a backward compatible manner. It indicates that the new version brings new capabilities without breaking existing functionality. Users can safely update to a higher MINOR version without worrying about compatibility issues.</p> <ul> <li>Examples: 1.1.0, 1.2.0, 1.3.0</li> </ul> </li> <li> <p>PATCH version</p> <p>The <code>PATCH</code> number is increased by backward-compatible bug fixes or patches to the software. It indicates that the new version includes only fixes for issues and does not introduce any new features. Users can update to a higher PATCH version without any concerns about compatibility.</p> <ul> <li>Examples: 1.0.1, 1.0.2, 1.0.3</li> </ul> </li> <li> <p>Pre-release version (optional)</p> </li> </ol> <p>The <code>Pre-release</code> is an optional label that can be appended to a version to indicate it is a pre-release or development version. It is typically denoted with a hyphen, followed by a series of alphanumeric identifiers like alpha, beta, rc (release candidate), etc.</p> <ul> <li> <p>Examples: 1.0.0-alpha.1, 2.0.0-beta.2</p> </li> <li> <p>Build metadata (optional)</p> </li> </ul> <p>The <code>Build</code> is an optional label that can be added to a version to identify the specific build or revision of the software. It is typically denoted with a plus sign, followed by build metadata such as a commit hash, build number, etc.</p> <ul> <li>Examples: 1.0.0+20130313144700, 2.3.4+sha34578</li> </ul>"},{"location":"convention/semantic-versioning/#2-principles","title":"2. Principles","text":"<p>Semantic versioning (SemVer) is guided by a set of principles that help define its purpose and usage.</p> <p>The principles of SemVer aim to improve version management, facilitate communication between developers and users, and enable informed decisions about software upgrades and compatibility. By adhering to these principles, software projects can effectively communicate the nature of changes and ensure compatibility across different versions.</p> <ul> <li> <p>Backward compatibility</p> <p>SemVer maintains a strong commitment to backward compatibility. It ensures that existing code that relies on a specific version of a software package will continue to work as expected when a new version with a higher MAJOR, MINOR, or PATCH number is released. Backward-compatible changes are introduced in MINOR and PATCH versions, while incompatible changes are reserved for a new MAJOR version.</p> </li> <li> <p>Clear versioning scheme</p> <p>SemVer provides a clear and structured versioning scheme in the format of MAJOR.MINOR.PATCH. This format makes it easy to understand the nature of changes and their impact on compatibility. Each component of the version number has a specific meaning, allowing developers and users to assess the implications of updating to a new version.</p> </li> <li> <p>Incremental versioning</p> <p>SemVer uses incremental version numbers. When making changes to a software package, developers increment the appropriate component of the version number based on the nature of the changes. This incrementation signals the type of changes made and helps users understand the significance of each version.</p> </li> <li> <p>Version precedence</p> <p>SemVer defines a set of rules for comparing and determining the precedence of different versions. It allows users to easily identify which version is newer or older, helping in dependency management and software selection. The precedence rules take into account the MAJOR, MINOR, and PATCH components of the version number.</p> </li> <li> <p>Pre-release and build metadata</p> <p>SemVer allows for the inclusion of pre-release versions and build metadata to provide additional information about a version. Pre-release versions indicate that a version is still in development or testing, while build metadata can be used to include additional information like commit hashes or build numbers. These labels help communicate the status and context of a version.</p> </li> </ul>"},{"location":"convention/semantic-versioning/#3-best-practice","title":"3. Best Practice","text":"<p>When working with Semantic Versioning (SemVer), there are several best practices that can help ensure clarity, consistency, and effective version management.</p> <ul> <li> <p>Follow the SemVer specification</p> <p>Adhere to the guidelines and rules outlined in the SemVer specification. This includes correctly incrementing version numbers based on the type of changes made and using the proper format of MAJOR.MINOR.PATCH.</p> </li> <li> <p>Clearly communicate breaking changes</p> <p>When releasing a new major version (MAJOR increment), clearly document and communicate the breaking changes to users and developers. Provide release notes or a changelog that highlights the incompatibilities and necessary modifications.</p> </li> <li> <p>Use pre-release versions for early development</p> <p>If a version is not yet stable or ready for production use, use pre-release identifiers (e.g., alpha, beta, rc) to indicate its development status. This helps set user expectations and makes it clear that the version may contain bugs or unfinished features.</p> </li> <li> <p>Update dependencies carefully</p> <p>When specifying dependencies on other software packages, consider both the version range and the type of changes introduced in newer versions. Use version constraints (e.g., ^1.0.0, &gt;=2.0.0) to define acceptable ranges and avoid unintentionally pulling in incompatible or breaking changes.</p> </li> <li> <p>Provide clear documentation</p> <p>Maintain thorough documentation for the software package, including details on the usage, changes, and upgrade instructions. Clearly describe how different versions are compatible or incompatible, and provide guidelines for users to handle version updates.</p> </li> <li> <p>Automate version management</p> <p>Utilize tools and workflows that automate version management tasks, such as version number increments, release note generation, and dependency updates. Automation reduces the chances of human error and streamlines the release process.</p> </li> <li> <p>Respect version precedence</p> <p>Understand and respect the version precedence rules defined by SemVer. When comparing versions, ensure that the correct ordering is followed, especially when managing dependencies or selecting the appropriate version for the project.</p> </li> <li> <p>Test compatibility and perform regression testing</p> <p>Regularly test the software package for compatibility with different versions of its dependencies. Perform regression testing to ensure that existing functionality remains intact after making changes or applying patches.</p> </li> <li> <p>Solicit user feedback and engage in the community</p> <p>Encourage users and developers to provide feedback and report issues related to version updates. Engage with the community and address concerns promptly. This collaboration helps improve the quality and stability of the software package.</p> </li> </ul>"},{"location":"convention/semantic-versioning/#4-terminology","title":"4. Terminology","text":"<p>Understanding the terminologies help to navigate and communicate effectively within the context of Semantic Versioning and version management in software development.</p> <ul> <li> <p>Version</p> <p>A specific release or iteration of a software package, identified by a version number.</p> </li> <li> <p>Version Number</p> <p>A numerical representation that identifies a particular version. In SemVer, version numbers are typically in the format of MAJOR.MINOR.PATCH.</p> </li> <li> <p>MAJOR Version</p> <p>The first component of a version number. It indicates incompatible changes with previous versions and signifies significant updates or overhauls to the software.</p> </li> <li> <p>MINOR Version</p> <p>The second component of a version number. It represents the addition of new features or functionality in a backward-compatible manner.</p> </li> <li> <p>PATCH Version</p> <p>The third component of a version number. It indicates bug fixes, patches, or security updates without introducing new features or breaking existing functionality.</p> </li> <li> <p>Pre-release Version</p> <p>A version that is denoted by appending a hyphen followed by identifiers (e.g., alpha, beta, rc) and a numeric value to the version number. Pre-release versions are used to indicate that a version is still under development or testing and may not be stable or fully functional.</p> </li> <li> <p>Build Metadata</p> <p>Additional information associated with a version that is denoted by appending a plus sign followed by identifiers like commit hashes or build numbers. Build metadata is optional and does not affect version precedence or compatibility.</p> </li> <li> <p>Backward Compatibility</p> <p>The ability of newer versions of a software package to work seamlessly with code written for older versions, without requiring modifications.</p> </li> <li> <p>Changelog</p> <p>A document or log that details the changes, additions, and fixes made in each version of a software package.</p> </li> <li> <p>Dependency</p> <p>A software component or library that is required by another software package to function correctly. Dependencies often have their own versioning schemes and compatibility requirements.</p> </li> <li> <p>Compatibility Range</p> <p>A range of acceptable versions that define the compatibility boundaries when specifying dependencies. It is often expressed using version constraints (e.g., ^1.0.0, &gt;=2.0.0) to allow for flexibility while ensuring compatibility.</p> </li> <li> <p>Regression Testing</p> <p>The process of retesting previously functioning features of a software package to ensure that they continue to work as expected after making changes or applying updates.</p> </li> </ul>"},{"location":"convention/semantic-versioning/#5-references","title":"5. References","text":"<ul> <li>Sentenz versioning guide article.</li> <li>Sentnez conventional commits article.</li> <li>Sentnez changelog article.</li> <li>Github semver repository.</li> </ul>"},{"location":"convention/spdx/","title":"SPDX","text":"<p>Software Package Data Exchange (SPDX) is an open standard for communicating software bill of material (SBOM) information, including provenance, components, licenses, copyrights, and security references. SPDX reduces redundant work by providing common formats for organizations, companies and communities to share important data, thereby streamlining and improving compliance, security, and dependability.</p> <p>The SPDX specification is recognized as the international open standard for security, license compliance, and other software supply chain artifacts as <code>ISO/IEC 5962:2021</code>.</p> <ul> <li>1. Category</li> <li>1.1. Specification</li> <li>1.2. License List</li> <li>2. References</li> </ul>"},{"location":"convention/spdx/#1-category","title":"1. Category","text":""},{"location":"convention/spdx/#11-specification","title":"1.1. Specification","text":"<p>The SPDX Specification is a standard format for communicating the components, licenses and copyrights associated with software packages.</p> <p>The SPDX standard helps facilitate compliance with free and open source software licenses by standardizing the way license information is shared across the software supply chain. SPDX reduces redundant work by providing a common format for companies and communities to share important data about software licenses and copyrights, thereby streamlining and improving compliance.</p>"},{"location":"convention/spdx/#12-license-list","title":"1.2. License List","text":"<p>The SPDX License List is an integral part of the SPDX Specification. The SPDX License List is a list of commonly found licenses and exceptions used in free and open or collaborative software, data, hardware, or documentation. The purpose of the SPDX License List is to enable efficient and reliable identification of licenses and exceptions in an SPDX document, in files in general, source files or objects. The SPDX License List includes a standardized short identifier, full name, vetted license text including matching guidelines markup as appropriate, and a canonical permanent URL for each license and exception.</p>"},{"location":"convention/spdx/#2-references","title":"2. References","text":"<ul> <li>Sentenz licenses article.</li> <li>Sentenz license guide article.</li> <li>SPDX tools article.</li> <li>GitHub SPDX license list repository.</li> <li>GitHub SPDX license dataset repository.</li> </ul>"},{"location":"guides/","title":"Guideline","text":"<p>Guidelines based on the defined conventions and style guides and best practices.</p> <ul> <li>Branching Strategies Guide</li> <li>Code Review Guide</li> <li>Commit Message Guide</li> <li>License Guide</li> <li>Software Development Guide</li> <li>Style Guide</li> <li>Versioning Guide</li> <li>XOps Guide</li> </ul>"},{"location":"guides/branching-strategies-guide/","title":"Branching Strategies Guide","text":"<p>Branching strategies defines how a team uses branching to achieve a level of concurrent development.</p> <ul> <li>1. Git Flow</li> <li>2. Scaled Trunk-Based Development</li> </ul>"},{"location":"guides/branching-strategies-guide/#1-git-flow","title":"1. Git Flow","text":"<p>Git Flow is a lightweight, branch-based workflow.</p> <p>The repository contains two base branches with an infinite lifetime:</p> <ul> <li><code>main</code> and <code>develop</code>.</li> </ul> <p>Next to the base branches there are supporting branches with limited life time:</p> <ul> <li><code>feature</code>, <code>release</code> and <code>fix</code>.</li> </ul> <p>The strategy contains the following rules:</p> <ol> <li>There are only two base branches, called main and develop.</li> <li> <p>Direct push to main and develop branches is forbidden.</p> </li> <li> <p>feature branches are created from and merged back into develop branches.</p> </li> <li>Branch naming convention: <code>feature/[ISSUETYPE-ID]-[short-describe]</code>.</li> <li>Pull requests (PR) of feature branches only into develop branch.</li> <li>Merge feature branch into develop by select <code>Squash and merge</code> option on PR merge (Squashing will combine all commits into one).</li> <li> <p>Features should never interact directly with main.</p> </li> <li> <p>Merge to main will create a new release version and deploy to production.</p> </li> <li>Merge of the release branch into the main branch will create a tag with the release version.</li> <li> <p>The release version is created according to the semver convention by analyzing the commit messages.</p> </li> <li> <p>release branches support preparation of a new production release and deploy to staging.</p> </li> <li>Create release branch from develop (naming convention, i.e. <code>release/[1.2.x]</code>).</li> <li>Identified bugs are fixed and committed directly to the release branch.</li> <li> <p>Pull requests (PR) of release branches first into main and next into develop branch.</p> </li> <li> <p>fix branches of production issues that need an immediate fix of a production version.</p> </li> <li>Create a fix branch from main (naming convention, i.e. <code>fix/[ISSUETYPE-ID]-[short-describe]</code>).</li> <li>Commit fix with commit message by conventional commits, i.e <code>fix(scope): what was fixed</code>.</li> <li> <p>Pull requests (PR) of fix branches first into main and next into develop branch.</p> </li> <li> <p>Pull requests (PR) title should follow the conventional commits.</p> </li> <li> <p>Supporting branches are to be deleted after merging.</p> </li> </ol>"},{"location":"guides/branching-strategies-guide/#2-scaled-trunk-based-development","title":"2. Scaled Trunk-Based Development","text":"<p>Scaled Trunk-Based Development is done with short-lived feature branches. One developer over a couple of days (max) and flowing through Pull-Request style code-review &amp; automation (CI/CD) before integrating (merging) into the trunk (main) branch.</p> <p>The repository contains one base branch with an infinite lifetime:</p> <ul> <li><code>trunk</code> or <code>main</code>.</li> </ul> <p>Next to the base branches there are supporting branches with limited life time:</p> <ul> <li><code>feature</code> and <code>release</code>.</li> </ul> <p>The strategy contains the following rules:</p> <ol> <li>There is only one base branch, called main.</li> <li>Direct push to main branch is forbidden.</li> <li>All development happens on the main branch.</li> <li>Pull requests (PR) of short-living feature branches still exist.</li> <li>Unfinished features are hidden behind feature flags until they are publish with an official release.</li> <li>Breaking changes stay behind feature flags.</li> <li> <p>The main contains only backward-compatible changes and feature additions.</p> </li> <li> <p>feature branches are created from and merged back into main branch.</p> </li> <li>Branch naming convention: <code>feature/[ISSUETYPE-ID]-[short-describe]</code>.</li> <li>Pull requests (PR) of feature branches only into main branch.</li> <li> <p>Merge feature branch into main by select <code>Squash and merge</code> option on PR merge (Squashing will combine all commits into one).</p> </li> <li> <p>Merge to main creates a release version.</p> </li> <li>Merge of the feature branch into the main branch will create a tag with the release version.</li> <li> <p>The release version is created according to the semver convention by analyzing the commit messages.</p> </li> <li> <p>release branches are cut from a specific revision of the main.</p> </li> <li>Release from main retroactively by selecting the revision in the past to branch from.</li> <li> <p>Branch naming convention, i.e. <code>release/[1.2.x]</code>.</p> </li> <li> <p>Fix production bugs.</p> </li> <li>Create a feature branch from main (naming convention, i.e. <code>feature/[ISSUETYPE-ID]-[fix]-[short-describe]</code>).</li> <li> <p>Fix bugs on the feature and cherry-picking them back to the release branch.</p> </li> <li> <p>Pull requests (PR) title should follow the conventional commits.</p> </li> <li> <p>Supporting branches are to be deleted after merging.</p> </li> </ol>"},{"location":"guides/clean-code-guide/","title":"Clean Code Guide","text":"<ul> <li>Clean Code</li> <li>Design Principles<ul> <li>DRY</li> <li>KISS</li> <li>YAGNI</li> <li>SOLID</li> <li>Four Rules of Simple Design</li> <li>Elegant Objects</li> </ul> </li> <li>Featured Articles</li> <li>Tutorials</li> <li>Videos</li> <li>Code Examples</li> <li>Git Hub</li> <li>Blogs</li> <li>Books</li> <li>Other</li> <li>Summary</li> <li>General rules</li> <li>Design rules</li> <li>Understandability tips</li> <li>Names rules</li> <li>Functions rules</li> <li>Comments rules</li> <li>Source code structure</li> <li>Objects and data structures</li> <li>Tests</li> <li>Code smells</li> </ul>"},{"location":"guides/clean-code-guide/#clean-code","title":"Clean Code","text":"<p>Inspired by Awesome Clean Code.</p>"},{"location":"guides/clean-code-guide/#design-principles","title":"Design Principles","text":"<p>The code follows these principles</p>"},{"location":"guides/clean-code-guide/#dry","title":"DRY","text":"<p>DRY (Don't Repeat Yourself)</p>"},{"location":"guides/clean-code-guide/#kiss","title":"KISS","text":"<p>KISS (Keep It Short and Simple)</p>"},{"location":"guides/clean-code-guide/#yagni","title":"YAGNI","text":"<p>YAGNI (You Ain't Gonna Need It)</p>"},{"location":"guides/clean-code-guide/#solid","title":"SOLID","text":"<p>(SRP, OCP, LSP, ISP, DIP)</p> <p>Acronym coined by Robert C. Martin (Uncle Bob) to describe the following five principles:</p> <ol> <li>The Single Responsibility Principle A class should have only one reason to change.</li> <li>The Open Closed Principle Software entities should be open for extension, but closed for modification.</li> <li>The Liskov Substitution Principle Derived classes must be substitutable for their base classes.</li> <li>The Interface Segregation Principle Make fine grained interfaces that are client specific.</li> <li>The Dependency Inversion Principle Depend on abstractions, not on concretions.</li> </ol> <p>A local copy of \"Design Principles and Design Patterns\" by Robert C. Marin from objectmentor.com website. More about this principles with examples can be found here</p>"},{"location":"guides/clean-code-guide/#four-rules-of-simple-design","title":"Four Rules of Simple Design","text":"<p>Inspired by Kent Beck's Four Rules of Simple Design A design which:</p> <ol> <li>Passes all tests.</li> <li>Reveals intention.</li> <li>No duplication.</li> <li>Fewest elements.</li> </ol>"},{"location":"guides/clean-code-guide/#elegant-objects","title":"Elegant Objects","text":"<p>Inspired by Elegant Objects</p> <ol> <li>No null.</li> <li>No code in constructors.</li> <li>No getters and setters.</li> <li>No mutable objects.</li> <li>No static methods, not even private ones.</li> <li>No instanceof, type casting, or reflection.</li> <li>No public methods without @Override.</li> <li>No statements in test methods except assertThat.</li> <li>No implementation inheritance.</li> </ol>"},{"location":"guides/clean-code-guide/#featured-articles","title":"Featured Articles","text":"<ul> <li>When A Method Can Do Nothing</li> <li>Tell Don't Ask</li> <li>Converting Queries to Commands</li> <li>Object Calisthenics</li> <li>How Interfaces Are Refactoring Our Code</li> <li>Your Constructors are Completely Irrational</li> <li>Class naming</li> <li>Names objects after things, not actions!</li> <li>Interfacing with hard-to-test third-party code</li> <li>How to Think About the \"new\" Operator with Respect to Unit Testing</li> <li>Avoiding Repetition</li> <li>Design Principles from Design Patterns - A Conversation with Erich Gamma</li> <li>Programming Like Kent Beck</li> <li>How Immutability Helps</li> <li>Clean Architecture</li> <li>Hexagonal Architecture: three principles and an implementation example</li> <li>Getting Started With DDD When Surrounded By Legacy Systems</li> </ul>"},{"location":"guides/clean-code-guide/#tutorials","title":"Tutorials","text":"<ul> <li> <p>Refactoring a JavaScript video store</p> </li> <li> <p>Bowling Game Kata</p> </li> <li>Refactoring from anemic model to DDD</li> <li>Writing Testable Code</li> <li>Essential Skills for Agile Development</li> <li>First Pop Coffee Company</li> <li>Testing legacy by Sandro Mancuso, part 2, and video</li> <li>Live Refactoring Towards Solid Code</li> <li>Introducing the Gilded Rose kata and writing test cases using Approval Tests by Emily Bache, part 2 and part 3</li> <li>Domain Driven Design Crash Course</li> <li>Reactive in practice: A complete guide to event-driven systems development in Java</li> <li>Writing Clean Tests</li> </ul>"},{"location":"guides/clean-code-guide/#videos","title":"Videos","text":"<ul> <li> <p>How To Design A Good API and Why it Matters by Joshua Bloch</p> </li> <li> <p>Inheritance, Polymorphism, &amp; Testing</p> </li> <li>Don't Look For Things!</li> <li>Don't Create Objects That End With -ER</li> <li>8 Lines of Code by Greg Young Accompanied slides</li> <li>19 \u00bd Things to Make You a Better Object Oriented Programmer Nice summary</li> <li>Railway oriented programming: Error handling in functional languages</li> <li>Yves Reynhout - Trench Talk: Evolving a Model</li> <li>Seven Ineffective Coding Habits of Many Programmers by Kevlin Henney</li> <li>Java Optional - The Mother of All Bikesheds by Stuart Marks</li> </ul>"},{"location":"guides/clean-code-guide/#code-examples","title":"Code Examples","text":"<ul> <li> <p>Code Katas</p> </li> <li> <p>hentai</p> </li> <li>Source code for the book, \"Growing Object-Oriented Software, Guided by Tests\"</li> <li>jcabi-email</li> <li>Assignment done for some interview</li> </ul>"},{"location":"guides/clean-code-guide/#git-hub","title":"Git Hub","text":"<ul> <li> <p>Nat Pryce</p> </li> <li> <p>Steve Freeman</p> </li> <li>Matteo Vaccari</li> </ul>"},{"location":"guides/clean-code-guide/#blogs","title":"Blogs","text":"<ul> <li> <p>Martin Fowler</p> </li> <li> <p>Michael Feathers</p> </li> <li>Robert C. Martin (Uncle Bob)</li> <li>Yegor Bugayenko</li> <li>Code Cop</li> <li>The Code Whisperer</li> <li>jbrains.ca</li> <li>Nat Pryce</li> <li>Steve Freeman</li> <li>Mi\u0161ko Hevery</li> <li>Matteo Vaccari</li> <li>Carlo Pescio</li> <li>Jeffrey Palermo</li> <li>Kenneth Truyers</li> <li>Vaughn Vernon</li> <li>Codurance</li> <li>Mihai</li> <li>Enterprise Craftsmanship</li> <li>The Iterate Blog</li> <li>The Holy Java</li> </ul>"},{"location":"guides/clean-code-guide/#books","title":"Books","text":"<ul> <li> <p>Refactoring: Improving the Design of Existing Code by Martin Fowler errata for the book can be found here</p> </li> <li> <p>Clean Code by Robert C. Martin</p> </li> <li>Design Patterns: Elements of Reusable Object-Oriented Software</li> <li>Elegant Objects by Yegor Bugayenko</li> <li>Growing Object-Oriented Software, Guided by Tests</li> </ul>"},{"location":"guides/clean-code-guide/#other","title":"Other","text":"<ul> <li>Code Katas</li> </ul>"},{"location":"guides/clean-code-guide/#summary","title":"Summary","text":"<p>Inspired by Summary of 'Clean code'.</p> <p>Code is clean if it can be understood easily \u2013 by everyone on the team. Clean code can be read and enhanced by a developer other than its original author. With understandability comes readability, changeability, extensibility and maintainability.</p>"},{"location":"guides/clean-code-guide/#general-rules","title":"General rules","text":"<ol> <li>Follow standard conventions.</li> <li>Keep it simple stupid. Simpler is always better. Reduce complexity as much as possible.</li> <li>Boy scout rule. Leave the campground cleaner than you found it.</li> <li>Always find root cause. Always look for the root cause of a problem.</li> </ol>"},{"location":"guides/clean-code-guide/#design-rules","title":"Design rules","text":"<ol> <li>Keep configurable data at high levels.</li> <li>Prefer polymorphism to if/else or switch/case.</li> <li>Separate multi-threading code.</li> <li>Prevent over-configurability.</li> <li>Use dependency injection.</li> <li>Follow Law of Demeter. A class should know only its direct dependencies.</li> </ol>"},{"location":"guides/clean-code-guide/#understandability-tips","title":"Understandability tips","text":"<ol> <li>Be consistent. If you do something a certain way, do all similar things in the same way.</li> <li>Use explanatory variables.</li> <li>Encapsulate boundary conditions. Boundary conditions are hard to keep track of. Put the processing for them in one place.</li> <li>Prefer dedicated value objects to primitive type.</li> <li>Avoid logical dependency. Don't write methods which works correctly depending on something else in the same class.</li> <li>Avoid negative conditionals.</li> </ol>"},{"location":"guides/clean-code-guide/#names-rules","title":"Names rules","text":"<ol> <li>Choose descriptive and unambiguous names.</li> <li>Make meaningful distinction.</li> <li>Use pronounceable names.</li> <li>Use searchable names.</li> <li>Replace magic numbers with named constants.</li> <li>Avoid encodings. Don't append prefixes or type information.</li> </ol>"},{"location":"guides/clean-code-guide/#functions-rules","title":"Functions rules","text":"<ol> <li>Small.</li> <li>Do one thing.</li> <li>Use descriptive names.</li> <li>Prefer fewer arguments.</li> <li>Have no side effects.</li> <li>Don't use flag arguments. Split method into several independent methods that can be called from the client without the flag.</li> </ol>"},{"location":"guides/clean-code-guide/#comments-rules","title":"Comments rules","text":"<ol> <li>Always try to explain yourself in code.</li> <li>Don't be redundant.</li> <li>Don't add obvious noise.</li> <li>Don't use closing brace comments.</li> <li>Don't comment out code. Just remove.</li> <li>Use as explanation of intent.</li> <li>Use as clarification of code.</li> <li>Use as warning of consequences.</li> </ol>"},{"location":"guides/clean-code-guide/#source-code-structure","title":"Source code structure","text":"<ol> <li>Separate concepts vertically.</li> <li>Related code should appear vertically dense.</li> <li>Declare variables close to their usage.</li> <li>Dependent functions should be close.</li> <li>Similar functions should be close.</li> <li>Place functions in the downward direction.</li> <li>Keep lines short.</li> <li>Don't use horizontal alignment.</li> <li>Use white space to associate related things and disassociate weakly related.</li> <li>Don't break indentation.</li> </ol>"},{"location":"guides/clean-code-guide/#objects-and-data-structures","title":"Objects and data structures","text":"<ol> <li>Hide internal structure.</li> <li>Prefer data structures.</li> <li>Avoid hybrids structures (half object and half data).</li> <li>Should be small.</li> <li>Do one thing.</li> <li>Small number of instance variables.</li> <li>Base class should know nothing about their derivatives.</li> <li>Better to have many functions than to pass some code into a function to select a behavior.</li> <li>Prefer non-static methods to static methods.</li> </ol>"},{"location":"guides/clean-code-guide/#tests","title":"Tests","text":"<ol> <li>One assert per test.</li> <li>Readable.</li> <li>Fast.</li> <li>Independent.</li> <li>Repeatable.</li> </ol>"},{"location":"guides/clean-code-guide/#code-smells","title":"Code smells","text":"<ol> <li>Rigidity. The software is difficult to change. A small change causes a cascade of subsequent changes.</li> <li>Fragility. The software breaks in many places due to a single change.</li> <li>Immobility. You cannot reuse parts of the code in other projects because of involved risks and high effort.</li> <li>Needless Complexity.</li> <li>Needless Repetition.</li> <li>Opacity. The code is hard to understand.</li> </ol>"},{"location":"guides/code-review-guide/","title":"Code Review Guide","text":"<p>The code review guide contains suggestions on how to conduct code reviews effectively.</p> <ul> <li>1. Code Review</li> <li>1.1. Look For</li> <li>1.2. Principles</li> <li>1.3. Speed</li> <li>2. Pull Request</li> <li>2.1. Description</li> <li>2.2. Size</li> <li>2.3. Reviewer</li> <li>3. Code Owners</li> <li>4. References</li> </ul> <p>This guide is inspired by:</p> <ul> <li>Google code review guide.</li> <li>SAP code review guide.</li> </ul>"},{"location":"guides/code-review-guide/#1-code-review","title":"1. Code Review","text":"<p>The primary purpose of code review is to ensure that the overall code health of the codebase is improving over time.  Code Review is a measure to ensure software quality through the exchange of knowledge, experience and opinions on complex topics such as data protection, privacy, security, concurrency, accessibility, internationalization, etc.</p>"},{"location":"guides/code-review-guide/#11-look-for","title":"1.1. Look For","text":"<ul> <li> <p>Design</p> <p>Is the code well-designed for appropriate system?</p> </li> <li> <p>Functionality</p> <p>Does the code behave as the author likely intended? Is the way the code behaves good for its users?</p> </li> <li> <p>Complexity</p> <p>Could the code be made simpler? Would another developer be able to easily understand and use this code when they come across it in the future? A particular type of complexity is over-engineering, where developers have made the code more generic than it needs to be, or added functionality that isn\u2019t presently needed by the system.</p> </li> <li> <p>Tests</p> <p>Does the code have correct and well-designed automated tests? Make sure that the tests in the PR are correct, sensible, and useful. Tests do not test themselves, and we rarely write tests for our tests. A human must ensure that tests are valid.</p> </li> <li> <p>Naming</p> <p>Did the developer choose clear names for variables, classes, methods, etc.?</p> </li> <li> <p>Comments</p> <p>Are the comments clear and useful? Usually comments are useful when they explain why some code exists, like the reasoning behind a decision, and should not be explaining what some code is doing. However, regular expressions and complex algorithms often benefit greatly from comments that explain what they\u2019re doing.</p> </li> <li> <p>Style</p> <p>Does the code follow our style guides? Prefix comments with <code>NIT</code> (nitpicking), if some style point isn\u2019t in the style guide and would improve the code but isn\u2019t mandatory.</p> </li> <li> <p>Consistency</p> <p>The style guides is the absolute authority: if something is required by the style guide, the PR should follow the guidelines. Bias towards following the style guide unless the local inconsistency would be too confusing.</p> </li> <li> <p>Documentation</p> <p>Did the developer also update relevant documentation? If documentation is missing, ask for it.</p> </li> </ul>"},{"location":"guides/code-review-guide/#12-principles","title":"1.2. Principles","text":"<ul> <li> <p>In general, reviewers should favor approving a PR once it is in a state where it definitely improves the overall code health of the system being worked on, even if the PR isn't perfect.</p> </li> <li> <p>Technical facts and data overrule opinions and personal preferences.</p> </li> <li> <p>On matters of style, the style guide is the absolute authority. Any purely style point (whitespace, etc.) that is not in the style guide is a matter of personal preference. The style should be consistent with what is there. If there is no previous style, accept the author's.</p> </li> <li> <p>Aspects of software design are almost never a pure style issue or just a personal preference. They are based on underlying principles and should be weighed on those principles, not simply by personal opinion. Sometimes there are a few valid options. If the author can demonstrate (either through data or based on solid engineering principles) that several approaches are equally valid, then the reviewer should accept the preference of the author. Otherwise the choice is dictated by standard principles of software design.</p> </li> <li> <p>If no other rule applies, then the reviewer may ask the author to be consistent with what is in the current codebase, as long as that doesn't worsen the overall code health of the system.</p> </li> </ul>"},{"location":"guides/code-review-guide/#13-speed","title":"1.3. Speed","text":"<p>Optimize for the speed at which a team of developers can produce a product.</p> <p>Unless the reviewer is in the middle of a focused task, a code review should be performed shortly after the request is received. A business day is the maximum amount of time required to respond to a code review request (i.e., first thing the next morning).</p> <p>If the reviewer is in the middle of a focused task, such as writing code, do not interrupt the task to perform a code review. Research has shown that it can take a long time for a developer to get back into a smooth flow of development after being interrupted. So interrupting the task while coding is actually more expensive to the team than making another developer wait a bit for a code review. Instead, wait for a break in the task before responding to a request for review. This could be when a current coding task is completed, after lunch, returning from a meeting, coming back from the breakroom, etc.</p>"},{"location":"guides/code-review-guide/#2-pull-request","title":"2. Pull Request","text":"<p>Git-based platforms provide features such as Gerrit <code>Change</code>, GitHub <code>Pull Request (PR)</code> and Google <code>Changelist (CL)</code> that support the submission of changes to version control.</p>"},{"location":"guides/code-review-guide/#21-description","title":"2.1. Description","text":"<p>Follow the merge commit message guide to create a PR description.</p> <p>Read the examples below to get a sense of bad and good PR descriptions. In summary:</p> <ul> <li>The first line should be short, focused, and stand alone, and the PR description body should include informative details that help reviewers and future code searchers understand each PR\u2019s effect.</li> <li> <p>Review the description before submitting the PR.</p> </li> <li> <p>Examples of bad descriptions.</p> <p>Descriptions like <code>fix bug</code> are an inadequate PR description. What bug? What did the author to fix it? Other similarly bad descriptions, that don't provide enough useful information:</p> <ul> <li>fix: fix build</li> <li>fix: add patch</li> <li>refactor: moving code from A to B</li> <li>feat: phase 1</li> <li>feat: add convenience functions</li> <li>refactor: kill weird URLs</li> </ul> </li> <li> <p>Examples of good descriptions.</p> </li> <li> <p>Description of a functionality change:       &gt; perf(rpc): remove size limit on RPC server message freelist (#1123)       &gt;       &gt; Servers like FizzBuzz have very large messages and would benefit from reuse. Make the freelist larger, and add a goroutine that frees the freelist entries slowly over time, so that idle servers eventually release all freelist entries.       &gt;       &gt; Closes #5813</p> <p>The first few words describe what the PR actually does. The rest of the description talks about the problem being solved, why this is a good solution, and a bit more information about the specific implementation.</p> </li> <li> <p>Description of a code refactoring:       &gt; refactor: construct a Task with a TimeKeeper to use its TimeStr and Now methods (#1123)       &gt;       &gt; Add a Now method to Task, so the borglet() getter method can be removed (which was only used by OOMCandidate to call borglet\u2019s Now method). This replaces the methods on Borglet that delegate to a TimeKeeper.       &gt;       &gt; Allowing Tasks to supply Now is a step toward eliminating the dependency on Borglet. Eventually, collaborators that depend on getting Now from the Task should be changed to use a TimeKeeper directly, but this has been an accommodation to refactoring in small steps.       &gt;       &gt; Continuing the long-range goal of refactoring the Borglet Hierarchy.       &gt;       &gt; Closes #2134       &gt; Related #5813</p> <p>The first line describes what the PR does and how this is a change from the past. The rest of the description talks about the specific implementation, the context of the PR, that the solution isn\u2019t ideal, and possible future direction. It also explains why this change is being made.</p> </li> <li> <p>Description of a new feature:       &gt; feat: create a Python3 build rule for <code>status.py</code> (#1123)       &gt;       &gt; This allows consumers who are already using this as in Python3 to depend on a rule that is next to the original status build rule instead of somewhere in their own tree. It encourages new consumers to use Python3 if they can, instead of Python2, and significantly simplifies some automated build file refactoring tools being worked on currently.       &gt;       &gt; Closes #5813</p> <p>The first sentence describes what\u2019s actually being done. The rest of the description explains why the change is being made and gives the reviewer a lot of context.</p> </li> </ul>"},{"location":"guides/code-review-guide/#22-size","title":"2.2. Size","text":"<p>In general, the right size for a PR is one self-contained change.</p> <p>Small PRs are:</p> <ul> <li> <p>Reviewed more quickly.</p> <p>It's easier for a reviewer to find five minutes several times to review small PRs than to set aside a 30 minute block to review one large PR.</p> </li> <li> <p>Reviewed more thoroughly.</p> <p>With large changes, reviewers and authors tend to get frustrated by large volumes of detailed commentary shifting back and forth\u2014sometimes to the point where important points get missed or dropped.</p> </li> <li> <p>Less likely to introduce bugs.</p> <p>Since you're making fewer changes, it's easier for you and your reviewer to reason effectively about the impact of the PR and see if a bug has been introduced.</p> </li> <li> <p>Less wasted work if they are rejected.</p> <p>If you write a huge PR and then your reviewer says that the overall direction is wrong, you've wasted a lot of work.</p> </li> <li> <p>Easier to merge.</p> <p>Working on a large PR takes a long time, so you will have lots of conflicts when you merge, and you will have to merge frequently.</p> </li> <li> <p>Easier to design well.</p> <p>It's a lot easier to polish the design and code health of a small change than it is to refine all the details of a large change.</p> </li> <li> <p>Less blocking on reviews.</p> <p>Sending self-contained portions of your overall change allows you to continue coding while you wait for your current PR in review.</p> </li> <li> <p>Simpler to roll back.</p> <p>A large PR will more likely touch files that get updated between the initial PR submission and a rollback PR, complicating the rollback (the intermediate PRs will probably need to be rolled back too).</p> </li> </ul> <p>NOTE Reviewers have discretion to reject your change outright for the sole reason of it being too large. Usually they will request to split it into a series of smaller changes. It's easier to just write small PRs in the first place.</p>"},{"location":"guides/code-review-guide/#23-reviewer","title":"2.3. Reviewer","text":"<ul> <li> <p>Draft Pull Request</p> <p>Create a pull request draft and add a reviewer in the early stages of feature development.</p> </li> <li> <p>Pull Request by Feature Flags</p> <p>Add a feature to the base branch using feature flags.</p> </li> <li> <p>Random Pull Request Reviews</p> <p>Add a reviewer at random. This creates an optimal distribution for the exchange of knowledge, experience and opinions.</p> </li> <li> <p>Code Owners</p> <p>A advanced concepts to select reviewers. Use a CODEOWNERS file to define individuals or teams that are responsible for code in a repository.</p> </li> </ul>"},{"location":"guides/code-review-guide/#3-code-owners","title":"3. Code Owners","text":"<p>Define individuals as code owners or teams that are responsible for code in a repository. People with admin or owner permissions can set up a CODEOWNERS file in a repository.</p> <p>Code owners are automatically requested for review when someone opens a pull request that modifies code that they own. Code owners are not automatically requested to review draft pull requests. When you mark a draft pull request as ready for review, code owners are automatically notified. If you convert a pull request to a draft, people who are already subscribed to notifications are not automatically unsubscribed.</p> <p>When someone with admin or owner permissions has enabled required reviews, they also can optionally require approval from a code owner before the author can merge a pull request in the repository. Repository owners can add branch protection rules to ensure that changed code is reviewed by the owners of the changed files.</p> <p>To use a CODEOWNERS file, create a new file called CODEOWNERS in the <code>root</code>, <code>docs/</code>, or <code>.github/</code> directory of the repository, in the base branch of the pull request. CODEOWNERS files must be under 3 MB in size. If any line in your CODEOWNERS file contains invalid syntax, that line will be skipped. When you navigate to the CODEOWNERS file in your repository on GitHub.com, you can see any errors highlighted.</p> <p>Example of a CODEOWNERS file:</p> <pre><code># This is a comment.\n# Each line is a file pattern followed by one or more owners.\n\n# These owners will be the default owners for everything in\n# the repo. Unless a later match takes precedence,\n# @global-owner1 and @global-owner2 will be requested for\n# review when someone opens a pull request.\n*       @global-owner1 @global-owner2\n\n# Order is important; the last matching pattern takes the most\n# precedence. When someone opens a pull request that only\n# modifies JS files, only @js-owner and not the global\n# owner(s) will be requested for a review.\n*.js    @js-owner\n\n# You can also use email addresses if you prefer. They'll be\n# used to look up users just like we do for commit author\n# emails.\n*.go docs@example.com\n\n# Teams can be specified as code owners as well. Teams should\n# be identified in the format @org/team-name. Teams must have\n# explicit write access to the repository. In this example,\n# the octocats team in the octo-org organization owns all .txt files.\n*.txt @octo-org/octocats\n\n# In this example, @doctocat owns any files in the build/logs\n# directory at the root of the repository and any of its\n# subdirectories.\n/build/logs/ @doctocat\n\n# The `docs/*` pattern will match files like\n# `docs/getting-started.md` but not further nested files like\n# `docs/build-app/troubleshooting.md`.\ndocs/*  docs@example.com\n\n# In this example, @octocat owns any file in an apps directory\n# anywhere in your repository.\napps/ @octocat\n\n# In this example, @doctocat owns any file in the `/docs`\n# directory in the root of your repository and any of its\n# subdirectories.\n/docs/ @doctocat\n\n# In this example, any change inside the `/scripts` directory\n# will require approval from @doctocat or @octocat.\n/scripts/ @doctocat @octocat\n\n# In this example, @octocat owns any file in the `/apps`\n# directory in the root of your repository except for the `/apps/github`\n# subdirectory, as its owners are left empty.\n/apps/ @octocat\n/apps/github\n</code></pre> <p>There are some syntax rules for gitignore files that do not work in CODEOWNERS files:</p> <ul> <li>Escaping a pattern starting with <code>#</code> using <code>\\</code> so it is treated as a pattern and not a comment</li> <li>Using <code>!</code> to negate a pattern</li> <li>Using <code>[ ]</code> to define a character range</li> </ul>"},{"location":"guides/code-review-guide/#4-references","title":"4. References","text":"<ul> <li>Google code review book.</li> <li>Google culture of review book.</li> <li>Google modern code review article.</li> </ul>"},{"location":"guides/commit-message-guide/","title":"Commit Message Guide","text":"<p>This specification is inspired by Angular and follows the Conventional Commits.</p> <ul> <li>1. Commit Message</li> <li>1.1. Header<ul> <li>1.1.1. Type</li> <li>1.1.2. Scope</li> </ul> </li> <li>1.2. Body</li> <li>1.3. Footer<ul> <li>1.3.1. Referencing Issues</li> <li>1.3.2. Breaking Changes</li> </ul> </li> <li>2. Merge Commit Message</li> <li>3. Update a Commit Message</li> <li>4. Lint a Commit Message</li> <li>5. References</li> </ul>"},{"location":"guides/commit-message-guide/#1-commit-message","title":"1. Commit Message","text":"<p>A commit message consists of a <code>header</code>, a <code>body</code>, and a <code>footer</code>.</p> <pre><code>&lt;header&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre> <p>The <code>header</code> is mandatory and must conform to the Commit Message Header format The <code>header</code> should not be longer than 70 characters.</p> <p>The <code>body</code> is mandatory for all commits except for those of type docs. When the body is present it must be at least 20 characters long and should be wrapped at 80 characters, and must conform to the Commit Message Body format.</p> <p>The <code>footer</code> is optional. The Commit Message Footer format describes what the footer is used for and the structure it must have.</p> <p>Provide in the <code>subject</code> field a succinct description of the change:</p> <ul> <li>use the imperative, present tense: <code>change</code> not <code>changed</code> nor <code>changes</code></li> <li>includes motivation for the change and contrasts with previous behavior</li> <li>don't capitalize the first letter</li> <li>no period (.) at the end</li> </ul> <p>Full example:</p> <pre><code>fix(middleware): ensure Range headers adhere more closely to RFC 2616\n\nAdd one new dependency, use `range-parser` (express dependency) to compute\nrange. It is more well-tested in the wild.\n</code></pre>"},{"location":"guides/commit-message-guide/#11-header","title":"1.1. Header","text":"<p>The <code>&lt;type&gt;</code> and <code>&lt;subject&gt;</code> fields are mandatory. The <code>(&lt;scope&gt;)</code> field is optional, e.g. if the change is a global or difficult to assign to a single component. The <code>type</code> and <code>scope</code> should always be lowercase.</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n</code></pre>"},{"location":"guides/commit-message-guide/#111-type","title":"1.1.1. Type","text":"<ul> <li> <p>fix</p> <p>A bug fix (this correlates with <code>PATCH</code> in semantic versioning).</p> </li> <li> <p>feat</p> <p>A new feature (this correlates with <code>MINOR</code> in Semantic Versioning).</p> </li> <li> <p>build</p> <p>Changes that affect the build system or external dependencies.</p> </li> <li> <p>chore</p> <p>Other changes that don`t modify src or test files.</p> </li> <li> <p>ci</p> <p>Changes to our CI configuration files and scripts.</p> </li> <li> <p>docs</p> <p>Documentation only changes.</p> </li> <li> <p>perf</p> <p>A code change that improves performance.</p> </li> <li> <p>refactor</p> <p>A code change that neither fixes a bug nor adds a feature.</p> </li> <li> <p>test</p> <p>Adding missing tests or correcting existing tests.</p> </li> <li> <p>revert</p> <p>Revert a previous commit</p> </li> </ul>"},{"location":"guides/commit-message-guide/#112-scope","title":"1.1.2. Scope","text":"<p>A scope can be provided to a commit\u2019s type, to provide additional contextual information and is contained within parenthesis.</p> <pre><code>feat(parser): allow provided config object to extend other configs\n</code></pre> <p>or</p> <pre><code>refactor(internal): modify function to provide additional information\n</code></pre>"},{"location":"guides/commit-message-guide/#12-body","title":"1.2. Body","text":"<p>Use the imperative, present tense: <code>fix</code> not <code>fixed</code> nor <code>fixes</code>.</p> <p>Explain the motivation for the change in the commit message body. This commit message should explain why you are making the change. Include a comparison of the previous behavior with the new behavior in order to illustrate the impact of the change.</p>"},{"location":"guides/commit-message-guide/#13-footer","title":"1.3. Footer","text":""},{"location":"guides/commit-message-guide/#131-referencing-issues","title":"1.3.1. Referencing Issues","text":"<p>Closed issues should be listed on a separate line in the footer prefixed with <code>Closes</code> keyword like this:</p> <pre><code>Closes #&lt;issue number&gt;\n</code></pre> <p>or in the case of multiple issues:</p> <pre><code>Closes #&lt;issue number&gt;, #&lt;issue number&gt;, #&lt;issue number&gt;\n</code></pre>"},{"location":"guides/commit-message-guide/#132-breaking-changes","title":"1.3.2. Breaking Changes","text":"<p>The footer can contain information about <code>breaking changes</code> and <code>deprecations</code> and is also the place to reference Azure stories, GitHub issues, Jira tickets, and other PRs that this commit closes or is related to.</p> <p>Breaking change section should start with the phrase <code>BREAKING CHANGE:</code> followed by a <code>subject</code> of the breaking change, a blank line, and a detailed description of the breaking change that also includes migration instructions.</p> <pre><code>BREAKING CHANGE: &lt;breaking change subject&gt;\n\n&lt;breaking change description + migration instructions&gt;\n\nFixes #&lt;issue number&gt;\n</code></pre> <p>Similarly, a Deprecation section should start with <code>DEPRECATED:</code> followed by a short description of what is deprecated, a blank line, and a detailed description of the deprecation that also mentions the recommended update path.</p> <pre><code>DEPRECATED: &lt;what is deprecated&gt;\n\n&lt;deprecation description + recommended update path&gt;\n\nCloses #&lt;issue number&gt;, #&lt;issue number&gt;, #&lt;issue number&gt;\n</code></pre> <p>NOTE A commit that has a footer <code>BREAKING CHANGE:</code>, or appends a <code>!</code> after the type/scope, introduces a breaking API change (correlating with <code>MAJOR</code> in Semantic Versioning).</p> <p>For example:</p> <pre><code>feat: modify validation tooling (#6658)\n\nBREAKING CHANGE: bash analyzer scripts are deprecated\n\nThe bash analyzer scripts are replaced with the pre-commit tool.\n\nCloses #5684\n</code></pre>"},{"location":"guides/commit-message-guide/#2-merge-commit-message","title":"2. Merge Commit Message","text":"<p>Unlike the standard commit messages, a merge commit message consists of additional information about the related Azure stories, GitHub issues, Jira tickets, and other PRs that are closed with this merge commit.</p> <ul> <li>header</li> </ul> <p>A commit message to merge a pull request (PR) modifies the <code>header</code> with the associated PR number.</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; (&lt;pr number&gt;)\n</code></pre> <p>For example:</p> <pre><code>feat(compiler): propagate standalone flag to runtime (#44973) \n</code></pre> <ul> <li>footer</li> </ul> <p>The <code>footer</code> includes related work items like Azure stories, GitHub issues or Jira tickets.</p> <p>For example:</p> <pre><code>Closes #&lt;issue number&gt;\n</code></pre> <p>Full example:</p> <pre><code>feat(compiler): propagate standalone flag to runtime (#44973) \n\nThis commit carries the `standalone` flag forward from a directive/pipe\ninto its generated directive/pipe definition, allowing the runtime to\nrecognize standalone entities.\n\nCloses #43484\n</code></pre>"},{"location":"guides/commit-message-guide/#3-update-a-commit-message","title":"3. Update a Commit Message","text":"<p>A reviewer might often suggest changes to a commit message (e.g., to add more context to a change or to comply with the commit message guide).</p> <p>In order to update the commit message of the last commit on the feature branch:</p> <ol> <li> <p>Check out the feature branch:</p> <pre><code>git checkout feature/7845-fix-opreation\n</code></pre> </li> <li> <p>Amend the last commit and modify the commit message:</p> <pre><code>git commit --amend\n</code></pre> </li> <li> <p>Push to repository:</p> <pre><code>git push --force-with-lease\n</code></pre> </li> </ol> <p>NOTE If a commit message of an earlier commit need to update, use <code>git rebase</code> in interactive mode. See the git docs for more details.</p>"},{"location":"guides/commit-message-guide/#4-lint-a-commit-message","title":"4. Lint a Commit Message","text":"<p>commitlint tooling for conventional commits. Checks if the commit messages meet the Conventional Commits format.</p>"},{"location":"guides/commit-message-guide/#5-references","title":"5. References","text":"<ul> <li>Karma git commit message guideline.</li> </ul>"},{"location":"guides/license-guide/","title":"License Guide","text":"<p>The licenses are classified according to the Google License Classification. For a deeper understanding, please refer to the link.</p> <ul> <li>1. Classification</li> <li>2. License</li> <li>2.1. External Use</li> <li>2.2. Internal Use</li> <li>3. Tools</li> <li>4. References</li> </ul>"},{"location":"guides/license-guide/#1-classification","title":"1. Classification","text":"<p>Information on to comply with open source licenses for externally distributed software, what these licenses are, what categories they fall under, and how to handle them.</p> <ul> <li> <p>Restricted licenses</p> <p>The <code>restricted</code> licenses. Licenses in this category require mandatory source distribution (including product source code) if a company ships a product that includes third-party code protected by such a license. Also, any use of source code under licenses of this type in a product will taint product source code with the restricted license. Third-party software made available under one of these licenses must not be part of company products that are delivered to outside customers. Such prohibited distribution methods include <code>client</code> (downloadable client software) and <code>embedded</code>.</p> </li> <li> <p>Reciprocal licenses</p> <p>The <code>reciprocal</code> licenses apply the same permissions and limitations set by the Restricted category of licenses, but with one important exception. The obligation to make source code available to recipients of software that depends on a Reciprocally licensed library only extends to the contents of the library itself, together with any additions to or modifications of that individual library. Unlike GPL and other Restricted licenses, the other components of the software depending on the Reciprocal library do not need to have their source released under the corresponding reciprocal license.</p> <p>The distribution of software that contains reciprocally licensed components triggers our obligation to make the corresponding source code of those components available to end users. Reciprocally licensed third-party source code must be made available by mirroring it to a repository. However, if there are compelling reasons not to mirror the internal modifications externally or share them upstream, then the reciprocally licensed package must not be used in software that is distributed outside of company.</p> </li> <li> <p>Notice licenses</p> <p>The <code>notice</code> licenses contain few restrictions, allowing original or modified third-party software to be shipped in any product without endangering or encumbering product source code. All of the licenses in this category do, however, have an \"original Copyright notice\" or \"advertising clause\", wherein any external distributions must include the notice or clause specified in the license.</p> <p>For downloaded <code>client</code> products, the necessary notice files can be installed in a subdirectory of the product. For <code>embedded</code> products, that make use of <code>notice</code> licensed source code, the required notices must either be linked on an \"about\" page or possibly included in printed documentation. If the software at issue is a binary that is being sent to end-users devices, that users only interact with through a command line interface, then we would recommend setting up a flag that can be passed through the tool to make the console read out these notices (e.g. --notices). Alternatively, these notices can be concatenated into a THIRD_PARTY_NOTICES.txt file that is bundled with the software, as long as the software is being sent to a device that permits the end user to view the text of that file.</p> </li> <li> <p>Permissive license</p> <p>The <code>permissive</code> license type can be used in (relatively rare) cases where third-party software is under a license (not \"Public Domain\" or \"free for any use\" like <code>unencumbered</code>) that is even more lenient than a <code>notice</code> license. Use the <code>permissive</code> license type when even a copyright notice is not required for license compliance. For example, this license type can be used when a third-party <code>cc_library()</code> rule only adds header files (licensed under, say, the GNU LGPL) to the include path for compilation, but no actual binary libraries or source files. (Yes, these cases exist, such as with header files that define interfaces to dynamically loaded libraries that are present in the operating system distribution itself.)</p> </li> <li> <p>Unencumbered licenses</p> <p>In addition to the Public Domain and Authored code cases, actual licenses exist that generally declare that the code is \"free for any use\".</p> <p><code>Public domain and \"Free For Any Use\"</code></p> <p>NOTE Apply this label with caution. Public domain is a complex topic that requires legal analysis on a case by case basis.</p> <p>The 'unencumbered' licenses include those often referred to as \"Public Domain\" or \"free for any use.\" These licenses have even fewer restrictions than licenses in the 'notice' category, but this licence type should be assigned to a third-party build rule with caution, because:</p> <ul> <li>It is non-trivial to place software in the public domain. While an explicit disclaimer of copyright protection is required, that is not necessarily sufficient.</li> <li>While works of the U.S. Government are placed in the public domain, works created by contractors under government contracts may not be.</li> </ul> <p>Third-party software whose copyright notice explicitly states that the work has been placed in the \"public domain\" by the author or is \"free for any use\" will be assigned the 'unencumbered' license type. As discussed in the links above, this does not necessarily mean that the work is unprotected by copyright, but granting of a lenient license by the author can most likely be assumed.</p> <p><code>Company-Authored code</code></p> <p>Another instance where the 'unencumbered' license type is appropriate is company-authored code in a third-party package (such as tests added by company) that a company has not open sourced but that exists in the //third_party package with the third-party code. This is typically handled by the licenses= build directive parameter for the specific build rules in the //third_party package that are building the company-authored code (while the rest of the package has its license type set by the file-scoped licenses() directive).</p> <p>As noted at go/thirdparty/documentation#google-owned-code, The LICENSE file for company-authored, not-yet-released code should only contain the following text: \"company owned, no external contributions.\"</p> </li> </ul>"},{"location":"guides/license-guide/#2-license","title":"2. License","text":"<p>Classification of licenses and assignment of severity levels.</p> Classification Severity Forbidden CRITICAL Restricted HIGH Reciprocal MEDIUM Notice LOW Permissive LOW Unencumbered LOW Unknown UNKNOWN <p>Classification of known licenses based on Google License Classifier.</p>"},{"location":"guides/license-guide/#21-external-use","title":"2.1. External Use","text":"<p>Forbidden:</p> <ul> <li>AGPL-1.0</li> <li>AGPL-3.0</li> <li>CC-BY-NC-1.0</li> <li>CC-BY-NC-2.0</li> <li>CC-BY-NC-2.5</li> <li>CC-BY-NC-3.0</li> <li>CC-BY-NC-4.0</li> <li>CC-BY-NC-ND-1.0</li> <li>CC-BY-NC-ND-2.0</li> <li>CC-BY-NC-ND-2.5</li> <li>CC-BY-NC-ND-3.0</li> <li>CC-BY-NC-ND-4.0</li> <li>CC-BY-NC-SA-1.0</li> <li>CC-BY-NC-SA-2.0</li> <li>CC-BY-NC-SA-2.5</li> <li>CC-BY-NC-SA-3.0</li> <li>CC-BY-NC-SA-4.0</li> <li>Commons-Clause</li> <li>Facebook-2-Clause</li> <li>Facebook-3-Clause</li> <li>Facebook-Examples</li> <li>WTFPL</li> </ul> <p>Restricted:</p> <ul> <li>BCL</li> <li>CC-BY-ND-1.0</li> <li>CC-BY-ND-2.0</li> <li>CC-BY-ND-2.5</li> <li>CC-BY-ND-3.0</li> <li>CC-BY-ND-4.0</li> <li>CC-BY-SA-1.0</li> <li>CC-BY-SA-2.0</li> <li>CC-BY-SA-2.5</li> <li>CC-BY-SA-3.0</li> <li>CC-BY-SA-4.0</li> <li>CERN 2</li> <li>GPL-1.0</li> <li>GPL-2.0</li> <li>GPL-2.0-with-autoconf-exception</li> <li>GPL-2.0-with-bison-exception</li> <li>GPL-2.0-with-classpath-exception</li> <li>GPL-2.0-with-font-exception</li> <li>GPL-2.0-with-GCC-exception</li> <li>GPL-3.0</li> <li>GPL-3.0-with-autoconf-exception</li> <li>GPL-3.0-with-GCC-exception</li> <li>LGPL-2.0</li> <li>LGPL-2.1</li> <li>LGPL-3.0</li> <li>NPL-1.0</li> <li>NPL-1.1</li> <li>OSL-1.0</li> <li>OSL-1.1</li> <li>OSL-2.0</li> <li>OSL-2.1</li> <li>OSL-3.0</li> <li>QPL-1.0</li> <li>Sleepycat</li> </ul> <p>NOTE <code>LGPL linking requirements</code></p> <p>Though marked as restricted, LGPL licensed components can be used without observing of the restricted type requirements if the component is dynamically linked.</p> <p>If distributing software outside of the company that incorporates any LGPL licensed libraries, there are ways of meeting the license obligations that are less restrictive than the GPL's obligations if certain requirements are observed. The following steps are required in order to be able to take advantage of the reduced requirements:</p> <ul> <li>The LGPL licensed library must be used as a shared library (dynamically linked).</li> <li>For LGPL v3 only, the user must be able to replace the shared library with a compatible library and have it work (the user must be able to swap out libraries on the device).</li> <li>Customers must receive either object files or source code (including any modifications) of the LGPL licensed library.</li> </ul> <p>NOTE YOU are NOT required to provide support for the LGPL licensed library. YOU are NOT required to work around bugs or problems in compatible libraries, or perform tech support for customer modifications. YOU are required to make library replacement possible (for LGPL v3). YOU are required to permit user modification of the software, and to not prohibit user modification in the terms of service.</p> <p>These requirements only apply to products shipped to end users. Software that is run internally (even if displayed on the web to the user) does not have to meet these requirements.</p> <p>Reciprocal:</p> <ul> <li>APSL-1.0</li> <li>APSL-1.1</li> <li>APSL-1.2</li> <li>APSL-2.0</li> <li>CDDL-1.0</li> <li>CDDL-1.1</li> <li>CPL-1.0</li> <li>EPL-1.0</li> <li>EPL-2.0</li> <li>FreeImage</li> <li>IPL-1.0</li> <li>MPL-1.0</li> <li>MPL-1.1</li> <li>MPL-2.0</li> <li>Ruby</li> </ul> <p>Notice:</p> <ul> <li>AFL-1.1</li> <li>AFL-1.2</li> <li>AFL-2.0</li> <li>AFL-2.1</li> <li>AFL-3.0</li> <li>Apache-1.0</li> <li>Apache-1.1</li> <li>Apache-2.0</li> <li>Artistic-1.0-cl8</li> <li>Artistic-1.0-Perl</li> <li>Artistic-1.0</li> <li>Artistic-2.0</li> <li>BSL-1.0</li> <li>BSD-2-Clause-FreeBSD</li> <li>BSD-2-Clause-NetBSD</li> <li>BSD-2-Clause</li> <li>BSD-3-Clause-Attribution</li> <li>BSD-3-Clause-Clear</li> <li>BSD-3-Clause-LBNL</li> <li>BSD-3-Clause</li> <li>BSD-4-Clause</li> <li>BSD-4-Clause-UC</li> <li>BSD-Protection</li> <li>CC-BY-1.0</li> <li>CC-BY-2.0</li> <li>CC-BY-2.5</li> <li>CC-BY-3.0</li> <li>CC-BY-4.0</li> <li>FTL</li> <li>ISC</li> <li>ImageMagick</li> <li>Libpng</li> <li>Lil-1.0</li> <li>Linux-OpenIB</li> <li>LPL-1.02</li> <li>LPL-1.0</li> <li>MS-PL</li> <li>MIT</li> <li>NCSA</li> <li>OpenSSL</li> <li>PHP-3.01</li> <li>PHP-3.0</li> <li>PIL</li> <li>Python-2.0</li> <li>Python-2.0-complete</li> <li>PostgreSQL</li> <li>SGI-B-1.0</li> <li>SGI-B-1.1</li> <li>SGI-B-2.0</li> <li>Unicode-DFS-2015</li> <li>Unicode-DFS-2016</li> <li>Unicode-TOU</li> <li>UPL-1.0</li> <li>W3C-19980720</li> <li>W3C-20150513</li> <li>W3C</li> <li>X11</li> <li>Xnet</li> <li>Zend-2.0</li> <li>zlib-acknowledgement</li> <li>Zlib</li> <li>ZPL-1.1</li> <li>ZPL-2.0</li> <li>ZPL-2.1</li> </ul> <p>Permissive:</p> <p>Unencumbered:</p> <ul> <li>CC0-1.0</li> <li>Unlicense</li> <li>0BSD</li> </ul>"},{"location":"guides/license-guide/#22-internal-use","title":"2.2. Internal Use","text":"<p>Restricted:</p> <ul> <li>CERN 1.1</li> <li>OFL-1.1</li> <li>OpenVision</li> </ul> <p>NOTE SIL Open Font License (OFL-1.1) is not a <code>notice</code> license and has additional restrictions. Please mark packages that use the OFL-1.1 license as <code>by_exception_only</code>.</p> <p>Notice:</p> <ul> <li>Beerware</li> <li>LaTeX License</li> </ul>"},{"location":"guides/license-guide/#3-tools","title":"3. Tools","text":"<ul> <li>Trivy License Scanner <p>Trivy scans any container image for license files and offers an opinionated view on the risk associated with the license. License are classified using the <code>Google License Classification</code>. In addition to package licenses, Trivy scans source code files, Markdown documents, text files and <code>LICENSE</code> documents to identify license usage within the image or filesystem.</p> </li> </ul> <p>Scan license in container image and filesystem.</p> <p>Standard scanning, specify an image name with <code>--security-checks</code> license.</p> <pre><code>trivy image --security-checks license --severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL alpine:3.15\n</code></pre> <p>Full scanning, specify an image name with <code>--license-full</code>.</p> <pre><code>trivy image --security-checks license --severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL --license-full grafana/grafana\n</code></pre> <p>Full scanning, specify a file system (fs) <code>--license-full</code>.</p> <pre><code>trivy fs --format json --security-checks license --severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL --license-full eddyData/ -o license.log\n</code></pre>"},{"location":"guides/license-guide/#4-references","title":"4. References","text":"<ul> <li>Sentenz SPDX convention article.</li> <li>Sentenz about licenses article.</li> </ul>"},{"location":"guides/scrum-guide/","title":"Scrum Guide","text":"<p>Within project management, scrum is a framework for developing, delivering, and sustaining products in a complex environment, with an initial emphasis on software development, although it has been used in other fields including research, sales, marketing and advanced technologies. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The scrum team assess progress in time-boxed daily meetings of 15 minutes or less, called daily scrums. At the end of the sprint, the team holds two further meetings: the sprint review which demonstrates the work done to stakeholders to elicit feedback, and sprint retrospective which enables the team to reflect and improve.</p> <p>Read the Scrum Guide version 2020.</p> <ul> <li>Purpose of the Scrum Guide</li> <li>Scrum Definition</li> <li>Scrum Theory</li> <li>Transparency</li> <li>Inspection</li> <li>Adaptation</li> <li>Scrum Values</li> <li>Scrum Team</li> <li>Developers</li> <li>Product Owner</li> <li>Scrum Master</li> <li>Scrum Events</li> <li>The Sprint</li> <li>Sprint Planning<ul> <li>Topic One: Why is this Sprint valuable?</li> <li>Topic Two: What can be Done this Sprint?</li> <li>Topic Three: How will the chosen work get done?</li> </ul> </li> <li>Daily Scrum</li> <li>Sprint Review</li> <li>Sprint Retrospective</li> <li>Scrum Artifacts</li> <li>Product Backlog<ul> <li>Commitment: Product Goal</li> </ul> </li> <li>Sprint Backlog<ul> <li>Commitment: Sprint Goal</li> </ul> </li> <li>Increment<ul> <li>Commitment: Definition of Done</li> </ul> </li> <li>End Note</li> <li>Acknowledgements<ul> <li>People</li> <li>Scrum Guide History</li> </ul> </li> </ul>"},{"location":"guides/scrum-guide/#purpose-of-the-scrum-guide","title":"Purpose of the Scrum Guide","text":"<p>We developed Scrum in the early 1990s. We wrote the first version of the Scrum Guide in 2010 to help people worldwide understand Scrum. We have evolved the Guide since then through small, functional updates. Together, we stand behind it.</p> <p>The Scrum Guide contains the definition of Scrum. Each element of the framework serves a specific purpose that is essential to the overall value and results realized with Scrum. Changing the core design or ideas of Scrum, leaving out elements, or not following the rules of Scrum, covers up problems and limits the benefits of Scrum, potentially even rendering it useless.</p> <p>We follow the growing use of Scrum within an ever-growing complex world. We are humbled to see Scrum being adopted in many domains holding essentially complex work, beyond software product development where Scrum has its roots. As Scrum's use spreads, developers, researchers, analysts, scientists, and other specialists do the work. We use the word \"developers\" in Scrum not to exclude, but to simplify. If you get value from Scrum, consider yourself included.</p> <p>As Scrum is being used, patterns, processes, and insights that fit the Scrum framework as described in this document, may be found, applied and devised. Their description is beyond the purpose of the Scrum Guide because they are context sensitive and differ widely between Scrum uses. Such tactics for using within the Scrum framework vary widely and are described elsewhere.</p>"},{"location":"guides/scrum-guide/#scrum-definition","title":"Scrum Definition","text":"<p>Scrum is a lightweight framework that helps people, teams and organizations generate value through adaptive solutions for complex problems.</p> <p>In a nutshell, Scrum requires a Scrum Master to foster an environment where:</p> <ol> <li>A Product Owner orders the work for a complex problem into a Product Backlog.</li> <li>The Scrum Team turns a selection of the work into an Increment of value during a Sprint.</li> <li>The Scrum Team and its stakeholders inspect the results and adjust for the next Sprint.</li> <li>Repeat</li> </ol> <p>Scrum is simple. Try it as is and determine if its philosophy, theory, and structure help to achieve goals and create value. The Scrum framework is purposefully incomplete, only defining the parts required to implement Scrum theory. Scrum is built upon by the collective intelligence of the people using it. Rather than provide people with detailed instructions, the rules of Scrum guide their relationships and interactions.</p> <p>Various processes, techniques and methods can be employed within the framework. Scrum wraps around existing practices or renders them unnecessary. Scrum makes visible the relative efficacy of current management, environment, and work techniques, so that improvements can be made.</p>"},{"location":"guides/scrum-guide/#scrum-theory","title":"Scrum Theory","text":"<p>Scrum is founded on empiricism and lean thinking. Empiricism asserts that knowledge comes from experience and making decisions based on what is observed. Lean thinking reduces waste and focuses on the essentials.</p> <p>Scrum employs an iterative, incremental approach to optimize predictability and to control risk. Scrum engages groups of people who collectively have all the skills and expertise to do the work and share or acquire such skills as needed.</p> <p>Scrum combines four formal events for inspection and adaptation within a containing event, the Sprint. These events work because they implement the empirical Scrum pillars of transparency, inspection, and adaptation.</p>"},{"location":"guides/scrum-guide/#transparency","title":"Transparency","text":"<p>The emergent process and work must be visible to those performing the work as well as those receiving the work. With Scrum, important decisions are based on the perceived state of its three formal artifacts. Artifacts that have low transparency can lead to decisions that diminish value and increase risk.</p> <p>Transparency enables inspection. Inspection without transparency is misleading and wasteful.</p>"},{"location":"guides/scrum-guide/#inspection","title":"Inspection","text":"<p>The Scrum artifacts and the progress toward agreed goals must be inspected frequently and diligently to detect potentially undesirable variances or problems. To help with inspection, Scrum provides cadence in the form of its five events.</p> <p>Inspection enables adaptation. Inspection without adaptation is considered pointless. Scrum events are designed to provoke change.</p>"},{"location":"guides/scrum-guide/#adaptation","title":"Adaptation","text":"<p>If any aspects of a process deviate outside acceptable limits or if the resulting product is unacceptable, the process being applied or the materials being produced must be adjusted. The adjustment must be made as soon as possible to minimize further deviation.</p> <p>Adaptation becomes more difficult when the people involved are not empowered or self-managing. A Scrum Team is expected to adapt the moment it learns anything new through inspection.</p>"},{"location":"guides/scrum-guide/#scrum-values","title":"Scrum Values","text":"<p>Successful use of Scrum depends on people becoming more proficient in living five values:</p> <p>Commitment, Focus, Openness, Respect, and Courage</p> <p>The Scrum Team commits to achieving its goals and to supporting each other. Their primary focus is on the work of the Sprint to make the best possible progress toward these goals. The Scrum Team and its stakeholders are open about the work and the challenges. Scrum Team members respect each other to be capable, independent people, and are respected as such by the people with whom they work. The Scrum Team members have the courage to do the right thing, to work on tough problems.</p> <p>These values give direction to the Scrum Team with regard to their work, actions, and behavior. The decisions that are made, the steps taken, and the way Scrum is used should reinforce these values, not diminish or undermine them. The Scrum Team members learn and explore the values as they work with the Scrum events and artifacts. When these values are embodied by the Scrum Team and the people they work with, the empirical Scrum pillars of transparency, inspection, and adaptation come to life building trust.</p>"},{"location":"guides/scrum-guide/#scrum-team","title":"Scrum Team","text":"<p>The fundamental unit of Scrum is a small team of people, a Scrum Team. The Scrum Team consists of one Scrum Master, one Product Owner, and Developers. Within a Scrum Team, there are no sub-teams or hierarchies. It is a cohesive unit of professionals focused on one objective at a time, the Product Goal.</p> <p>Scrum Teams are cross-functional, meaning the members have all the skills necessary to create value each Sprint. They are also self-managing, meaning they internally decide who does what, when, and how.</p> <p>The Scrum Team is small enough to remain nimble and large enough to complete significant work within a Sprint, typically 10 or fewer people. In general, we have found that smaller teams communicate better and are more productive. If Scrum Teams become too large, they should consider reorganizing into multiple cohesive Scrum Teams, each focused on the same product. Therefore, they should share the same Product Goal, Product Backlog, and Product Owner.</p> <p>The Scrum Team is responsible for all product-related activities from stakeholder collaboration, verification, maintenance, operation, experimentation, research and development, and anything else that might be required. They are structured and empowered by the organization to manage their own work. Working in Sprints at a sustainable pace improves the Scrum Team's focus and consistency.</p> <p>The entire Scrum Team is accountable for creating a valuable, useful Increment every Sprint. Scrum defines three specific accountabilities within the Scrum Team: the Developers, the Product Owner, and the Scrum Master.</p>"},{"location":"guides/scrum-guide/#developers","title":"Developers","text":"<p>Developers are the people in the Scrum Team that are committed to creating any aspect of a usable Increment each Sprint.</p> <p>The specific skills needed by the Developers are often broad and will vary with the domain of work. However, the Developers are always accountable for:</p> <ul> <li>Creating a plan for the Sprint, the Sprint Backlog;</li> <li>Instilling quality by adhering to a Definition of Done;</li> <li>Adapting their plan each day toward the Sprint Goal; and,</li> <li>Holding each other accountable as professionals.</li> </ul>"},{"location":"guides/scrum-guide/#product-owner","title":"Product Owner","text":"<p>The Product Owner is accountable for maximizing the value of the product resulting from the work of the Scrum Team. How this is done may vary widely across organizations, Scrum Teams, and individuals.</p> <p>The Product Owner is also accountable for effective Product Backlog management, which includes:</p> <ul> <li>Developing and explicitly communicating the Product Goal;</li> <li>Creating and clearly communicating Product Backlog items;</li> <li>Ordering Product Backlog items; and,</li> <li>Ensuring that the Product Backlog is transparent, visible and understood.</li> </ul> <p>The Product Owner may do the above work or may delegate the responsibility to others. Regardless, the Product Owner remains accountable.</p> <p>For Product Owners to succeed, the entire organization must respect their decisions. These decisions are visible in the content and ordering of the Product Backlog, and through the inspectable Increment at the Sprint Review.</p> <p>The Product Owner is one person, not a committee. The Product Owner may represent the needs of many stakeholders in the Product Backlog. Those wanting to change the Product Backlog can do so by trying to convince the Product Owner.</p>"},{"location":"guides/scrum-guide/#scrum-master","title":"Scrum Master","text":"<p>The Scrum Master is accountable for establishing Scrum as defined in the Scrum Guide. They do this by helping everyone understand Scrum theory and practice, both within the Scrum Team and the organization.</p> <p>The Scrum Master is accountable for the Scrum Team's effectiveness. They do this by enabling the Scrum Team to improve its practices, within the Scrum framework.</p> <p>Scrum Masters are true leaders who serve the Scrum Team and the larger organization.</p> <p>The Scrum Master serves the Scrum Team in several ways, including:</p> <ul> <li>Coaching the team members in self-management and cross-functionality;</li> <li>Helping the Scrum Team focus on creating high-value Increments that meet the Definition of Done;</li> <li>Causing the removal of impediments to the Scrum Team's progress; and,</li> <li>Ensuring that all Scrum events take place and are positive, productive, and kept within the timebox.</li> </ul> <p>The Scrum Master serves the Product Owner in several ways, including:</p> <ul> <li>Helping find techniques for effective Product Goal definition and Product Backlog management;</li> <li>Helping the Scrum Team understand the need for clear and concise Product Backlog items;</li> <li>Helping establish empirical product planning for a complex environment; and,</li> <li>Facilitating stakeholder collaboration as requested or needed.</li> </ul> <p>The Scrum Master serves the organization in several ways, including:</p> <ul> <li>Leading, training, and coaching the organization in its Scrum adoption;</li> <li>Planning and advising Scrum implementations within the organization;</li> <li>Helping employees and stakeholders understand and enact an empirical approach for complex work; and,</li> <li>Removing barriers between stakeholders and Scrum Teams.</li> </ul>"},{"location":"guides/scrum-guide/#scrum-events","title":"Scrum Events","text":"<p>The Sprint is a container for all other events. Each event in Scrum is a formal opportunity to inspect and adapt Scrum artifacts. These events are specifically designed to enable the transparency required. Failure to operate any events as prescribed results in lost opportunities to inspect and adapt. Events are used in Scrum to create regularity and to minimize the need for meetings not defined in Scrum.</p> <p>Optimally, all events are held at the same time and place to reduce complexity.</p>"},{"location":"guides/scrum-guide/#the-sprint","title":"The Sprint","text":"<p>Sprints are the heartbeat of Scrum, where ideas are turned into value.</p> <p>They are fixed length events of one month or less to create consistency. A new Sprint starts immediately after the conclusion of the previous Sprint.</p> <p>All the work necessary to achieve the Product Goal, including Sprint Planning, Daily Scrums, Sprint Review, and Sprint Retrospective, happen within Sprints.</p> <p>During the Sprint:</p> <ul> <li>No changes are made that would endanger the Sprint Goal;</li> <li>Quality does not decrease;</li> <li>The Product Backlog is refined as needed; and,</li> <li>Scope may be clarified and renegotiated with the Product Owner as more is learned.</li> </ul> <p>Sprints enable predictability by ensuring inspection and adaptation of progress toward a Product Goal at least every calendar month. When a Sprint's horizon is too long the Sprint Goal may become invalid, complexity may rise, and risk may increase. Shorter Sprints can be employed to generate more learning cycles and limit risk of cost and effort to a smaller time frame. Each Sprint may be considered a short project.</p> <p>Various practices exist to forecast progress, like burn-downs, burn-ups, or cumulative flows. While proven useful, these do not replace the importance of empiricism. In complex environments, what will happen is unknown. Only what has already happened may be used for forward-looking decision making.</p> <p>A Sprint could be cancelled if the Sprint Goal becomes obsolete. Only the Product Owner has the authority to cancel the Sprint.</p>"},{"location":"guides/scrum-guide/#sprint-planning","title":"Sprint Planning","text":"<p>Sprint Planning initiates the Sprint by laying out the work to be performed for the Sprint. This resulting plan is created by the collaborative work of the entire Scrum Team.</p> <p>The Product Owner ensures that attendees are prepared to discuss the most important Product Backlog items and how they map to the Product Goal. The Scrum Team may also invite other people to attend Sprint Planning to provide advice.</p> <p>Sprint Planning addresses the following topics:</p>"},{"location":"guides/scrum-guide/#topic-one-why-is-this-sprint-valuable","title":"Topic One: Why is this Sprint valuable?","text":"<p>The Product Owner proposes how the product could increase its value and utility in the current Sprint. The whole Scrum Team then collaborates to define a Sprint Goal that communicates why the Sprint is valuable to stakeholders. The Sprint Goal must be finalized prior to the end of Sprint Planning.</p>"},{"location":"guides/scrum-guide/#topic-two-what-can-be-done-this-sprint","title":"Topic Two: What can be Done this Sprint?","text":"<p>Through discussion with the Product Owner, the Developers select items from the Product Backlog to include in the current Sprint. The Scrum Team may refine these items during this process, which increases understanding and confidence.</p> <p>Selecting how much can be completed within a Sprint may be challenging. However, the more the Developers know about their past performance, their upcoming capacity, and their Definition of Done, the more confident they will be in their Sprint forecasts.</p>"},{"location":"guides/scrum-guide/#topic-three-how-will-the-chosen-work-get-done","title":"Topic Three: How will the chosen work get done?","text":"<p>For each selected Product Backlog item, the Developers plan the work necessary to create an Increment that meets the Definition of Done. This is often done by decomposing Product Backlog items into smaller work items of one day or less. How this is done is at the sole discretion of the Developers. No one else tells them how to turn Product Backlog items into Increments of value.</p> <p>The Sprint Goal, the Product Backlog items selected for the Sprint, plus the plan for delivering them are together referred to as the Sprint Backlog.</p> <p>Sprint Planning is timeboxed to a maximum of eight hours for a one-month Sprint. For shorter Sprints, the event is usually shorter.</p>"},{"location":"guides/scrum-guide/#daily-scrum","title":"Daily Scrum","text":"<p>The purpose of the Daily Scrum is to inspect progress toward the Sprint Goal and adapt the Sprint Backlog as necessary, adjusting the upcoming planned work.</p> <p>The Daily Scrum is a 15-minute event for the Developers of the Scrum Team. To reduce complexity, it is held at the same time and place every working day of the Sprint. If the Product Owner or Scrum Master are actively working on items in the Sprint Backlog, they participate as Developers.</p> <p>The Developers can select whatever structure and techniques they want, as long as their Daily Scrum focuses on progress toward the Sprint Goal and produces an actionable plan for the next day of work. This creates focus and improves self-management.</p> <p>Daily Scrums improve communications, identify impediments, promote quick decision-making, and consequently eliminate the need for other meetings.</p> <p>The Daily Scrum is not the only time Developers are allowed to adjust their plan. They often meet throughout the day for more detailed discussions about adapting or re-planning the rest of the Sprint's work.</p>"},{"location":"guides/scrum-guide/#sprint-review","title":"Sprint Review","text":"<p>The purpose of the Sprint Review is to inspect the outcome of the Sprint and determine future adaptations. The Scrum Team presents the results of their work to key stakeholders and progress toward the Product Goal is discussed.</p> <p>During the event, the Scrum Team and stakeholders review what was accomplished in the Sprint and what has changed in their environment. Based on this information, attendees collaborate on what to do next. The Product Backlog may also be adjusted to meet new opportunities. The Sprint Review is a working session and the Scrum Team should avoid limiting it to a presentation.</p> <p>The Sprint Review is the second to last event of the Sprint and is timeboxed to a maximum of four hours for a one-month Sprint. For shorter Sprints, the event is usually shorter.</p>"},{"location":"guides/scrum-guide/#sprint-retrospective","title":"Sprint Retrospective","text":"<p>The purpose of the Sprint Retrospective is to plan ways to increase quality and effectiveness.</p> <p>The Scrum Team inspects how the last Sprint went with regards to individuals, interactions, processes, tools, and their Definition of Done. Inspected elements often vary with the domain of work. Assumptions that led them astray are identified and their origins explored. The Scrum Team discusses what went well during the Sprint, what problems it encountered, and how those problems were (or were not) solved.</p> <p>The Scrum Team identifies the most helpful changes to improve its effectiveness. The most impactful improvements are addressed as soon as possible. They may even be added to the Sprint Backlog for the next Sprint.</p> <p>The Sprint Retrospective concludes the Sprint. It is timeboxed to a maximum of three hours for a one-month Sprint. For shorter Sprints, the event is usually shorter.</p>"},{"location":"guides/scrum-guide/#scrum-artifacts","title":"Scrum Artifacts","text":"<p>Scrum's artifacts represent work or value. They are designed to maximize transparency of key information. Thus, everyone inspecting them has the same basis for adaptation.</p> <p>Each artifact contains a commitment to ensure it provides information that enhances transparency and focus against which progress can be measured:</p> <ul> <li>For the Product Backlog it is the Product Goal.</li> <li>For the Sprint Backlog it is the Sprint Goal.</li> <li>For the Increment it is the Definition of Done.</li> </ul> <p>These commitments exist to reinforce empiricism and the Scrum values for the Scrum Team and their stakeholders.</p>"},{"location":"guides/scrum-guide/#product-backlog","title":"Product Backlog","text":"<p>The Product Backlog is an emergent, ordered list of what is needed to improve the product. It is the single source of work undertaken by the Scrum Team.</p> <p>Product Backlog items that can be Done by the Scrum Team within one Sprint are deemed ready for selection in a Sprint Planning event. They usually acquire this degree of transparency after refining activities. Product Backlog refinement is the act of breaking down and further defining Product Backlog items into smaller more precise items. This is an ongoing activity to add details, such as a description, order, and size. Attributes often vary with the domain of work.</p> <p>The Developers who will be doing the work are responsible for the sizing. The Product Owner may influence the Developers by helping them understand and select trade-offs.</p>"},{"location":"guides/scrum-guide/#commitment-product-goal","title":"Commitment: Product Goal","text":"<p>The Product Goal describes a future state of the product which can serve as a target for the Scrum Team to plan against. The Product Goal is in the Product Backlog. The rest of the Product Backlog emerges to define \"what\" will fulfill the Product Goal.</p> <p>A product is a vehicle to deliver value. It has a clear boundary, known stakeholders, well-defined users or customers. A product could be a service, a physical product, or something more abstract.</p> <p>The Product Goal is the long-term objective for the Scrum Team. They must fulfill (or abandon) one objective before taking on the next.</p>"},{"location":"guides/scrum-guide/#sprint-backlog","title":"Sprint Backlog","text":"<p>The Sprint Backlog is composed of the Sprint Goal (why), the set of Product Backlog items selected for the Sprint (what), as well as an actionable plan for delivering the Increment (how).</p> <p>The Sprint Backlog is a plan by and for the Developers. It is a highly visible, real-time picture of the work that the Developers plan to accomplish during the Sprint in order to achieve the Sprint Goal. Consequently, the Sprint Backlog is updated throughout the Sprint as more is learned. It should have enough detail that they can inspect their progress in the Daily Scrum.</p>"},{"location":"guides/scrum-guide/#commitment-sprint-goal","title":"Commitment: Sprint Goal","text":"<p>The Sprint Goal is the single objective for the Sprint. Although the Sprint Goal is a commitment by the Developers, it provides flexibility in terms of the exact work needed to achieve it. The Sprint Goal also creates coherence and focus, encouraging the Scrum Team to work together rather than on separate initiatives.</p> <p>The Sprint Goal is created during the Sprint Planning event and then added to the Sprint Backlog. As the Developers work during the Sprint, they keep the Sprint Goal in mind. If the work turns out to be different than they expected, they collaborate with the Product Owner to negotiate the scope of the Sprint Backlog within the Sprint without affecting the Sprint Goal.</p>"},{"location":"guides/scrum-guide/#increment","title":"Increment","text":"<p>An Increment is a concrete stepping stone toward the Product Goal. Each Increment is additive to all prior Increments and thoroughly verified, ensuring that all Increments work together. In order to provide value, the Increment must be usable.</p> <p>Multiple Increments may be created within a Sprint. The sum of the Increments is presented at the Sprint Review thus supporting empiricism. However, an Increment may be delivered to stakeholders prior to the end of the Sprint. The Sprint Review should never be considered a gate to releasing value.</p> <p>Work cannot be considered part of an Increment unless it meets the Definition of Done.</p>"},{"location":"guides/scrum-guide/#commitment-definition-of-done","title":"Commitment: Definition of Done","text":"<p>The Definition of Done is a formal description of the state of the Increment when it meets the quality measures required for the product.</p> <p>The moment a Product Backlog item meets the Definition of Done, an Increment is born.</p> <p>The Definition of Done creates transparency by providing everyone a shared understanding of what work was completed as part of the Increment. If a Product Backlog item does not meet the Definition of Done, it cannot be released or even presented at the Sprint Review. Instead, it returns to the Product Backlog for future consideration.</p> <p>If the Definition of Done for an increment is part of the standards of the organization, all Scrum Teams must follow it as a minimum. If it is not an organizational standard, the Scrum Team must create a Definition of Done appropriate for the product.</p> <p>The Developers are required to conform to the Definition of Done. If there are multiple Scrum Teams working together on a product, they must mutually define and comply with the same Definition of Done.</p>"},{"location":"guides/scrum-guide/#end-note","title":"End Note","text":"<p>Scrum is free and offered in this Guide. The Scrum framework, as outlined herein, is immutable. While implementing only parts of Scrum is possible, the result is not Scrum. Scrum exists only in its entirety and functions well as a container for other techniques, methodologies, and practices.</p>"},{"location":"guides/scrum-guide/#acknowledgements","title":"Acknowledgements","text":""},{"location":"guides/scrum-guide/#people","title":"People","text":"<p>Of the thousands of people who have contributed to Scrum, we should single out those who were instrumental at the start: Jeff Sutherland worked with Jeff McKenna and John Scumniotales, and Ken Schwaber worked with Mike Smith and Chris Martin, and all of them worked together. Many others contributed in the ensuing years and without their help Scrum would not be refined as it is today.</p>"},{"location":"guides/scrum-guide/#scrum-guide-history","title":"Scrum Guide History","text":"<p>Ken Schwaber and Jeff Sutherland first co-presented Scrum at the OOPSLA Conference in 1995. It essentially documented the learning that Ken and Jeff gained over the previous few years and made public the first formal definition of Scrum.</p> <p>The Scrum Guide documents Scrum as developed, evolved, and sustained for 30-plus years by Jeff Sutherland and Ken Schwaber. Other sources provide patterns, processes, and insights that complement the Scrum framework. These may increase productivity, value, creativity, and satisfaction with the results.</p> <p>The complete history of Scrum is described elsewhere. To honor the first places where it was tried and proven, we recognize Individual Inc., Newspage, Fidelity Investments, and IDX (now GE Medical).</p>"},{"location":"guides/software-development-guide/","title":"Software Development Guide","text":"<p>Software development refers to the design, documentation, programming, testing, and maintenance of a software deliverable. The combination of these steps are used to create a workflow pipeline, a sequence of stages that when followed produce high-quality software deliverables. This pipeline is known as the Software Development Life Cycle (SDLC).</p> <ul> <li>1. Category</li> <li>1.1. Software Development Life Cycle<ul> <li>1.1.1. Discover</li> <li>1.1.2. Plan</li> <li>1.1.3. Code</li> <li>1.1.4. Build, Test and Release</li> <li>1.1.5. Deploy</li> <li>1.1.6. Operate</li> <li>1.1.7. Observe</li> <li>1.1.8. Security</li> </ul> </li> <li>1.2. Key Features<ul> <li>1.2.1. Agile</li> <li>1.2.2. XOps</li> <li>1.2.3. Continuous Pipelines</li> <li>1.2.4. Software Architecture</li> </ul> </li> <li>2. References</li> </ul>"},{"location":"guides/software-development-guide/#1-category","title":"1. Category","text":""},{"location":"guides/software-development-guide/#11-software-development-life-cycle","title":"1.1. Software Development Life Cycle","text":"<p>The Software Development Life Cycle (SDLC) is a systematic approach to software development that consists of various phases and activities. Each phase has specific goals and deliverables, and it provides a structured framework for managing and controlling the software development process.</p>"},{"location":"guides/software-development-guide/#111-discover","title":"1.1.1. Discover","text":"<p>Projects are envisioned, designed, and prioritized.</p> <p>Features of Discover:</p> <ol> <li> <p>Process Principles</p> <ul> <li> <p>Agile</p> </li> <li> <p>Scrum     &gt; Scrum is an Agile framework for managing and delivering complex projects. It provides a flexible and iterative approach to software development that focuses on delivering value to customers through regular product increments. Scrum promotes collaboration, transparency, and adaptability, allowing teams to respond quickly to changing requirements and market dynamics.</p> </li> <li> <p>Extreme Programming     &gt; Extreme Programming (XP) is an agile methodology that focuses on producing high-quality software through iterative and incremental development. It emphasizes collaboration, customer involvement, and continuous feedback.</p> </li> </ul> </li> <li> <p>Software Analysis</p> <ul> <li>Requirements Analysis <p>Requirements analysis sets the foundation for the development process, guiding subsequent phases such as design, implementation, and testing. Conducting requirements analysis reduces the risk of scope creep, improves software quality, and delivers a product that meets stakeholder expectations.</p> </li> </ul> </li> <li> <p>Architecture Decision Records</p> <p>Architecture Decision Records (ADR) captures the key options available, the main requirements that drive a decision, and the design decisions themselves.</p> </li> </ol>"},{"location":"guides/software-development-guide/#112-plan","title":"1.1.2. Plan","text":"<p>Stakeholders are identified, budgets set, and infrastructure requisitioned. Design documents from the concept phase are broken down into actionable tasks.</p> <p>Features of Plan:</p> <ol> <li> <p>Software Architecture Description</p> <p>A software architecture description provides an overview and detailed documentation of the architecture of a software system. It describes the structure, components, relationships, and behavior of the software system, enabling developers, architects, and stakeholders to understand and communicate the design decisions and principles underlying the system.</p> </li> <li> <p>Software Development Environment</p> <ul> <li>Development Environment <p>This environment is used for developing, testing and debugging software applications. It provides the necessary resources for developers to write code and create new features.</p> </li> </ul> </li> <li> <p>Dependency Manager</p> <p>Dependency managers automate the process of acquiring, installing and managing the dependencies of a software project. They ensure that the necessary dependencies are available and compatible with each other, eliminating the need to manually download, configure and track external libraries, frameworks, modules or packages.</p> </li> <li> <p>Build System</p> <p>A build system is a software tool or framework that automates the process of compiling source code into executable software or other target artifacts. It helps manage the dependencies, configurations, and tasks required to build a software project.</p> </li> <li> <p>Project Layout</p> <p>Project layout refers to the arrangement of files, folders, and other resources within a project. A well-designed project layout can improve the organization, readability, and maintainability of the project's codebase.</p> </li> <li> <p>Software Architectural Patterns</p> <ul> <li>Component-Driven Development (CDD) <p>Component-Driven Development (CDD) is an approach to software development that emphasizes the construction of applications by composing modular, reusable components. It focuses on building and integrating self-contained, loosely coupled components as the primary building blocks of an application.</p> </li> </ul> </li> <li> <p>Branching Strategies</p> <p>Branching strategies are used in software development team to manage concurrent work and organize code changes within a version control system. They define how developers collaborate on different features, bug fixes, and releases while maintaining stability and minimizing conflicts.</p> </li> <li> <p>Merging Strategies</p> <p>Merging strategies are used in a version control system to combine changes into a single cohesive version.</p> </li> </ol>"},{"location":"guides/software-development-guide/#113-code","title":"1.1.3. Code","text":"<p>Development teams work to build production-ready software that meets requirements and feedback.</p> <p>Features of Code:</p> <ol> <li> <p>Coding</p> <ul> <li> <p>Comment</p> <p>Comment styles and tags are used to providing code context and documentation, and annotate code with additional information for improving code quality and maintainability.</p> </li> <li> <p>App Resources</p> <p>App resources refer to the various assets and components utilized by an application, such as images, icons, sounds, videos, and other media files, as well as configuration files, database connections, external API endpoints, and managing static strings and magic numbers.</p> </li> <li> <p>Feature Toggles</p> <p>Feature toggles are a software development technique that enables developers to turn on and off certain features or functionality of an application or service.</p> </li> <li> <p>Software Design Principals</p> <p>Software design patterns are general reusable solutions to common software design problems that have been proven effective over time. Design patterns provide a way to organize and structure software code in a way that is easily maintainable, scalable, and extensible.</p> </li> <li> <p>Software Design Patterns</p> <p>Software design patterns are general reusable solutions to common software design problems that have been proven effective over time. Design patterns provide a way to organize and structure software code in a way that is easily maintainable, scalable, and extensible.</p> </li> <li> <p>Software Architectural Patterns</p> <p>Software architectural patterns are high-level design templates or solutions that provide a structured approach for organizing and designing software systems. Architectural patterns offer proven solutions to recurring design problems and help architects and developers build scalable, maintainable, and robust software applications.</p> </li> </ul> </li> <li> <p>Software Testing</p> <ul> <li> <p>Test-Driven Development</p> <p>Test-Driven Development (TDD) is a software development approach that emphasizes writing automated tests before writing the actual code. TDD is based on the idea that writing tests first can lead to better code quality, better design, and improved overall software development processes.</p> </li> <li> <p>Testing Patterns</p> <p>Testing patterns are reusable techniques to common testing problems that can be used to improve the effectiveness of software testing. They allow to organize and structure test code in a way that is maintainable, scalable, and extensible.</p> </li> <li> <p>Unit Testing</p> <p>Unit testing focuses on testing individual components of the software, such as classes or functions, to ensure that they work as expected. This type of testing is performed during the development phase and is an important part of the software development process.</p> </li> <li> <p>Fuzz Testing</p> <p>Fuzz testing is a type of testing that involves sending random or malformed inputs to the software in an attempt to find defects. It helps identify any security or reliability issues that may occur in the software, and helps ensure that the software is robust and resilient.</p> </li> <li> <p>Test Double</p> <p>Test Double is a concept from software testing and is used in the context of writing unit tests. It refers to a type of test-specific object that is used to replace a real component or collaborator within a test scenario. The purpose of using Test Doubles is to isolate the unit being tested from its dependencies or collaborators, ensuring that the test evaluates only the behavior of the unit itself.</p> </li> </ul> </li> <li> <p>Everything as Code (XaC)</p> <p>Everything as Code (XaC) is a software development philosophy that treats infrastructure as code.</p> <ul> <li>Infrastructure-as-Code (IaC)</li> </ul> <p>Infrastructure-as-Code (IaC) involves managing and provisioning infrastructure resources (e.g. virtual machines, networks, storage) through code, rather than using manual processes to configure devices or systems.</p> <ul> <li> <p>Configuration-as-Code (CaC)</p> <p>In Configuration-as-Code (CaC) application and system configurations are represented as code, treating application config resources as versioned artifacts to manage and deploy consistent configurations across different environments.</p> </li> <li> <p>Documentation-as-Code (DaC)</p> <p>Documentation-as-Code (DaC) involves writing documentation as code, allowing teams to manage documentation in version-controlled repositories and automate documentation generation.</p> </li> </ul> </li> <li> <p>Software Analysis</p> <ul> <li> <p>Static Analysis</p> <p>Static analysis is a software analysis technique that examines software artifacts, such as source code, design documents, or models, without executing the program. It analyzes the structure, syntax, and semantics of the code to identify potential issues, vulnerabilities, and quality concerns. Static analysis helps detect defects, improve code quality, and ensure adherence to coding standards.</p> </li> <li> <p>Dynamic Analysis</p> <p>Dynamic analysis is a software analysis technique that involves observing and analyzing the behavior of a software system during its execution. It helps uncover defects, validate functionality, memory leaks, assess performance, and understand system behavior under different conditions.</p> </li> </ul> </li> <li> <p>Conventional Commits</p> <p>Conventional Commits is a commit message convention that provides a standardized and structured format for commit messages in software development projects. It aims to make commit messages more readable, informative, and useful for both humans and automated tools.</p> </li> <li> <p>Technical Dept</p> <p>Technical debt refers to the accumulated consequences of shortcuts, suboptimal solutions, and compromises made during the development process of software. Technical debt can lead to increased complexity, reduced maintainability, decreased productivity, and higher costs in the future.</p> </li> </ol>"},{"location":"guides/software-development-guide/#114-build-test-and-release","title":"1.1.4. Build, Test and Release","text":"<p>A reliable system is resilient to failures and meets its documented service level objectives, which may also include security guarantees. CI/CD pipelines ensure an efficient developer experience.</p> <p>Features of Build:</p> <ol> <li> <p>Continuous Pipelines</p> <ul> <li> <p>Continuous Integration</p> <p>Continuous Integration (CI) refers to the practice of automating the process of integrating code changes from multiple developers into a single version control repository. This process typically involves building and testing the code changes as soon as they are committed to the repository, and providing feedback to developers on the quality and stability of their changes. The goal of CI is to catch issues early in the development process and promote collaboration among team members.</p> </li> <li> <p>Continuous Build</p> <p>Continuous Build refers to the practice of automatically building and compiling software applications and systems, whenever changes are committed to the source code repository. This can involve automating the process of building, compiling, and packaging software, and can help organizations to ensure that software is always up-to-date and ready for deployment. Continuous Build helps to speed up the software development process, reduce errors and inconsistencies, and improve the overall efficiency of software development and deployment.</p> </li> </ul> </li> </ol> <p>Features of Test:</p> <ol> <li> <p>Continuous Pipelines</p> <ul> <li>Continuous Testing <p>Continuous Testing refers to the practice of automating the process of testing code changes throughout the software development lifecycle. This process typically involves the use of test automation tools and scripts that run automatically as code changes are committed, built, and deployed. Continuous Testing helps to ensure that code changes are thoroughly tested and validated before they are released to production, reducing the risk of defects and improving the overall quality of the software.</p> </li> </ul> </li> </ol> <p>Features of Release:</p> <ol> <li> <p>Code Review</p> <p>Code review is used to ensure the overall health of the codebase over time. Code Review is a measure to ensure software quality through the exchange of knowledge, experience and opinions. Collaborative code review and pull requests that plug into the code review process.</p> </li> <li> <p>Semantic Versioning</p> <p>Semantic Versioning (SemVer) is a versioning scheme for software that aims to convey meaning about the underlying code changes and their impact on compatibility.</p> </li> <li> <p>Changlog</p> <p>A changelog is a document or file that tracks and records the changes, updates, and additions made to a software project over time. It serves as a historical record of the project's evolution and provides a summary of the modifications introduced in each version or release.</p> </li> <li> <p>Continuous Pipelines</p> <ul> <li>Continuous Release <p>Continuous Release refers to the practice of automating the process of releasing software applications and systems. This can involve automating the process of packaging, deploying, and configuring software, and can help organizations to ensure that software is always up-to-date and ready for deployment. Continuous Release helps to speed up the software development process, reduce errors and inconsistencies, and improve the overall efficiency of software development and deployment.</p> </li> </ul> </li> </ol>"},{"location":"guides/software-development-guide/#115-deploy","title":"1.1.5. Deploy","text":"<p>After the code is approved and merged, it's time to deliver it.</p> <p>Features of Deploy:</p> <ol> <li> <p>Deployment Strategies</p> <p>Deployment strategies define  how a running instance of an application should be deployed, modified, or updated.</p> </li> <li> <p>Software Development Environment</p> <ul> <li>Production Environment <p>The production environment is the live environment where software applications are deployed and made available to the end-users. This environment is monitored closely to ensure the software is functioning correctly and meeting the performance and availability requirements of the business.</p> </li> </ul> </li> <li> <p>Software Analysis</p> <ul> <li> <p>Software Composition Analysis</p> </li> <li> <p>Bill of Materials (BOM)     &gt; A Bill of Materials (BOM) is a comprehensive list or inventory of all the materials, components, parts, and sub-assemblies required to manufacture or build a product. It provides detailed information about each item, including its name, quantity, description, and sometimes additional attributes such as part numbers or suppliers. BOMs are commonly used in various industries, including manufacturing, engineering, construction, and software development.</p> </li> </ul> </li> <li> <p>Patch Management</p> <p>Patch management refers to the process of planning, testing, deploying, and monitoring updates (or patches) to software applications, operating systems, and other IT systems. Effective patch management helps keep systems secure by addressing vulnerabilities and fixing bugs. It's crucial to regularly update software to protect against cyberattacks and ensure optimal performance.</p> </li> <li> <p>Continuous Pipelines</p> <ul> <li>Continuous Deployment <p>Continuous Release refers to the practice of automating the process of releasing software applications and systems. This can involve automating the process of packaging, deploying, and configuring software, and can help organizations to ensure that software is always up-to-date and ready for deployment. Continuous Release helps to speed up the software development process, reduce errors and inconsistencies, and improve the overall efficiency of software development and deployment.</p> </li> </ul> </li> </ol>"},{"location":"guides/software-development-guide/#116-operate","title":"1.1.6. Operate","text":"<p>Support and maintenance are required of active software projects to reduce down times.</p> <p>Features of Operate:</p> <ol> <li> <p>Issue Management</p> <p>Issue management refers to the process of identifying, addressing, and resolving problems or concerns that arise within an organization or project. It involves a systematic approach to effectively handle issues and minimize their impact on operations and outcomes. Issue management is crucial in maintaining the functioning of businesses, projects, and teams.</p> </li> </ol>"},{"location":"guides/software-development-guide/#117-observe","title":"1.1.7. Observe","text":"<p>Incident management is when Development and Operations respond to unplanned events and restore services using reliable methods for prioritizing incidents and getting to resolution fast.</p> <p>Features of Observe:</p> <ol> <li> <p>Incident Management</p> <p>Incident management is the process of identifying, analyzing, and resolving incidents that occur in a software system. An incident is any event that disrupts or degrades the normal operation of the system, such as a system outage, a performance degradation, or a security breach.</p> </li> <li> <p>Logging and Monitoring</p> <p>Logging and monitoring are essential components of modern software systems and infrastructure. They play a crucial role in ensuring the reliability, performance, and security of applications and services.</p> </li> <li> <p>Software Metric</p> <p>Software metric are measurements used to assess various aspects of the software development process, project, or product. They help teams track progress, identify bottlenecks, make informed decisions, and improve overall efficiency and quality.</p> <ul> <li>DORA <p>The DORA (DevOps Research and Assessment) metrics are a set of key performance indicators (KPIs) developed by the DevOps Research and Assessment organization. These metrics are designed to assess the performance of software delivery teams and provide insights into the effectiveness of DevOps practices.</p> </li> </ul> </li> </ol>"},{"location":"guides/software-development-guide/#118-security","title":"1.1.8. Security","text":"<p>Security should be included throughout the Software Development Life Cycle (SDLC) in order to minimize vulnerabilities in software code.</p> <ol> <li> <p>Cryptography</p> <p>Cryptography is the practice of securing communication from third-party interference, where third-party is anyone who is not authorized to access the communication. Cryptography uses mathematical algorithms to convert the original message, known as plaintext, into an unreadable format called ciphertext.</p> <ul> <li>Password Hashing Algorithms <p>Password hashing algorithms are a specific type of hash function that are designed to store and verify passwords securely. Passwords are often the first line of defense in protecting user accounts and sensitive information, so it is important to store them securely.</p> </li> </ul> </li> <li> <p>Risk Management</p> <p>Risk management involves identifying, assessing, and mitigating potential risks that could impact the success of a software project. It includes steps like risk identification, analysis, prioritization, and implementing strategies to minimize or address those risks. Effective risk management helps ensure the project stays on track and delivers the desired outcomes.</p> <ul> <li> <p>ISO 31000</p> <p>Provides principles and guidelines for effective risk management practices that can be applied to any type of organization and industry.</p> </li> <li> <p>IEC 62443</p> <p>IEC 62443 is a series of international standards developed by the International Electrotechnical Commission (IEC) that provides a framework for implementing cybersecurity in industrial automation and control systems (IACS). The series comprises of several parts, each of which addresses a specific aspect of IACS cybersecurity.</p> </li> </ul> </li> <li> <p>Identity and Access Management (IAM)</p> <p>Identity and Access Management (IAM) is primarily concerned with the authentication and authorization of the user in the network, i.e. with establishing the identity of the user and the associated access rights that the user has in the network, applications and systems.</p> </li> <li> <p>Software Analysis</p> <ul> <li> <p>Security Analysis</p> <p>Security analysis is a software analysis technique focused on assessing the security posture of a software system. It involves identifying vulnerabilities, weaknesses, and potential risks in the software design, implementation, configuration, and deployment. The objective of security analysis is to uncover security flaws and recommend mitigations to protect the system against unauthorized access, data breaches, and malicious attacks.</p> </li> <li> <p>Software Composition Analysis</p> <p>Software Composition Analysis (SCA) is a software analysis technique that focuses on identifying, analyzing and managing the composition of third-party and open-source software components used in a software system.</p> </li> </ul> </li> <li> <p>Continuous Pipelines</p> <ul> <li>Continuous Security <p>Continuous Security refers to the practice of integrating security into the software development lifecycle, and continuously monitoring and verifying the security of applications and systems. This can involve automated security testing, security scans, and regular security audits, and helps organizations to identify and remediate security vulnerabilities before they can be exploited.</p> </li> </ul> </li> </ol>"},{"location":"guides/software-development-guide/#12-key-features","title":"1.2. Key Features","text":"<p>Key features of effective software development.</p>"},{"location":"guides/software-development-guide/#121-agile","title":"1.2.1. Agile","text":"<p>Agile is an iterative approach to project management and software development that helps teams deliver value to their customers faster and with fewer headaches. Agile methodologies empower teams to be inherently flexible, well-organized, and capable of responding to change.</p>"},{"location":"guides/software-development-guide/#122-xops","title":"1.2.2. XOps","text":"<p>XOps is a set of principles and practices, tools, and a cultural philosophy that automate and integrate the processes of software development. XOps, an umbrella term used for a combination of IT tech like DevOps, DevSecOps, AIOps, MLOps, GitOps, and BizDevOps. XOps supposedly helps to shorten a system development cycles and provides continuous delivery with high software quality. It focuses team empowerment, cross-team communication and collaboration, and technology automation.</p>"},{"location":"guides/software-development-guide/#123-continuous-pipelines","title":"1.2.3. Continuous Pipelines","text":"<p>Continuous Pipelines describes the key stages in an automated software development and deployment flow. This flow typically includes design, coding, validation, testing, integration, delivery and phased deployment activities orchestrated as a release workflow before operation in a target/production environment.</p>"},{"location":"guides/software-development-guide/#124-software-architecture","title":"1.2.4. Software Architecture","text":"<p>Software Architectural Patterns are high-level design templates or solutions that provide a structured approach for organizing and designing software systems. Architectural patterns offer proven solutions to recurring design problems and help architects and developers build scalable, maintainable, and robust software applications.</p>"},{"location":"guides/software-development-guide/#2-references","title":"2. References","text":"<ul> <li>Atlassian software development article.</li> </ul>"},{"location":"guides/style-guide/","title":"Style Guide","text":"<ul> <li>1. C/C++</li> <li>1.1. Convention</li> <li>1.2. Library</li> <li>1.3. Code Analysis</li> <li>1.4. Build System</li> <li>1.5. Package Manager</li> <li>1.6. Software Testing</li> <li>1.7. Compiler</li> <li>1.8. Architecture</li> <li>1.9. Editor</li> <li>2. Go</li> <li>2.1. Convention</li> <li>2.2. Library<ul> <li>2.2.1. UI</li> </ul> </li> <li>2.3. Code Analysis</li> <li>2.4. Software Testing</li> <li>2.5. Architecture</li> <li>2.6. Editor</li> <li>2.7. Links</li> <li>3. Python</li> <li>3.1. Convention</li> <li>3.2. Library</li> <li>3.3. Code Analysis</li> <li>3.4. Software Testing</li> <li>3.5. Tool</li> <li>3.6. Architecture</li> <li>3.7. Editor</li> <li>4. Kotlin</li> <li>4.1. Convention</li> <li>4.2. Code Analysis</li> <li>4.3. Guides</li> <li>4.4. Editor</li> <li>5. Rust</li> <li>5.1. Convention</li> <li>5.2. Code Analysis</li> <li>5.3. Software Testing</li> <li>6. Shell</li> <li>6.1. Convention</li> <li>6.2. Code Analysis</li> <li>6.3. Editor</li> <li>7. Git</li> <li>7.1. Convention</li> <li>7.2. Code Analysis</li> <li>8. Markdown</li> <li>8.1. Convention</li> <li>8.2. Code Analysis</li> <li>8.3. Editor</li> <li>9. CMake</li> <li>9.1. Convention</li> <li>9.2. Code Analysis</li> <li>9.3. Editor</li> </ul>"},{"location":"guides/style-guide/#1-cc","title":"1. C/C++","text":"Documentation Build System Package Manager Software Testing Code Coverage Code Formatting Code Linting Cross-Compilation Dependency Dependency Dependency Dependency Dependency Dependency Dependency Dependency"},{"location":"guides/style-guide/#11-convention","title":"1.1. Convention","text":"<ul> <li>Google C++ Style Guide</li> <li>C++ Core Guidelines</li> </ul>"},{"location":"guides/style-guide/#12-library","title":"1.2. Library","text":"<ul> <li>Doxygen documentation package for C/C++ projects.</li> </ul>"},{"location":"guides/style-guide/#13-code-analysis","title":"1.3. Code Analysis","text":"<p>Code Check</p> <ul> <li> <p>clang-tidy tool and doc</p> <p>Configure .clang-tidy in the project.</p> </li> <li> <p>Cpplint</p> <ul> <li>Detect style errors, see cpplint.py tool.</li> <li>Configure CPPLINT.cfg in the root project.</li> <li>False positives can be ignored by putting <code>// NOLINT</code> at the end of the line or <code>// NOLINTNEXTLINE</code> in the previous line.</li> </ul> </li> <li> <p>scan-build</p> <p>Tool for clang-analyzer.</p> </li> <li> <p>Cppcheck</p> <ul> <li>Configure cppcheck-suppressions.txt in the project.</li> <li>See manual.</li> </ul> </li> <li> <p>Infer</p> <p>A static analyzer for Java, C, C++, and Objective-C.</p> </li> </ul> <p>Code Format</p> <ul> <li>clang-format <p>Configure in the root project.</p> <ul> <li>.clang-format</li> <li>.clang-format-ignore</li> <li>Use clang-format-configurator to modify a .clang-format file.</li> </ul> </li> </ul> <p>Sanitizers</p> <ul> <li>Valgrind</li> <li>Google Sanitizers</li> </ul>"},{"location":"guides/style-guide/#14-build-system","title":"1.4. Build System","text":"<ul> <li>CMake</li> </ul>"},{"location":"guides/style-guide/#15-package-manager","title":"1.5. Package Manager","text":"<ul> <li>Conan</li> </ul>"},{"location":"guides/style-guide/#16-software-testing","title":"1.6. Software Testing","text":"<ul> <li> <p>GoogleTest</p> <p>Unit-testing framework with testing with mocking and fixtures support.</p> </li> <li> <p>Google Benchmark</p> <p>A microbenchmark support library.</p> <p>Documentation guide for benchmark.</p> </li> <li> <p>gcov</p> <p>Test code coverage tool.</p> </li> <li> <p>OSS-Fuzz</p> <p>Ffuzzing technique for automated injections.</p> </li> <li> <p>FuzzBench</p> <p>Fuzzer benchmarking as a service.</p> </li> </ul>"},{"location":"guides/style-guide/#17-compiler","title":"1.7. Compiler","text":"<p>See Compiler User Guides for the complete option list.</p> <ol> <li>Using Common Compiler Options</li> <li>Selecting source language options</li> <li>Selecting optimization options and Options That Control Optimization</li> <li>Writing Optimized Code</li> <li>Effect of the volatile keyword</li> <li>Optimizing loops</li> <li>Inlining functions</li> <li>Stack use in C and C++</li> <li>Embedded Software Development</li> <li>Default memory map</li> <li>Run-time memory models</li> <li>The vector table</li> <li>Code size optimization</li> <li>-ffunction-sections, -fdata-sections, &amp; --gc-sections</li> </ol>"},{"location":"guides/style-guide/#18-architecture","title":"1.8. Architecture","text":"<ul> <li>Project Layout</li> </ul>"},{"location":"guides/style-guide/#19-editor","title":"1.9. Editor","text":"<ul> <li>Visual Studio Code</li> </ul>"},{"location":"guides/style-guide/#2-go","title":"2. Go","text":"Documentation Build System Package Manager Software Testing Code Coverage Code Formatting Code Linting Cross-Compilation Built-in Built-in Built-in Built-in Built-in Built-in Built-in Built-in"},{"location":"guides/style-guide/#21-convention","title":"2.1. Convention","text":"<ul> <li>Google Go Style Guide</li> <li>Effective Go</li> <li>Golang Code Review Comments</li> <li>Uber Go Style Guide</li> </ul>"},{"location":"guides/style-guide/#22-library","title":"2.2. Library","text":"<ul> <li> <p>Go kit</p> <p>Standard library for microservices or elegant monoliths.</p> </li> <li> <p>GORM</p> <p>Object-Relational Mapping (ORM) library for query and manipulate databases.</p> </li> <li> <p>Cobra</p> <p>Library for CLI apps.</p> </li> <li> <p>cli</p> <p>Library for CLI apps.</p> </li> <li> <p>fuzzy</p> <p>Library that provides fuzzy string matching.</p> </li> <li> <p>lo</p> <p>Lodash for Go library.</p> </li> <li> <p>Gonum</p> <p>Set of packages designed to writing numerical and scientific algorithms.</p> </li> <li> <p>grpc-go</p> <p>Go language implementation of gRPC.</p> </li> </ul>"},{"location":"guides/style-guide/#221-ui","title":"2.2.1. UI","text":"<ul> <li>go-echarts</li> <li>go-chart</li> <li>Statsview</li> <li>Gonum Plot</li> <li>Go GUI Projects</li> </ul>"},{"location":"guides/style-guide/#23-code-analysis","title":"2.3. Code Analysis","text":"<ul> <li>golangci-lint <p>Configure .golangci.yml in the project.</p> </li> <li>Awesome Go Linters</li> </ul>"},{"location":"guides/style-guide/#24-software-testing","title":"2.4. Software Testing","text":"<ul> <li> <p>go test</p> <p>Package testing provides support for automated testing of Go packages.</p> <ul> <li>Support for Benchmarks.</li> <li>Support for Examples.</li> <li>Support for Fuzzing see docs.</li> <li>Support for Skipping.</li> <li>Support for Main.</li> </ul> </li> <li> <p>OSS-Fuzz</p> <p>Fuzzing technique for automated injections with a Go project support.</p> </li> <li> <p>FuzzBench</p> <p>Fuzzer benchmarking as a service.</p> </li> </ul>"},{"location":"guides/style-guide/#25-architecture","title":"2.5. Architecture","text":"<ul> <li>Project Layout</li> <li>Project Layout Generator</li> <li>Go Clean Template</li> </ul>"},{"location":"guides/style-guide/#26-editor","title":"2.6. Editor","text":"<ul> <li>Visual Studio Code</li> </ul>"},{"location":"guides/style-guide/#27-links","title":"2.7. Links","text":"<ul> <li>FAQ</li> </ul>"},{"location":"guides/style-guide/#3-python","title":"3. Python","text":"Documentation Build System Package Manager Software Testing Code Coverage Code Formatting Code Linting Cross-Compilation Dependency Built-in Built-in Dependency Dependency Dependency Dependency Dependency"},{"location":"guides/style-guide/#31-convention","title":"3.1. Convention","text":"<ul> <li>Google Python Style Guide</li> <li>PEP8</li> <li>PEP257</li> <li>PEP484</li> </ul>"},{"location":"guides/style-guide/#32-library","title":"3.2. Library","text":"<ul> <li> <p>Click</p> <p>Package for creating CLI apps.</p> </li> <li> <p>Pandas</p> <p>Package that provides flexible and expressive data structures.</p> </li> <li> <p>Faker</p> <p>Package that generates fake data.</p> </li> <li> <p>Pendulum</p> <p>Package for datetimes.</p> </li> <li> <p>grpcio</p> <p>Python language implementation of gRPC.</p> </li> <li> <p>Sphinx</p> <p>Documentation package for Python projects.</p> </li> </ul>"},{"location":"guides/style-guide/#33-code-analysis","title":"3.3. Code Analysis","text":"<p>Code Check</p> <ul> <li> <p>Pylint</p> <p>Static code analysis tool.</p> </li> <li> <p>Flake8</p> <p>Wrapper of tools to check code style and quality.</p> </li> <li> <p>Bandit</p> <p>Find security issues in code.</p> </li> <li> <p>Pylama</p> <p>Code audit tool.</p> </li> <li> <p>MyPy</p> <p>Optional static typing.</p> </li> <li> <p>pytype</p> <p>A static type analyzer.</p> </li> <li> <p>Pyre</p> <p>Performant type-checking.</p> </li> </ul> <p>Code Format</p> <ul> <li> <p>Black</p> <p>Uncompromising code formatter.</p> </li> <li> <p>yapf</p> <p>Code formatter for PEP8 or Google style.</p> </li> <li> <p>autopep8</p> <p>Code formatter to conform to the PEP8 style guide.</p> </li> <li> <p>isort</p> <p>Sort imports alphabetically.</p> </li> <li> <p>autoflake</p> <p>Removes unused imports and unused variables.</p> </li> </ul>"},{"location":"guides/style-guide/#34-software-testing","title":"3.4. Software Testing","text":"<ul> <li> <p>pytest</p> <p>Unit-testing framework.</p> </li> <li> <p>pytest-xdist</p> <p>pytest plugin for distributed testing and loop-on-failures testing modes.</p> </li> <li> <p>pytest fixtures</p> <p>Fixtures initialize test functions.</p> </li> <li> <p>pytest-mock</p> <p>Supports mock test for pytest.</p> </li> <li> <p>pytest-cov</p> <p>Supports test code coverage for pytest.</p> </li> <li> <p>atheris fuzzing</p> <p>Fuzzing technique for automated injections.</p> </li> <li> <p>OSS-Fuzz</p> <p>Fuzzing technique for automated injections with a Python project support.</p> </li> <li> <p>FuzzBench</p> <p>Fuzzer benchmarking as a service.</p> </li> </ul>"},{"location":"guides/style-guide/#35-tool","title":"3.5. Tool","text":"<ul> <li> <p>virtualenv</p> <p>Tool for creating isolated virtual python environments.</p> </li> <li> <p>tox</p> <p>Command line (CLI) driven CI frontend and development task automation tool.</p> </li> </ul>"},{"location":"guides/style-guide/#36-architecture","title":"3.6. Architecture","text":"<ul> <li> <p>PyScaffold</p> <p>Python project generator.</p> </li> <li> <p>python-blueprint</p> <p>Python project using best practices.</p> </li> <li> <p>Python Project Template</p> </li> </ul>"},{"location":"guides/style-guide/#37-editor","title":"3.7. Editor","text":"<ul> <li>Visual Studio Code <p>Getting Started with Python in VS Code.</p> </li> </ul>"},{"location":"guides/style-guide/#4-kotlin","title":"4. Kotlin","text":"Documentation Build System Package Manager Software Testing Code Coverage Code Formatting Code Linting Cross-Compilation"},{"location":"guides/style-guide/#41-convention","title":"4.1. Convention","text":"<ul> <li>Kotlin Style Guide</li> </ul>"},{"location":"guides/style-guide/#42-code-analysis","title":"4.2. Code Analysis","text":"<p>Code Check</p> <ul> <li>Kotlin Lint <p>Manually run inspections</p> </li> </ul>"},{"location":"guides/style-guide/#43-guides","title":"4.3. Guides","text":"<ul> <li>Learn Kotlin with Getting Started.</li> </ul>"},{"location":"guides/style-guide/#44-editor","title":"4.4. Editor","text":"<ul> <li>Android Studio</li> </ul>"},{"location":"guides/style-guide/#5-rust","title":"5. Rust","text":"Documentation Build System Package Manager Software Testing Code Coverage Code Formatting Code Linting Cross-Compilation"},{"location":"guides/style-guide/#51-convention","title":"5.1. Convention","text":"<ul> <li>TODO</li> </ul>"},{"location":"guides/style-guide/#52-code-analysis","title":"5.2. Code Analysis","text":"<p>Code Check</p> <ul> <li>Miri</li> </ul>"},{"location":"guides/style-guide/#53-software-testing","title":"5.3. Software Testing","text":"<ul> <li> <p>OSS-Fuzz</p> <p>Fuzzing technique for automated injections with a Rust project support.</p> </li> <li> <p>FuzzBench</p> <p>Fuzzer benchmarking as a service.</p> </li> </ul>"},{"location":"guides/style-guide/#6-shell","title":"6. Shell","text":""},{"location":"guides/style-guide/#61-convention","title":"6.1. Convention","text":"<ul> <li>Google Shell Style Guide <p>Gitlab Shell Scripting Recommendation</p> </li> </ul>"},{"location":"guides/style-guide/#62-code-analysis","title":"6.2. Code Analysis","text":"<p>Code Check</p> <ul> <li>shellcheck <p>Configure .shellcheckrc in the root project.</p> </li> </ul> <p>Code Format</p> <ul> <li>shfmt</li> </ul>"},{"location":"guides/style-guide/#63-editor","title":"6.3. Editor","text":"<ul> <li>Visual Studio Code</li> </ul>"},{"location":"guides/style-guide/#7-git","title":"7. Git","text":""},{"location":"guides/style-guide/#71-convention","title":"7.1. Convention","text":"<ul> <li>Conventional Commits</li> <li>Semantic Versioning</li> </ul>"},{"location":"guides/style-guide/#72-code-analysis","title":"7.2. Code Analysis","text":"<p>Commit Check</p> <ul> <li> <p>commitlint</p> <p>Configure in the root project.</p> <ul> <li>.commitlintrc.js</li> <li>git-husky-commitlint in <code>commit-msg</code></li> </ul> </li> <li> <p>commitlint-github-action</p> <p>Commit check for CI</p> </li> </ul>"},{"location":"guides/style-guide/#8-markdown","title":"8. Markdown","text":""},{"location":"guides/style-guide/#81-convention","title":"8.1. Convention","text":"<ul> <li>Google Markdown Style Guide</li> <li>Cirosantilli Markdown Style Guide</li> </ul>"},{"location":"guides/style-guide/#82-code-analysis","title":"8.2. Code Analysis","text":"<p>Code Check</p> <ul> <li> <p>markdownlint</p> <p>Description of all markdown rules.</p> </li> <li> <p>markdownlint</p> <p>Code check for NPM - A Node.js style checker and lint tool.</p> <p>Configure in the root project.</p> <ul> <li>.markdownlint.json</li> </ul> </li> <li> <p>markdownlint-cli</p> <p>Code check for CLI - A Command Line Interface for MarkdownLint.</p> <p>Configure in the root project.</p> <ul> <li>.markdownlint.json</li> </ul> </li> <li> <p>markdownlint</p> <p>Code check for CI - A Github Action tool.</p> </li> <li> <p>markdown-link-check</p> <p>Link check for markdown.</p> <p>Configure in the root project.</p> <ul> <li>.markdown-link-check.json</li> </ul> </li> </ul> <p>Spell Check</p> <ul> <li> <p>markdown-spellcheck</p> <p>Configure in the root project.</p> <ul> <li>.spelling</li> </ul> </li> <li> <p>alex catch insensitive, inconsiderate writing.</p> <p>Configure in the root project.</p> <ul> <li>.alexrc.yml</li> <li>.alexignore</li> </ul> </li> </ul>"},{"location":"guides/style-guide/#83-editor","title":"8.3. Editor","text":"<ul> <li>Visual Studio Code</li> </ul>"},{"location":"guides/style-guide/#9-cmake","title":"9. CMake","text":""},{"location":"guides/style-guide/#91-convention","title":"9.1. Convention","text":"<ul> <li>KDE CMake Coding Style</li> <li>CMake Developer</li> </ul>"},{"location":"guides/style-guide/#92-code-analysis","title":"9.2. Code Analysis","text":"<p>Code Check</p> <ul> <li>cmake-lint</li> </ul> <p>Code Format</p> <ul> <li>cmake_format <p>Configure in the root project.</p> <ul> <li>cmake-format</li> <li>VS Code Extension</li> </ul> </li> </ul>"},{"location":"guides/style-guide/#93-editor","title":"9.3. Editor","text":"<ul> <li>Visual Studio Code</li> </ul>"},{"location":"guides/versioning-guide/","title":"Versioning Guide","text":"<p>Semantic Versioning (SemVer) is a system that aids in tracking versioning projects.</p> <ul> <li>1. Versioning</li> <li>1.1. Release</li> <li>1.2. Pre-release</li> <li>2. Tools</li> <li>2.1. Semantic Release</li> <li>2.2. Standard Version</li> <li>2.3. Conventional Changelog</li> <li>2.4. Conventional Changelog Configuration Spec</li> <li>2.5. Release Tools</li> </ul>"},{"location":"guides/versioning-guide/#1-versioning","title":"1. Versioning","text":"<p>The actions to create a <code>version tag</code> and a <code>CHANGELOG.md</code> file based on the <code>semantic versioning</code> convention rely on the commit messages following the <code>conventional commits</code> convention.</p> <p>An yaml file containing actions for continuous release pipeline is triggered from base branch.</p>"},{"location":"guides/versioning-guide/#11-release","title":"1.1. Release","text":"<p>SemVer is a 3-component system <code>x</code>.<code>y</code>.<code>z</code>:</p> <ul> <li> <p><code>x</code></p> <p>Stands for a major version. Introduces a breaking change, code that contains incompatible or significant changes.</p> </li> <li> <p><code>y</code></p> <p>Stands for a minor version. Introduces a feature change, code that includes new backward compatible changes.</p> </li> <li> <p><code>z</code></p> <p>Stands for a patch. Introduces a bug fix, code that contains backward compatible fix changes.</p> </li> </ul> <p><code>Major</code>.<code>Minor</code>.<code>Patch</code>, increment cases:</p> <ul> <li> <p><code>Major</code></p> <p>Increments the version tag <code>1.0.0</code> -&gt; <code>2.0.0</code> and modifies the <code>CHANGELOG.md</code>.</p> </li> <li> <p><code>Minor</code></p> <p>Increments the version tag <code>1.0.0</code> -&gt; <code>1.1.0</code> and modifies the <code>CHANGELOG.md</code>.</p> </li> <li> <p><code>Patch</code></p> <p>Increments the version tag <code>1.0.0</code> -&gt; <code>1.0.1</code> and modifies the <code>CHANGELOG.md</code>.</p> </li> </ul>"},{"location":"guides/versioning-guide/#12-pre-release","title":"1.2. Pre-release","text":"<p>With Semantic Versioning, pre-releases or release candidate (rc) for a given release can be defined by appending a hyphen and an identifier to a version.</p> <p>Regular releases to the <code>next</code> distribution channel from the branch <code>next</code> if it exists:</p> <ul> <li><code>1.0.0</code> -&gt; <code>1.1.0-rc.1</code> -&gt; <code>1.1.0-rc.2</code> -&gt; <code>1.1.0</code></li> </ul> <p>or</p> <ul> <li><code>1.0.0</code> -&gt; <code>1.1.0-next.1</code> -&gt; <code>1.1.0-next.2</code> -&gt; <code>1.1.0</code></li> </ul>"},{"location":"guides/versioning-guide/#2-tools","title":"2. Tools","text":"<p>Tools to generate changelogs and version tag from a commit messages and metadata.</p>"},{"location":"guides/versioning-guide/#21-semantic-release","title":"2.1. Semantic Release","text":"<p>semantic-release automates the whole package release workflow including: determining the next version number by <code>Semantic Versioning</code>, generating the release notes based on <code>Conventional Commits</code>, and publishing the package. Unlike <code>standard-version</code>,  <code>semantic-release</code> is meant to be executed on the CI environment after every successful build on the release branch.</p>"},{"location":"guides/versioning-guide/#22-standard-version","title":"2.2. Standard Version","text":"<p>standard-version is a CLI utility for automate versioning and CHANGELOG generation based on <code>Semantic Versioning</code> and <code>Conventional Commits</code>.</p>"},{"location":"guides/versioning-guide/#23-conventional-changelog","title":"2.3. Conventional Changelog","text":"<p>Generate changelogs and release notes from a project's commit messages and metadata.</p>"},{"location":"guides/versioning-guide/#24-conventional-changelog-configuration-spec","title":"2.4. Conventional Changelog Configuration Spec","text":"<p>A spec describing the config options supported by conventional-config for upstream tooling.</p>"},{"location":"guides/versioning-guide/#25-release-tools","title":"2.5. Release Tools","text":"<p>Create a GitHub/GitLab/etc. release using a project's commit messages and metadata.</p>"},{"location":"guides/xops-guide/","title":"XOps Guide","text":"<p>XOps is designed to run on-premises or in a private cloud that meets specific requirements. It is built to run enterprise-wide, globally distributed, and interconnected, especially in analytic environments. It is a combination of traditional business applications, such as data and analytics management, with modern technologies such as:</p> <ul> <li>1. GitOps</li> <li>2. DevOps</li> <li>3. DevSecOps</li> <li>3.1. Secrets Management</li> <li>3.2. Identity and Access Management</li> <li>3.3. Signing Contributions</li> <li>3.4. Signing Containers</li> </ul>"},{"location":"guides/xops-guide/#1-gitops","title":"1. GitOps","text":"<p>GitOps is a DevOps methodology that leverages the Git version control system as a single source of truth for declarative infrastructure and application code.</p> <p>The GitOps process can be divided into several stages or categories, including:</p> <ul> <li> <p>Review</p> <p>Teams review and approve code changes and infrastructure updates through <code>Pull Requests</code>. Code review is ensuring that the code meets the quality standards and requirements. Review may involve automated tests and peer reviews to ensure that the code is efficient, reliable, and secure.</p> </li> <li> <p>Merge</p> <p>Teams merge approved changes to the base branch of the Git repository, triggering an automated deployment pipeline. Merging is ensuring that the code changes are integrated and tested before deployment. The deployment pipeline may include automated testing, building, and packaging the code for deployment.</p> <p>This stage includes the categories:</p> <ul> <li>Merging Strategies</li> <li>Branching Strategies</li> <li>Conventional Commits</li> <li>Semantic Versioning</li> <li>Changelog</li> </ul> </li> <li> <p>Auditing Complaince</p> <p>Software vendors conduct software compliance audits to verify adherence to the contracts and license terms they have concluded.</p> </li> <li> <p>Rollback</p> <p>Teams have the option to roll back changes when an issue is detected in the production environment. Rollback involves reverting to a previous version of the infrastructure or application code, providing a quick solution to any issues that may arise.</p> </li> </ul>"},{"location":"guides/xops-guide/#2-devops","title":"2. DevOps","text":"<p>DevOps is a software development practice that combines software development and information technology operations to streamline the software delivery process. It aims to increase collaboration and communication between development and operations teams, automate the software delivery pipeline, and promote a culture of continuous improvement and innovation. DevOps is based on the principles of Agile development and focuses on delivering high-quality software quickly, efficiently, and at a low cost. Key practices include continuous integration and deployment, automation, testing, and monitoring. DevOps helps organizations to respond faster to changing customer demands, improve software quality and reliability, and increase operational efficiency.</p> <p>The DevOps process can be divided into several stages or categories, including:</p> <ul> <li> <p>Plan</p> <p>Plan stage involves defining the goals, objectives, and scope of the software development project. The planning stage involves defining the requirements, identifying the target audience, and determining the most appropriate delivery methods.</p> </li> <li> <p>Code</p> <p>Code stage involves writing the code for the software, typically using Agile development methodologies. Code development involves implementing the requirements and ensuring that the code meets the quality standards and requirements.</p> </li> <li> <p>Build</p> <p>Build stage involves compiling and packaging the code into a deployable artifact, such as a binary or a Docker container. This stage also involves performing automated testing to ensure that the code meets the quality standards and requirements.</p> </li> <li> <p>Release</p> <p>Release stage involves deploying the code to production and making it available to users. The release stage involves performing quality assurance checks, setting up monitoring and logging systems, and preparing the infrastructure for deployment.</p> </li> <li> <p>Deployment</p> <p>Deployment stage involves installing and configuring the software on the target environment, typically using automation tools such as Puppet or Ansible. This stage involves configuring the software to work in the target environment, setting up security measures, and ensuring that the software meets the requirements of the target audience.</p> </li> <li> <p>Operate</p> <p>Operate stage involves maintaining the software in the production environment, ensuring it is functional, and providing support to end-users. The operate stage involves monitoring the software to detect and resolve any issues that may arise, applying security patches and updates, and performing regular maintenance tasks to maintain the software's performance.</p> </li> <li> <p>Monitoring</p> <p>Monitoring stage involves monitoring the software's performance and usage, and collecting data for analysis and reporting. Monitoring is an ongoing process and involves using a variety of tools and techniques to ensure that the software is performing as expected and meeting the needs of the target audience.</p> </li> <li> <p>Feedback</p> <p>Feedback stage involves gathering feedback from users and stakeholders, and using that feedback to drive continuous improvement in the software development process. Feedback is an ongoing process that involves gathering information from users and stakeholders, analyzing the data, and making changes to the software development process based on that data.</p> </li> </ul>"},{"location":"guides/xops-guide/#3-devsecops","title":"3. DevSecOps","text":"<p>DevSecOps is a methodology that emphasizes the integration of security practices into the DevOps process, enabling organizations to build secure software that meets the needs of their target audience.</p> <p>The DevSecOps process can be divided into several stages or categories, including:</p> <ul> <li> <p>Plan</p> <p>Teams define the security requirements, identifying the target audience, and determining the most appropriate delivery methods. Planning may involve defining the security controls, identifying vulnerabilities, and creating a risk management plan.</p> </li> <li> <p>Develop</p> <p>Teams develop the infrastructure and application code using secure coding practices. Development may involve creating secure coding standards, performing threat modeling, and conducting security testing in a local development environment.</p> </li> <li> <p>Test</p> <p>Teams perform security testing using Application Security Testing (AST) methodology on the application code and infrastructure to identify and mitigate vulnerabilities. Testing may involve automated security testing tools, manual penetration testing, and code reviews to ensure that the code is secure and meets the security requirements.</p> </li> <li> <p>Monitor</p> <p>Teams monitor the application and infrastructure in the production environment, using Dynamic Application Security Testing (DAST) methodology. Monitoring involves collecting and analyzing data about the performance of the application and infrastructure, identifying security incidents and attacks on an application, and resolving them to improve system security and availability.</p> </li> <li> <p>Respond</p> <p>Teams respond to security incidents in a timely and effective manner. This may involve creating incident response plans, conducting root cause analysis, and performing remediation actions to mitigate the impact of a security incident.</p> </li> <li> <p>Govern</p> <p>Teams establish governance policies and procedures to ensure that security is embedded throughout the software development lifecycle. Governance may involve creating security standards and policies, establishing security training programs, and conducting security audits to ensure that the security controls are effective and meet compliance requirements.</p> </li> </ul>"},{"location":"guides/xops-guide/#31-secrets-management","title":"3.1. Secrets Management","text":"<p>Secrets Management is an important aspect of DevSecOps, and it can be integrated into various stages of the DevSecOps process. Secrets refer to sensitive information such as passwords, API keys, certificates, and other credentials that are used to authenticate and authorize access to systems, databases, and other resources.</p> <p>Integrating secrets management into the DevSecOps process, teams can ensure that secrets are managed securely throughout the software development lifecycle. This helps to reduce the risk of security incidents and ensure compliance with industry regulations.</p> <p>To ensure that secrets are managed securely, a secrets management system can be integrated into the development process. This can involve the following stages:</p> <ul> <li> <p>Plan</p> <p>During the planning stage, security requirements should include the management of secrets. Teams should identify the types of secrets that will be used in the application and determine how they will be stored and managed.</p> </li> <li> <p>Develop</p> <p>During the development stage, secrets should be managed using secure coding practices, such as encryption and key management. Secure coding standards should be established to ensure that secrets are not hardcoded into the application code.</p> </li> <li> <p>Test</p> <p>During the testing stage, security testing should be performed to ensure that secrets are being managed securely. Security testing tools can be used to identify vulnerabilities in the secrets management system.</p> </li> <li> <p>Deploy</p> <p>During the deployment stage, secrets should be deployed securely using automated tools. A secure secrets management system should be used to ensure that secrets are not exposed during deployment.</p> </li> <li> <p>Monitor</p> <p>During the monitoring stage, teams should continuously monitor the secrets management system to identify any security incidents. Teams should analyze the data collected to identify and respond to any security incidents promptly.</p> </li> <li> <p>Respond</p> <p>During the response stage, teams should have an incident response plan in place to respond to security incidents that may affect the secrets management system. Remediation actions should be taken to mitigate the impact of any security incidents.</p> </li> </ul>"},{"location":"guides/xops-guide/#32-identity-and-access-management","title":"3.2. Identity and Access Management","text":"<p>Identity and Access Management (IAM) is an important aspect of DevSecOps. IAM involves managing and controlling user access to systems, applications, and data. IAM is critical to ensuring the security of an organization's IT infrastructure and data.</p> <p>Integrating IAM into the DevSecOps process, teams can ensure that user access is managed and controlled effectively. This helps to reduce the risk of security incidents and ensure compliance with industry regulations.</p> <p>IAM can be integrated into various stages of the DevSecOps process. This can involve the following stages:</p> <ul> <li> <p>Plan</p> <p>During the planning stage, teams should identify the types of users who will be accessing the application and determine the access controls that will be implemented. This includes identifying the roles and permissions of users and defining the access policies that will be used to control user access.</p> </li> <li> <p>Develop</p> <p>During the development stage, teams should implement secure coding practices that support IAM. This includes implementing user authentication and authorization controls, such as two-factor authentication and role-based access control (RBAC).</p> </li> <li> <p>Test</p> <p>During the testing stage, teams should perform security testing to ensure that the IAM controls are working as expected. This includes testing the user authentication and authorization controls to ensure that they are not vulnerable to attacks.</p> </li> <li> <p>Deploy</p> <p>During the deployment stage, IAM controls should be deployed using automated tools. This includes deploying IAM policies, permissions, and access controls to the target environment.</p> </li> <li> <p>Monitor</p> <p>During the monitoring stage, teams should continuously monitor the IAM controls to identify any security incidents. Teams should analyze the data collected to identify and respond to any security incidents promptly.</p> </li> <li> <p>Respond</p> <p>During the response stage, teams should have an incident response plan in place to respond to security incidents that may affect the IAM controls. Remediation actions should be taken to mitigate the impact of any security incidents.</p> </li> <li> <p>Govern</p> <p>During the governance stage, teams should establish policies and procedures to ensure that IAM controls are implemented consistently across the organization. This includes creating IAM standards and policies, establishing IAM training programs, and conducting IAM audits to ensure that the IAM controls are effective and meet compliance requirements.</p> </li> </ul>"},{"location":"guides/xops-guide/#33-signing-contributions","title":"3.3. Signing Contributions","text":"<p>Signing contributions is an important practice in DevSecOps and software development in general. It involves digitally signing the code changes made by developers to verify their identity and ensure the integrity of the code.</p> <p>Signing contributions is particularly important in open-source software development, where multiple developers contribute code to a project. By signing their contributions, developers can demonstrate that the code changes are legitimate and not tampered with.</p> <p>Integrating signing contributions into the DevSecOps process, teams can ensure that code changes are legitimate and secure. This helps to reduce the risk of security incidents and ensure the quality and integrity of the software being developed.</p> <p>Signing contributions can be integrated into the DevSecOps process:</p> <ul> <li> <p>Plan</p> <p>During the planning stage, teams should identify the signing process that will be used to sign code contributions. This may involve selecting a digital signature tool or developing a custom signing process that meets the organization's needs.</p> </li> <li> <p>Develop</p> <p>During the development stage, developers should be required to sign their code contributions using the selected signing process. This helps to verify the identity of the developer and ensure the integrity of the code changes.</p> </li> <li> <p>Test</p> <p>During the testing stage, teams should test the signing process to ensure that it is working as expected. This may involve testing the digital signature tool or custom signing process to ensure that the signed code changes can be verified.</p> </li> <li> <p>Deploy</p> <p>During the deployment stage, signed code changes should be deployed using automated tools. This helps to ensure that the signed code changes are not tampered with during deployment.</p> </li> <li> <p>Monitor</p> <p>During the monitoring stage, teams should continuously monitor the signed code changes to identify any security incidents. Teams should analyze the data collected to identify and respond to any security incidents promptly.</p> </li> <li> <p>Respond</p> <p>During the response stage, teams should have an incident response plan in place to respond to security incidents that may affect the signed code changes. Remediation actions should be taken to mitigate the impact of any security incidents.</p> </li> </ul>"},{"location":"guides/xops-guide/#34-signing-containers","title":"3.4. Signing Containers","text":"<p>Signing containers is an important aspect of DevSecOps. Containers are used to package applications and dependencies in a portable format that can be deployed across different environments. However, since containers can be easily tampered with, it is essential to sign containers to ensure their integrity and authenticity.</p> <p>Integrating container signing into the DevSecOps process, teams can ensure that their container images are authentic, tamper-proof, and free from vulnerabilities. This helps to reduce the risk of security incidents and ensure compliance with industry regulations.</p> <p>Signing containers can be integrated into the DevSecOps process:</p> <ul> <li> <p>Plan</p> <p>During the planning stage, teams should identify the types of containers that will be signed. Teams should also identify the tools and processes that will be used to sign the containers.</p> </li> <li> <p>Develop</p> <p>During the development stage, teams should implement signing into their container images. This includes using digital signatures to ensure that the container image is coming from a trusted source, and implementing checksums to detect unauthorized changes.</p> </li> <li> <p>Test</p> <p>During the testing stage, teams should perform security testing to ensure that the signed container images are working as expected. This includes testing the integrity of the signed container images to ensure that they are not vulnerable to attacks.</p> </li> <li> <p>Deploy</p> <p>During the deployment stage, signed container images should be deployed using automated tools. This includes deploying signed containers to the target environment.</p> </li> <li> <p>Monitor</p> <p>During the monitoring stage, teams should continuously monitor the signed container images to identify any security incidents. Teams should analyze the data collected to identify and respond to any security incidents promptly.</p> </li> <li> <p>Respond</p> <p>During the response stage, teams should have an incident response plan in place to respond to security incidents that may affect the signed container images. Remediation actions should be taken to mitigate the impact of any security incidents.</p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""}]}